<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-12T00:00:00Z">2024-11-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">76</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models as Causal Effect Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucius E. J. Bynum, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for large language model (LLM) based data generation
with controllable causal structure. In particular, we define a procedure for
turning any language model and any directed acyclic graph (DAG) into a
sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM
is a causal model with user-defined structure and LLM-defined structural
equations. We characterize how an SD-SCM allows sampling from observational,
interventional, and counterfactual distributions according to the desired
causal structure. We then leverage this procedure to propose a new type of
benchmark for causal inference methods, generating individual-level
counterfactual data without needing to manually specify functional
relationships between variables. We create an example benchmark consisting of
thousands of datasets, and test a suite of popular estimation methods on these
datasets for average, conditional average, and individual treatment effect
estimation, both with and without hidden confounding. Apart from generating
data, the same procedure also allows us to test for the presence of a causal
effect that might be encoded in an LLM. This procedure can underpin auditing
LLMs for misinformation, discrimination, or otherwise undesirable behavior. We
believe SD-SCMs can serve as a useful tool in any application that would
benefit from sequential data with controllable causal structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExpressivityArena: Can LLMs Express Information Implicitly? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Tint, Som Sagar, Aditya Taparia, Kelly Raines, Bimsara Pathiraja, Caleb Liu, Ransalu Senanayake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have demonstrated remarkable performance
in certain dimensions, their ability to express implicit language cues that
human use for effective communication remains unclear. This paper presents
ExpressivityArena, a Python library for measuring the implicit communication
abilities of LLMs. We provide a comprehensive framework to evaluate
expressivity of arbitrary LLMs and explore its practical implications. To this
end, we refine the definition and measurements of ``expressivity,'' and use our
framework in a set of small experiments. These experiments test LLMs in
creative and logical tasks such as poetry, coding, and emotion-based responses.
They are then evaluated by an automated grader, through ExpressivityArena,
which we verify to be the most pragmatic for testing expressivity. Building on
these experiments, we deepen our understanding of the expressivity of LLMs by
assessing their ability to remain expressive in conversations. Our findings
indicate that LLMs are capable of generating and understanding expressive
content, however, with some limitations. These insights will inform the future
development and deployment of expressive LLMs. We provide the code for
ExpressivityArena alongside our paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can adversarial attacks by large language models be attributed? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Cebrian, Jan Arne Telle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attributing outputs from Large Language Models (LLMs) in adversarial
settings-such as cyberattacks and disinformation-presents significant
challenges that are likely to grow in importance. We investigate this
attribution problem using formal language theory, specifically language
identification in the limit as introduced by Gold and extended by Angluin. By
modeling LLM outputs as formal languages, we analyze whether finite text
samples can uniquely pinpoint the originating model. Our results show that due
to the non-identifiability of certain language classes, under some mild
assumptions about overlapping outputs from fine-tuned models it is
theoretically impossible to attribute outputs to specific LLMs with certainty.
This holds also when accounting for expressivity limitations of Transformer
architectures. Even with direct model access or comprehensive monitoring,
significant computational hurdles impede attribution efforts. These findings
highlight an urgent need for proactive measures to mitigate risks posed by
adversarial LLM use as their influence continues to expand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivational Morphology Reveals Analogical Generalization in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Hofmann, Leonie Weissweiler, David Mortensen, Hinrich Schütze, Janet Pierrehumbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What mechanisms underlie linguistic generalization in large language models
(LLMs)? This question has attracted considerable attention, with most studies
analyzing the extent to which the language skills of LLMs resemble rules. As of
yet, it is not known whether linguistic generalization in LLMs could equally
well be explained as the result of analogical processes, which can be
formalized as similarity operations on stored exemplars. A key shortcoming of
prior research is its focus on linguistic phenomena with a high degree of
regularity, for which rule-based and analogical approaches make the same
predictions. Here, we instead examine derivational morphology, specifically
English adjective nominalization, which displays notable variability. We
introduce a new method for investigating linguistic generalization in LLMs:
focusing on GPT-J, we fit cognitive models that instantiate rule-based and
analogical learning to the LLM training data and compare their predictions on a
set of nonce adjectives with those of the LLM, allowing us to draw direct
conclusions regarding underlying mechanisms. As expected, rule-based and
analogical models explain the predictions of GPT-J equally well for adjectives
with regular nominalization patterns. However, for adjectives with variable
nominalization patterns, the analogical model provides a much better match.
Furthermore, GPT-J's behavior is sensitive to the individual word frequencies,
even for regular forms, a behavior that is consistent with an analogical
account of regular forms but not a rule-based one. These findings refute the
hypothesis that GPT-J's linguistic generalization on adjective nominalization
involves rules, suggesting similarity operations on stored exemplars as the
underlying mechanism. Overall, our study suggests that analogical processes
play a bigger role in the linguistic generalization of LLMs than previously
thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified
  Multimodal Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, Chong Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present JanusFlow, a powerful framework that unifies image understanding
and generation in a single model. JanusFlow introduces a minimalist
architecture that integrates autoregressive language models with rectified
flow, a state-of-the-art method in generative modeling. Our key finding
demonstrates that rectified flow can be straightforwardly trained within the
large language model framework, eliminating the need for complex architectural
modifications. To further improve the performance of our unified model, we
adopt two key strategies: (i) decoupling the understanding and generation
encoders, and (ii) aligning their representations during unified training.
Extensive experiments show that JanusFlow achieves comparable or superior
performance to specialized models in their respective domains, while
significantly outperforming existing unified approaches across standard
benchmarks. This work represents a step toward more efficient and versatile
vision-language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From General to Specific: Utilizing General Hallucation to Automatically
  Measure the Role Relationship Fidelity for Specific Role-Play Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyi Kong, Ziyang Luo, Hongzhan Lin, Zhiyuan Fan, Yaxin Fan, Yuxi Sun, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advanced role-playing capabilities of Large Language Models (LLMs) have
paved the way for developing Role-Playing Agents (RPAs). However, existing
benchmarks, such as HPD, which incorporates manually scored character
relationships into the context for LLMs to sort coherence, and SocialBench,
which uses specific profiles generated by LLMs in the context of
multiple-choice tasks to assess character preferences, face limitations like
poor generalizability, implicit and inaccurate judgments, and excessive context
length. To address the above issues, we propose an automatic, scalable, and
generalizable paradigm. Specifically, we construct a benchmark by extracting
relations from a general knowledge graph and leverage RPA's inherent
hallucination properties to prompt it to interact across roles, employing
ChatGPT for stance detection and defining relationship hallucination along with
three related metrics. Extensive experiments validate the effectiveness and
stability of our metrics. Our findings further explore factors influencing
these metrics and discuss the trade-off between relationship hallucination and
factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CryptoLLM: Unleashing the Power of <span class="highlight-title">Prompt</span>ed LLMs for SmartQnA and
  Classification of Crypto Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of social media has resulted in an large volume of
user-generated content, particularly in niche domains such as cryptocurrency.
This task focuses on developing robust classification models to accurately
categorize cryptocurrency-related social media posts into predefined classes,
including but not limited to objective, positive, negative, etc. Additionally,
the task requires participants to identify the most relevant answers from a set
of posts in response to specific questions. By leveraging advanced LLMs, this
research aims to enhance the understanding and filtering of cryptocurrency
discourse, thereby facilitating more informed decision-making in this volatile
sector. We have used a prompt-based technique to solve the classification task
for reddit posts and twitter posts. Also, we have used 64-shot technique along
with prompts on GPT-4-Turbo model to determine whether a answer is relevant to
a question or not.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Opinion Extraction and Question
  Answering from CryptoCurrency-Related Tweets and Reddit posts (CryptOQA))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping the Podcast Ecosystem with the Structured Podcast Research
  Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Litterer, David Jurgens, Dallas Card
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Podcasts provide highly diverse content to a massive listener base through a
unique on-demand modality. However, limited data has prevented large-scale
computational analysis of the podcast ecosystem. To fill this gap, we introduce
a massive dataset of over 1.1M podcast transcripts that is largely
comprehensive of all English language podcasts available through public RSS
feeds from May and June of 2020. This data is not limited to text, but rather
includes audio features and speaker turns for a subset of 370K episodes, and
speaker role inferences and other metadata for all 1.1M episodes. Using this
data, we also conduct a foundational investigation into the content, structure,
and responsiveness of this ecosystem. Together, our data and analyses open the
door to continued computational research of this popular and impactful medium.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustful LLMs: Customizing and Grounding Text Generation with Knowledge
  Bases and Dual Decoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Zhu, Jaya Krishna Mandivarapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbosity $\neq$ Veracity: Demystify Verbosity Compensation Behavior of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusen Zhang, Sarkar Snigdha Sarathi Das, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When unsure about an answer, humans often respond with more words than
necessary, hoping that part of the response will be correct. We observe a
similar behavior in large language models (LLMs), which we term "Verbosity
Compensation" (VC). VC is harmful because it confuses the user understanding,
leading to low efficiency, and influences the LLM services by increasing the
latency and cost of generating useless tokens. In this paper, we present the
first work that defines and analyzes Verbosity Compensation, explores its
causes, and proposes a simple mitigating approach. We define Verbosity
Compensation as the behavior of generating responses that can be compressed
without information loss when prompted to write concisely. Our experiments,
conducted on five datasets of knowledge and reasoning-based QA tasks with 14
newly developed LLMs, reveal three conclusions. 1) We reveal a pervasive
presence of verbosity compensation across all models and all datasets. Notably,
GPT-4 exhibits a VC frequency of 50.40%. 2) We reveal the large performance gap
between verbose and concise responses, with a notable difference of 27.61% on
the Qasper dataset. We also demonstrate that this difference does not naturally
diminish as LLM capability increases. Both 1) and 2) highlight the urgent need
to mitigate the frequency of VC behavior and disentangle verbosity with
veracity. We propose a simple yet effective cascade algorithm that replaces the
verbose responses with the other model-generated responses. The results show
that our approach effectively alleviates the VC of the Mistral model from
63.81% to 16.16% on the Qasper dataset. 3) We also find that verbose responses
exhibit higher uncertainty across all five datasets, suggesting a strong
connection between verbosity and model uncertainty. Our dataset and code are
available at https://github.com/psunlpgroup/VerbosityLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tucano: Advancing Neural Text Generation for Portuguese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Kluge Corrêa, Aniket Sen, Sophia Falk, Shiza Fatimah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advances have been made in natural language processing in recent
years. However, our current deep learning approach to language modeling
requires substantial resources in terms of data and computation. One of the
side effects of this data-hungry paradigm is the current schism between
languages, separating those considered high-resource, where most of the
development happens and resources are available, and the low-resource ones,
which struggle to attain the same level of performance and autonomy. This study
aims to introduce a new set of resources to stimulate the future development of
neural text generation in Portuguese. In this work, we document the development
of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting
to 200 billion tokens. Via this corpus, we trained a series of
decoder-transformers named Tucano. Our models perform equal or superior to
other Portuguese and multilingual language models of similar size in several
Portuguese benchmarks. The evaluation of our models also reveals that model
performance on many currently available benchmarks used by the Portuguese NLP
community has little to no correlation with the scaling of token ingestion
during training, highlighting the limitations of such evaluations when it comes
to the assessment of Portuguese generative language models. All derivatives of
our study are openly released on GitHub and Hugging Face. See
https://nkluge-correa.github.io/Tucano/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IAE: Irony-based Adversarial Examples for Sentiment Analysis Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyin Yi, Jiacheng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples, which are inputs deliberately perturbed with
imperceptible changes to induce model errors, have raised serious concerns for
the reliability and security of deep neural networks (DNNs). While adversarial
attacks have been extensively studied in continuous data domains such as
images, the discrete nature of text presents unique challenges. In this paper,
we propose Irony-based Adversarial Examples (IAE), a method that transforms
straightforward sentences into ironic ones to create adversarial text. This
approach exploits the rhetorical device of irony, where the intended meaning is
opposite to the literal interpretation, requiring a deeper understanding of
context to detect. The IAE method is particularly challenging due to the need
to accurately locate evaluation words, substitute them with appropriate
collocations, and expand the text with suitable ironic elements while
maintaining semantic coherence. Our research makes the following key
contributions: (1) We introduce IAE, a strategy for generating textual
adversarial examples using irony. This method does not rely on pre-existing
irony corpora, making it a versatile tool for creating adversarial text in
various NLP tasks. (2) We demonstrate that the performance of several
state-of-the-art deep learning models on sentiment analysis tasks significantly
deteriorates when subjected to IAE attacks. This finding underscores the
susceptibility of current NLP systems to adversarial manipulation through
irony. (3) We compare the impact of IAE on human judgment versus NLP systems,
revealing that humans are less susceptible to the effects of irony in text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics
  Statements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonia Karamolegkou, Sandrine Schiller Hansen, Ariadni Christopoulou, Filippos Stamatiou, Anne Lauscher, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What ethical concerns, if any, do LLM researchers have? We introduce EthiCon,
a corpus of 1,580 ethical concern statements extracted from scientific papers
published in the ACL Anthology. We extract ethical concern keywords from the
statements and show promising results in automating the concern identification
process. Through a survey, we compare the ethical concerns of the corpus to the
concerns listed by the general public and professionals in the field. Finally,
we compare our retrieved ethical concerns with existing taxonomies pointing to
gaps and future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain Association-based Attacking and Shielding Natural Language
  Processing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Huang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association as a gift enables people do not have to mention something in
completely straightforward words and allows others to understand what they
intend to refer to. In this paper, we propose a chain association-based
adversarial attack against natural language processing systems, utilizing the
comprehension gap between humans and machines. We first generate a chain
association graph for Chinese characters based on the association paradigm for
building search space of potential adversarial examples. Then, we introduce an
discrete particle swarm optimization algorithm to search for the optimal
adversarial examples. We conduct comprehensive experiments and show that
advanced natural language processing models and applications, including large
language models, are vulnerable to our attack, while humans appear good at
understanding the perturbed text. We also explore two methods, including
adversarial training and associative graph-based recovery, to shield systems
from chain association-based attack. Since a few examples that use some
derogatory terms, this paper contains materials that may be offensive or
upsetting to some people.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youan Cong, Cheng Wang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the \textit{Extract-Refine-Retrieve-Read} (ERRR) framework, a
novel approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Likelihood as a Performance Gauge for Retrieval-Augmented Generation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Liu, Jirui Qi, Paul He, Arianna Bisazza, Mrinmaya Sachan, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work finds that retrieval-augmented generation with large language
models is prone to be influenced by the order of retrieved documents in the
context. However, the lack of in-depth analysis limits the use of this
phenomenon for prompt engineering in practice. In this study, we posit that
likelihoods serve as an effective gauge for language model performance. Through
experiments on two question-answering datasets with a variety of
state-of-the-art language models, we reveal correlations between answer
accuracy and the likelihood of the question at both the corpus level and the
instance level. In addition, we find that question likelihood can also indicate
the position of the task-relevant information in the context. Based on these
findings, we propose two methods that use question likelihood as a gauge for
selecting and constructing prompts that lead to better performance. We
demonstrate their effectiveness with experiments. In addition, our
likelihood-based methods are efficient, as they only need to compute the
likelihood of the input, requiring much fewer language model passes than
heuristic prompt engineering methods that require generating responses. Our
analysis deepens our understanding of how input prompts affect model
performance and provides a promising direction for efficient prompt
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at NAACL 2025. Code is available at
  https://github.com/lyutyuh/poptimizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Album Sequencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Herrmann, Dylan R. Ashley, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented as a late breaking demo in the 25th International Society
  for Music Information Retrieval Conference; 3 pages in main text, 3 figures
  in main text; source code available at
  https://github.com/dylanashley/automatic-album-sequencing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spider 2.0: Evaluating Language Models on Real-World Enterprise
  Text-to-SQL Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, Tao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world enterprise text-to-SQL workflows often involve complex cloud or
local data across various database systems, multiple SQL queries in various
dialects, and diverse operations from data transformation to analytics. We
introduce Spider 2.0, an evaluation framework comprising 632 real-world
text-to-SQL workflow problems derived from enterprise-level database use cases.
The databases in Spider 2.0 are sourced from real data applications, often
containing over 1,000 columns and stored in local or cloud database systems
such as BigQuery and Snowflake. We show that solving problems in Spider 2.0
frequently requires understanding and searching through database metadata,
dialect documentation, and even project-level codebases. This challenge calls
for models to interact with complex SQL workflow environments, process
extremely long contexts, perform intricate reasoning, and generate multiple SQL
queries with diverse operations, often exceeding 100 lines, which goes far
beyond traditional text-to-SQL challenges. Our evaluations indicate that based
on o1-preview, our code agent framework successfully solves only 17.0% of the
tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on
Spider 2.0 show that while language models have demonstrated remarkable
performance in code generation -- especially in prior text-to-SQL benchmarks --
they require significant improvement in order to achieve adequate performance
for real-world enterprise usage. Progress on Spider 2.0 represents crucial
steps towards developing intelligent, autonomous, code agents for real-world
enterprise settings. Our code, baseline models, and data are available at
https://spider2-sql.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Bias in Queer Representation within Large Language Models: A
  Collaborative Agent Approach <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Huang, Arya Somasundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often perpetuate biases in pronoun usage,
leading to misrepresentation or exclusion of queer individuals. This paper
addresses the specific problem of biased pronoun usage in LLM outputs,
particularly the inappropriate use of traditionally gendered pronouns ("he,"
"she") when inclusive language is needed to accurately represent all
identities. We introduce a collaborative agent pipeline designed to mitigate
these biases by analyzing and optimizing pronoun usage for inclusivity. Our
multi-agent framework includes specialized agents for both bias detection and
correction. Experimental evaluations using the Tango dataset-a benchmark
focused on gender pronoun usage-demonstrate that our approach significantly
improves inclusive pronoun classification, achieving a 32.6 percentage point
increase over GPT-4o in correctly disagreeing with inappropriate traditionally
gendered pronouns $(\chi^2 = 38.57, p < 0.0001)$. These results accentuate the
potential of agent-driven frameworks in enhancing fairness and inclusivity in
AI-generated content, demonstrating their efficacy in reducing biases and
promoting socially responsible AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Queer in AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotating Constructions with UD: the experience of the Italian
  Constructicon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovica Pannitto, Beatrice Bernasconi, Lucia Busso, Flavio Pisciotta, Giulia Rambelli, Francesca Masini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper descirbes a first attempt of linking the Italian constructicon to
UD resources
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Preference Optimization Using Sparse Feature-Level Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment of large language models (LLMs) with human preferences remains
a key challenge. While post-training techniques like Reinforcement Learning
from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have
achieved notable success, they often introduce computational inefficiencies and
training instability. In this paper, we propose Feature-level constrained
Preference Optimization (FPO), a novel method designed to simplify the
alignment process while ensuring stability. FPO leverages pre-trained Sparse
Autoencoders (SAEs) and introduces feature-level constraints, allowing for
efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using
sparse features activated in a well-trained sparse autoencoder and the quality
of sequential KL divergence by using the feature-level offline reference.
Experimental results on benchmark datasets demonstrate that FPO achieves a
5.08% absolute improvement in win rate with much lower computational cost
compared to state-of-the-art baselines, making it a promising solution for
efficient and controllable LLM alignments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Clinical Reasoning through Knowledge-augmented Rationale
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Niu, Jing Ma, Liang Bai, Zhihua Wang, Yida Xu, Yunya Song, Xian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical rationales play a pivotal role in accurate disease diagnosis;
however, many models predominantly use discriminative methods and overlook the
importance of generating supportive rationales. Rationale distillation is a
process that transfers knowledge from large language models (LLMs) to smaller
language models (SLMs), thereby enhancing the latter's ability to break down
complex tasks. Despite its benefits, rationale distillation alone is inadequate
for addressing domain knowledge limitations in tasks requiring specialized
expertise, such as disease diagnosis. Effectively embedding domain knowledge in
SLMs poses a significant challenge. While current LLMs are primarily geared
toward processing textual data, multimodal LLMs that incorporate time series
data, especially electronic health records (EHRs), are still evolving. To
tackle these limitations, we introduce ClinRaGen, an SLM optimized for
multimodal rationale generation in disease diagnosis. ClinRaGen incorporates a
unique knowledge-augmented attention mechanism to merge domain knowledge with
time series EHR data, utilizing a stepwise rationale distillation strategy to
produce both textual and time series-based clinical rationales. Our evaluations
show that ClinRaGen markedly improves the SLM's capability to interpret
multimodal EHR data and generate accurate clinical rationales, supporting more
reliable disease diagnosis, advancing LLM applications in healthcare, and
narrowing the performance divide between LLMs and SLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Circuit Complexity Bounds for RoPE-based <span class="highlight-title">Transformer</span> Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
tighter circuit complexity bound for Transformers with $\mathsf{RoPE}$
attention. Our key contribution is that we show that unless $\mathsf{TC}^0 =
\mathsf{NC}^1$, a $\mathsf{RoPE}$-based Transformer with
$\mathrm{poly}(n)$-precision, $O(1)$ layers, hidden dimension $d \leq O(n)$
cannot solve the arithmetic problem or the Boolean formula value problem. This
result significantly demonstrates the fundamental limitation of the
expressivity of the $\mathsf{RoPE}$-based Transformer architecture, although it
achieves giant empirical success. Our theoretical framework not only
establishes tighter complexity bounds but also may instruct further work on the
$\mathsf{RoPE}$-based Transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring
  Conversations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rose E. Wang, Pawan Wirawarn, Kenny Lam, Omar Khattab, Dorottya Demszky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many open-ended conversations (e.g., tutoring lessons or business meetings)
revolve around pre-defined reference materials, like worksheets or meeting
bullets. To provide a framework for studying such conversation structure, we
introduce Problem-Oriented Segmentation & Retrieval (POSR), the task of jointly
breaking down conversations into segments and linking each segment to the
relevant reference item. As a case study, we apply POSR to education where
effectively structuring lessons around problems is critical yet difficult. We
present LessonLink, the first dataset of real-world tutoring lessons, featuring
3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT
math problems. We define and evaluate several joint and independent approaches
for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT),
and large language models (LLMs) methods. Our results highlight that modeling
POSR as one joint task is essential: POSR methods outperform independent
segmentation and retrieval pipelines by up to +76% on joint metrics and surpass
traditional segmentation methods by up to +78% on segmentation metrics. We
demonstrate POSR's practical impact on downstream education applications,
deriving new insights on the language and time use in real-world lesson
structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Findings. Our code and dataset are open-sourced at
  https://github.com/rosewang2008/posr</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy Controllable Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motoki Omura, Yasuhiro Fujita, Toshiki Kataoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Language <span class="highlight-title">Prompt</span>ing to Ease False Positives in Medical
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Myung Jin Kim, Hyeong Seok Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A pre-trained visual-language model, contrastive language-image pre-training
(CLIP), successfully accomplishes various downstream tasks with text prompts,
such as finding images or localizing regions within the image. Despite CLIP's
strong multi-modal data capabilities, it remains limited in specialized
environments, such as medical applications. For this purpose, many CLIP
variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives
related to normal regions persist. Thus, we aim to present a simple yet
important goal of reducing false positives in medical anomaly detection. We
introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both
positive and negative text prompts. This straightforward approach identifies
potential lesion regions by visual attention to the positive prompts in the
given image. To reduce false positives, we attenuate attention on normal
regions using negative prompts. Extensive experiments with the BMAD dataset,
including six biomedical benchmarks, demonstrate that CLAP method enhances
anomaly detection performance. Our future plans include developing an automated
fine prompting method for more practical usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Neurolinguistic Subjects: Identifying Internal
  Representations for Form and Meaning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyang He, Ercong Nie, Helmut Schmid, Hinrich Schütze, Nima Mesgarani, Jonathan Brennan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the linguistic understanding of Large Language Models
(LLMs) regarding signifier (form) and signified (meaning) by distinguishing two
LLM evaluation paradigms: psycholinguistic and neurolinguistic. Traditional
psycholinguistic evaluations often reflect statistical biases that may
misrepresent LLMs' true linguistic capabilities. We introduce a neurolinguistic
approach, utilizing a novel method that combines minimal pair and diagnostic
probing to analyze activation patterns across model layers. This method allows
for a detailed examination of how LLMs represent form and meaning, and whether
these representations are consistent across languages. Our contributions are
three-fold: (1) We compare neurolinguistic and psycholinguistic methods,
revealing distinct patterns in LLM assessment; (2) We demonstrate that LLMs
exhibit higher competence in form compared to meaning, with the latter largely
correlated to the former; (3) We present new conceptual minimal pair datasets
for Chinese (COMPS-ZH) and German (COMPS-DE), complementing existing English
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecEncoder: Logs are All You Need in Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Fatih Bulut, Yingqi Liu, Naveed Ahmad, Maximilian Turner, Sami Ait Ouahmane, Cameron Andrews, Lloyd Greenwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large and Small Language Models (LMs) are typically pretrained using
extensive volumes of text, which are sourced from publicly accessible platforms
such as Wikipedia, Book Corpus, or through web scraping. These models, due to
their exposure to a wide range of language data, exhibit impressive
generalization capabilities and can perform a multitude of tasks
simultaneously. However, they often fall short when it comes to domain-specific
tasks due to their broad training data. This paper introduces SecEncoder, a
specialized small language model that is pretrained using security logs.
SecEncoder is designed to address the domain-specific limitations of general
LMs by focusing on the unique language and patterns found in security logs.
Experimental results indicate that SecEncoder outperforms other LMs, such as
BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)
models, which are pretrained mainly on natural language, across various tasks.
Furthermore, although SecEncoder is primarily pretrained on log data, it
outperforms models pretrained on natural language for a range of tasks beyond
log analysis, such as incident prioritization and threat intelligence document
retrieval. This suggests that domain specific pretraining with logs can
significantly enhance the performance of LMs in security. These findings pave
the way for future research into security-specific LMs and their potential
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-enhanced Network for Hateful Meme Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxi Liu, Yanyan Feng, Jiehai Chen, Yun Xue, Fenghuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamic expansion of social media has led to an inundation of hateful
memes on media platforms, accentuating the growing need for efficient
identification and removal. Acknowledging the constraints of conventional
multimodal hateful meme classification, which heavily depends on external
knowledge and poses the risk of including irrelevant or redundant content, we
developed Pen -- a prompt-enhanced network framework based on the prompt
learning approach. Specifically, after constructing the sequence through the
prompt method and encoding it with a language model, we performed region
information global extraction on the encoded sequence for multi-view
perception. By capturing global information about inference instances and
demonstrations, Pen facilitates category selection by fully leveraging sequence
information. This approach significantly improves model classification
accuracy. Additionally, to bolster the model's reasoning capabilities in the
feature space, we introduced prompt-aware contrastive learning into the
framework to improve the quality of sample feature distributions. Through
extensive ablation experiments on two public datasets, we evaluate the
effectiveness of the Pen framework, concurrently comparing it with
state-of-the-art model baselines. Our research findings highlight that Pen
surpasses manual prompt methods, showcasing superior generalization and
classification accuracy in hateful meme classification tasks. Our code is
available at https://github.com/juszzi/Pen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the Thirty-Third International Joint
  Conference on Artificial Intelligence Main Track. Pages 6397-6405</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Summarization: Bridging Quality and Diversity in Extractive
  Summaries <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Bagheri Nezhad, Sayan Bandyapadhyay, Ameeta Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in multi-document summarization of user-generated content remains a
critical challenge in natural language processing (NLP). Existing summarization
methods often fail to ensure equitable representation across different social
groups, leading to biased outputs. In this paper, we introduce two novel
methods for fair extractive summarization: FairExtract, a clustering-based
approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.
We evaluate these methods using Divsumm summarization dataset of White-aligned,
Hispanic, and African-American dialect tweets and compare them against relevant
baselines. The results obtained using a comprehensive set of summarization
quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well
as a fairness metric F, demonstrate that FairExtract and FairGPT achieve
superior fairness while maintaining competitive summarization quality.
Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that
integrate quality and fairness into a single evaluation framework, offering a
more nuanced understanding of the trade-offs between these objectives. This
work highlights the importance of fairness in summarization and sets a
benchmark for future research in fairness-aware NLP models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Algorithmic Fairness through the Lens of Metrics and
  Evaluation Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparrowVQE: Visual Question Explanation for Course Content Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialu Li, Manish Kumar Thota, Ruslan Gokhman, Radek Holik, Youshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) research seeks to create AI systems to answer
natural language questions in images, yet VQA methods often yield overly
simplistic and short answers. This paper aims to advance the field by
introducing Visual Question Explanation (VQE), which enhances the ability of
VQA to provide detailed explanations rather than brief responses and address
the need for more complex interaction with visual content. We first created an
MLVQE dataset from a 14-week streamed video machine learning course, including
885 slide images, 110,407 words of transcripts, and 9,416 designed
question-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3
billion parameters multimodal model. We trained our model with a three-stage
training mechanism consisting of multimodal pre-training (slide images and
transcripts feature alignment), instruction tuning (tuning the pre-trained
model with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide
image and QA pairs). Eventually, our SparrowVQE can understand and connect
visual information using the SigLIP model with transcripts using the Phi-2
language model with an MLP adapter. Experimental results demonstrate that our
SparrowVQE achieves better performance in our developed MLVQE dataset and
outperforms state-of-the-art methods in the other five benchmark VQA datasets.
The source code is available at
\url{https://github.com/YoushanZhang/SparrowVQE}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rapid Response: Mitigating LLM Jailbreaks with a Few Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alwin Peng, Julian Michael, Henry Sleight, Ethan Perez, Mrinank Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) grow more powerful, ensuring their safety
against misuse becomes crucial. While researchers have focused on developing
robust defenses, no method has yet achieved complete invulnerability to
attacks. We propose an alternative approach: instead of seeking perfect
adversarial robustness, we develop rapid response techniques to look to block
whole classes of jailbreaks after observing only a handful of attacks. To study
this setting, we develop RapidResponseBench, a benchmark that measures a
defense's robustness against various jailbreak strategies after adapting to a
few observed examples. We evaluate five rapid response methods, all of which
use jailbreak proliferation, where we automatically generate additional
jailbreaks similar to the examples observed. Our strongest method, which
fine-tunes an input classifier to block proliferated jailbreaks, reduces attack
success rate by a factor greater than 240 on an in-distribution set of
jailbreaks and a factor greater than 15 on an out-of-distribution set, having
observed just one example of each jailbreaking strategy. Moreover, further
studies suggest that the quality of proliferation model and number of
proliferated examples play an key role in the effectiveness of this defense.
Overall, our results highlight the potential of responding rapidly to novel
jailbreaks to limit LLM misuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Evaluation of Syntactic Knowledge in Multilingual Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daria Kryvosheieva, Roger Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are capable of acquiring elements of human-like
syntactic knowledge. Targeted syntactic evaluation tests have been employed to
measure how well they form generalizations about syntactic phenomena in
high-resource languages such as English. However, we still lack a thorough
understanding of LMs' capacity for syntactic generalizations in low-resource
languages, which are responsible for much of the diversity of syntactic
patterns worldwide. In this study, we develop targeted syntactic evaluation
tests for three low-resource languages (Basque, Hindi, and Swahili) and use
them to evaluate five families of open-access multilingual Transformer LMs. We
find that some syntactic tasks prove relatively easy for LMs while others
(agreement in sentences containing indirect objects in Basque, agreement across
a prepositional phrase in Swahili) are challenging. We additionally uncover
issues with publicly available Transformers, including a bias toward the
habitual aspect in Hindi in multilingual BERT and underperformance compared to
similar-sized models in XGLM-4.5B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kawshik Manikantan, Makarand Tapaswi, Vineet Gandhi, Shubham Toshniwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent evaluations of LLMs on coreference resolution have revealed that
traditional output formats and evaluation metrics do not fully capture the
models' referential understanding. To address this, we introduce IdentifyMe, a
new benchmark for mention resolution presented in a multiple-choice question
(MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long
narratives and employs heuristics to exclude easily identifiable mentions,
creating a more challenging task. The benchmark also consists of a curated
mixture of different mention types and corresponding entities, allowing for a
fine-grained analysis of model performance. We evaluate both closed- and open
source LLMs on IdentifyMe and observe a significant performance gap (20-30%)
between the state-of-the-art sub-10B open models vs. closed ones. We observe
that pronominal mentions, which have limited surface information, are typically
much harder for models to resolve than nominal mentions. Additionally, we find
that LLMs often confuse entities when their mentions overlap in nested
structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy,
highlighting the strong referential capabilities of state-of-the-art LLMs while
also indicating room for further improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating
  Machine Learning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Gandhi, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in diverse applications including
generation of code snippets, but often struggle with generating code for
complex Machine Learning (ML) tasks. Although existing LLM single-agent based
systems give varying performance depending on the task complexity, they purely
rely on larger and expensive models such as GPT-4. Our investigation reveals
that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama
perform far worse than GPT-4 in a single-agent setting. With the motivation of
developing a cost-efficient LLM based solution for solving ML tasks, we propose
an LLM Multi-Agent based system which leverages combination of experts using
profiling, efficient retrieval of past observations, LLM cascades, and
ask-the-expert calls. Through empirical analysis on ML engineering tasks in the
MLAgentBench benchmark, we demonstrate the effectiveness of our system, using
no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and
expert to serve occasional ask-the-expert calls for planning. With 94.2\%
reduction in the cost (from \$0.931 per run cost averaged over all tasks for
GPT-4 single agent system to \$0.054), our system is able to yield better
average success rate of 32.95\% as compared to GPT-4 single-agent system
yielding 22.72\% success rate averaged over all the tasks of MLAgentBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at AIMLSystems '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deco<span class="highlight-title">Prompt</span> : Decoding <span class="highlight-title">Prompt</span>s Reduces Hallucinations when Large Language
  Models Meet False Premises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Xu, Xuezhe Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have demonstrated increasing power, they
have also called upon studies on their hallucinated outputs that deviate from
factually correct statements. In this paper, we focus on one important scenario
of false premises, where LLMs are distracted by misaligned claims although the
model possesses the required factual knowledge to answer original questions
accurately. Inspired by the observation that entropy of the false-premise
prompt is closely related to its likelihood to elicit hallucination generation,
we propose a new prompting algorithm, named DecoPrompt, to mitigate
hallucination. DecoPrompt leverages LLMs to "decode" the false-premise prompts
without really eliciting hallucination output from LLMs. We perform experiments
on two datasets, demonstrating that DecoPrompt can reduce hallucinations
effectively on outputs from different LLMs. Moreover, DecoPrompt exhibits
cross-model transferability, which facilitates its applications to scenarios
such as LLMs of large sizes or unavailable model logits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Accurate <span class="highlight-title">Prompt</span> Optimization: the Benefit of Memory in
  Exemplar-Guided Reflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao, Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang Kang, Yangyang Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic prompt engineering aims to enhance the generation quality of large
language models (LLMs). Recent works utilize feedbacks generated from erroneous
cases to guide the prompt optimization. During inference, they may further
retrieve several semantically-related exemplars and concatenate them to the
optimized prompts to improve the performance. However, those works only utilize
the feedback at the current step, ignoring historical and unseleccted feedbacks
which are potentially beneficial. Moreover, the selection of exemplars only
considers the general semantic relationship and may not be optimal in terms of
task performance and matching with the optimized prompt. In this work, we
propose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize
more efficient and accurate prompt optimization. Specifically, we design an
exemplar-guided reflection mechanism where the feedback generation is
additionally guided by the generated exemplars. We further build two kinds of
memory to fully utilize the historical feedback information and support more
effective exemplar retrieval. Empirical evaluations show our method surpasses
previous state-of-the-arts with less optimization steps, i.e., improving F1
score by 10.1 on LIAR dataset, and reducing half of the optimization steps on
ProTeGi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plausible Extractive Rationalization through Semi-Supervised Entailment
  Signal <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08479v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08479v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jie Yeo, Ranjan Satapathy, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of complex and opaque black box models requires the
adoption of interpretable measures, one such option is extractive rationalizing
models, which serve as a more interpretable alternative. These models, also
known as Explain-Then-Predict models, employ an explainer model to extract
rationales and subsequently condition the predictor with the extracted
information. Their primary objective is to provide precise and faithful
explanations, represented by the extracted rationales. In this paper, we take a
semi-supervised approach to optimize for the plausibility of extracted
rationales. We adopt a pre-trained natural language inference (NLI) model and
further fine-tune it on a small set of supervised rationales ($10\%$). The NLI
predictor is leveraged as a source of supervisory signals to the explainer via
entailment alignment. We show that, by enforcing the alignment agreement
between the explanation and answer in a question-answering task, the
performance can be improved without access to ground truth labels. We evaluate
our approach on the ERASER dataset and show that our approach achieves
comparable results with supervised extractive models and outperforms
unsupervised approaches by $> 100\%$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-training Large Language Models through Knowledge Detection <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jie Yeo, Teddy Ferdinan, Przemyslaw Kazienko, Ranjan Satapathy, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often necessitate extensive labeled datasets and
training compute to achieve impressive performance across downstream tasks.
This paper explores a self-training paradigm, where the LLM autonomously
curates its own labels and selectively trains on unknown data samples
identified through a reference-free consistency method. Empirical evaluations
demonstrate significant improvements in reducing hallucination in generation
across multiple subjects. Furthermore, the selective training framework
mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing
a critical limitation in training LLMs. Our findings suggest that such an
approach can substantially reduce the dependency on large labeled datasets,
paving the way for more scalable and cost-effective language model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Language Model Factuality via Activation-Based Confidence
  Calibration and Guided Decoding <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Liu, Farima Fatahi Bayat, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calibrating language models (LMs) aligns their generation confidence with the
actual likelihood of answer correctness, which can inform users about LMs'
reliability and mitigate hallucinated content. However, prior calibration
methods, such as self-consistency-based and logit-based approaches, are either
limited in inference-time efficiency or fall short of providing informative
signals. Moreover, simply filtering out low-confidence responses reduces the
LM's helpfulness when the answers are correct. Therefore, effectively using
calibration techniques to enhance an LM's factuality remains an unsolved
challenge. In this paper, we first propose an activation-based calibration
method, ActCab, which trains a linear layer on top of the LM's last-layer
activations that can better capture the representations of knowledge. Built on
top of ActCab, we further propose CoDec, a confidence-guided decoding strategy
to elicit truthful answers with high confidence from LMs. By evaluating on five
popular QA benchmarks, ActCab achieves superior calibration performance than
all competitive baselines, e.g., by reducing the average expected calibration
error (ECE) score by up to 39%. Further experiments on CoDec show consistent
improvements in several LMs' factuality on challenging QA datasets, such as
TruthfulQA, highlighting the value of confidence signals in enhancing
factuality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting User Comments for Early Detection of Fake News Prior to
  Users' Commenting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10429v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10429v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Nan, Qiang Sheng, Juan Cao, Yongchun Zhu, Danding Wang, Guang Yang, Jintao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both accuracy and timeliness are key factors in detecting fake news on social
media. However, most existing methods encounter an accuracy-timeliness dilemma:
Content-only methods guarantee timeliness but perform moderately because of
limited available information, while social con-text-based ones generally
perform better but inevitably lead to latency because of social context
accumulation needs. To break such a dilemma, a feasible but not well-studied
solution is to leverage social contexts (e.g., comments) from historical news
for training a detection model and apply it to newly emerging news without
social contexts. This requires the model to (1) sufficiently learn helpful
knowledge from social contexts, and (2) be well compatible with situations that
social contexts are available or not. To achieve this goal, we propose to
absorb and parameterize useful knowledge from comments in historical news and
then inject it into a content-only detection model. Specifically, we design the
Comments ASsisted FakE News Detection method (CAS-FEND), which transfers useful
knowledge from a comment-aware teacher model to a content-only student model
and detects newly emerging news with the student model. Experiments show that
the CAS-FEND student model outperforms all content-only methods and even
comment-aware ones with 1/4 comments as inputs, demonstrating its superiority
for early detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 7 tables. The article has been accepted by
  Frontiers of Computer Science (FCS), with the DOI:
  {10.1007/s11704-024-40674-6}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Do Large Language Models Acquire Factual Knowledge During
  <span class="highlight-title">Pretrain</span>ing? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent observation that large language models (LLMs) can store
substantial factual knowledge, there is a limited understanding of the
mechanisms of how they acquire factual knowledge through pretraining. This work
addresses this gap by studying how LLMs acquire factual knowledge during
pretraining. The findings reveal several important insights into the dynamics
of factual knowledge acquisition during pretraining. First, counterintuitively,
we observe that pretraining on more data shows no significant improvement in
the model's capability to acquire and maintain factual knowledge. Next, there
is a power-law relationship between training steps and forgetting of
memorization and generalization of factual knowledge, and LLMs trained with
duplicated training data exhibit faster forgetting. Third, training LLMs with
larger batch sizes can enhance the models' robustness to forgetting. Overall,
our observations suggest that factual knowledge acquisition in LLM pretraining
occurs by progressively increasing the probability of factual knowledge
presented in the pretraining data at each step. However, this increase is
diluted by subsequent forgetting. Based on this interpretation, we demonstrate
that we can provide plausible explanations for recently observed behaviors of
LLMs, such as the poor performance of LLMs on long-tail knowledge and the
benefits of deduplicating the pretraining corpus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Multi-Task Learning Architecture for Hate Detection Leveraging
  User-Based Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Kapil, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech, offensive language, aggression, racism, sexism, and other
abusive language are common phenomena in social media. There is a need for
Artificial Intelligence(AI)based intervention which can filter hate content at
scale. Most existing hate speech detection solutions have utilized the features
by treating each post as an isolated input instance for the classification.
This paper addresses this issue by introducing a unique model that improves
hate speech identification for the English language by utilising intra-user and
inter-user-based information. The experiment is conducted over single-task
learning (STL) and multi-task learning (MTL) paradigms that use deep neural
networks, such as convolutional neural networks (CNN), gated recurrent unit
(GRU), bidirectional encoder representations from the transformer (BERT), and A
Lite BERT (ALBERT). We use three benchmark datasets and conclude that combining
certain user features with textual features gives significant improvements in
macro-F1 and weighted-F1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, and two tables. Accepted at the 20th International
  Conference on Natural Language Processing (ICON) 2023.
  https://aclanthology.org/2023.icon-1.53</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Early FIRST Reproduction and Improvements to Single-Token Decoding
  for Fast Listwise Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Chen, Ronak Pradeep, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have demonstrated that large language models (LLMs) excel as
listwise rerankers, but their high computational demands remain a barrier to
widespread adoption. Further, the traditional language modeling (LM) objective
is not ideally suited for reranking tasks. FIRST is a novel approach that
addresses these challenges by integrating a learning-to-rank objective and
leveraging the logits of only the first generated token, thereby significantly
reducing inference latency compared to traditional LLM rerankers. In this
study, we extend the evaluation of FIRST to the TREC Deep Learning datasets
(DL19-22), validating its robustness across diverse domains. We investigate the
influence of different first-stage retrievers on FIRST rerankers, observing
diminishing returns and patterns consistent with traditional LLM rerankers.
Through applying the FIRST objective to a broader range of backbone models, we
achieve effectiveness surpassing the original implementation. Our experiments
confirm that fast reranking with single-token logits does not compromise
out-of-domain reranking quality. To better quantify the computational savings
in the original study, we measure and compare latency to find a 21%-42% gain
across various models and benchmarks. Moreover, while LM training implicitly
improves zero-shot single-token reranking, our experiments also raise questions
about whether LM pre-training may hinder subsequent fine-tuning with the FIRST
objective. These findings pave the way for more efficient and effective
listwise reranking in future applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Can Evolve Continually on Modality for X-Modal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazuo Yu, Haomiao Xiong, Lu Zhang, Haiwen Diao, Yunzhi Zhuge, Lanqing Hong, Dong Wang, Huchuan Lu, You He, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have gained significant attention
due to their impressive capabilities in multimodal understanding. However,
existing methods rely heavily on extensive modal-specific pretraining and
joint-modal tuning, leading to significant computational burdens when expanding
to new modalities. In this paper, we propose PathWeave, a flexible and scalable
framework with modal-Path sWitching and ExpAnsion abilities that enables MLLMs
to continually EVolve on modalities for $\mathbb{X}$-modal reasoning. We
leverage the concept of Continual Learning and develop an incremental training
strategy atop pre-trained MLLMs, enabling their expansion to new modalities
using uni-modal data, without executing joint-modal pretraining. In detail, a
novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and
cross-modal adapters are seamlessly integrated to facilitate efficient modality
alignment and collaboration. Additionally, an MoE-based gating module is
applied between two types of adapters to further enhance the multimodal
interaction. To investigate the proposed method, we establish a challenging
benchmark called Continual Learning of Modality (MCL), which consists of
high-quality QA data from five distinct modalities: image, video, audio, depth
and point cloud. Extensive experiments demonstrate the effectiveness of the
proposed AnA framework on learning plasticity and memory stability during
continual learning. Furthermore, PathWeave performs comparably to
state-of-the-art MLLMs while concurrently reducing parameter training burdens
by 98.73%. Our code locates at https://github.com/JiazuoYu/PathWeave
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Dark Patterns of Personalized Persuasion in Large Language Models:
  Exposing Persuasive Linguistic Features for Big Five Personality Traits in
  LLMs Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wiktoria Mieleszczenko-Kowszewicz, Dawid Płudowski, Filip Kołodziejczyk, Jakub Świstak, Julian Sienkiewicz, Przemysław Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores how the Large Language Models (LLMs) adjust linguistic
features to create personalized persuasive outputs. While research showed that
LLMs personalize outputs, a gap remains in understanding the linguistic
features of their persuasive capabilities. We identified 13 linguistic features
crucial for influencing personalities across different levels of the Big Five
model of personality. We analyzed how prompts with personality trait
information influenced the output of 19 LLMs across five model families. The
findings show that models use more anxiety-related words for neuroticism,
increase achievement-related words for conscientiousness, and employ fewer
cognitive processes words for openness to experience. Some model families excel
at adapting language for openness to experience, others for conscientiousness,
while only one model adapts language for neuroticism. Our findings show how
LLMs tailor responses based on personality cues in prompts, indicating their
potential to create persuasive content affecting the mind and well-being of the
recipients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient LLM Comparative Assessment: a Product of Experts Framework for
  Pairwise Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark Gales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-judge approaches are a practical and effective way of assessing a
range of text tasks. However, when using pairwise comparisons to rank a set of
candidates, the computational cost scales quadratically with the number of
candidates, which has practical limitations. This paper introduces a Product of
Expert (PoE) framework for efficient LLM Comparative Assessment. Here
individual comparisons are considered experts that provide information on a
pair's score difference. The PoE framework combines the information from these
experts to yield an expression that can be maximized with respect to the
underlying set of candidates, and is highly flexible where any form of expert
can be assumed. When Gaussian experts are used one can derive simple
closed-form solutions for the optimal candidate ranking, and expressions for
selecting which comparisons should be made to maximize the probability of this
ranking. Our approach enables efficient comparative assessment, where by using
only a small subset of the possible comparisons, one can generate score
predictions that correlate well with human judgements. We evaluate the approach
on multiple NLG tasks and demonstrate that our framework can yield considerable
computational savings when performing pairwise comparative assessment. With
many candidate texts, using as few as 2% of comparisons the PoE solution can
achieve similar performance to when all comparisons are used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Qwen2.5-Coder Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we introduce the Qwen2.5-Coder series, a significant upgrade
from its predecessor, CodeQwen1.5. This series includes six models:
Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model,
Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained
on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,
scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder
demonstrates impressive code generation capabilities while retaining general
and math skills. These models have been evaluated on a wide range of
code-related tasks, achieving state-of-the-art (SOTA) performance across more
than 10 benchmarks, including code generation, completion, reasoning, and
repair, consistently outperforming larger models of the same model size. We
believe that the release of the Qwen2.5-Coder series will advance research in
code intelligence and, with its permissive licensing, support wider adoption by
developers in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kwai-STaR: Transform LLMs into State-Transition Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Lu, Yuhang Hu, Changyi Liu, Tianke Zhang, Zhenyu Yang, Zhixiang Ding, Shengsheng Qian, Meng Du, Ruiwen Kang, Kaiyu Tang, Fan Yang, Tingting Gao, Di Zhang, Hai-Tao Zheng, Bin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning presents a significant challenge to the cognitive
capabilities of LLMs. Various methods have been proposed to enhance the
mathematical ability of LLMs. However, few recognize the value of state
transition for LLM reasoning. In this work, we define mathematical
problem-solving as a process of transiting from an initial unsolved state to
the final resolved state, and propose Kwai-STaR framework, which transforms
LLMs into State-Transition Reasoners to improve their intuitive reasoning
capabilities. Our approach comprises three main steps: (1) Define the state
space tailored to the mathematical reasoning. (2) Generate state-transition
data based on the state space. (3) Convert original LLMs into State-Transition
Reasoners via a curricular training strategy. Our experiments validate the
effectiveness of Kwai-STaR in enhancing mathematical reasoning: After training
on the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and
LLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard
dataset. Additionally, the state transition-based design endows Kwai-STaR with
remarkable training and inference efficiency. Further experiments are underway
to establish the generality of Kwai-STaR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs for Generating and Evaluating Counterfactuals: A Comprehensive
  Study <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Bach Nguyen, Paul Youssef, Christin Seifert, Jörg Schlötterer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As NLP models become more complex, understanding their decisions becomes more
crucial. Counterfactuals (CFs), where minimal changes to inputs flip a model's
prediction, offer a way to explain these models. While Large Language Models
(LLMs) have shown remarkable performance in NLP tasks, their efficacy in
generating high-quality CFs remains uncertain. This work fills this gap by
investigating how well LLMs generate CFs for two NLU tasks. We conduct a
comprehensive comparison of several common LLMs, and evaluate their CFs,
assessing both intrinsic metrics, and the impact of these CFs on data
augmentation. Moreover, we analyze differences between human and LLM-generated
CFs, providing insights for future research directions. Our results show that
LLMs generate fluent CFs, but struggle to keep the induced changes minimal.
Generating CFs for Sentiment Analysis (SA) is less challenging than NLI where
LLMs show weaknesses in generating CFs that flip the original label. This also
reflects on the data augmentation performance, where we observe a large gap
between augmenting with human and LLMs CFs. Furthermore, we evaluate LLMs'
ability to assess CFs in a mislabelled data setting, and show that they have a
strong bias towards agreeing with the provided labels. GPT4 is more robust
against this bias and its scores correlate well with automatic metrics. Our
findings reveal several limitations and point to potential future work
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLHF Workflow: From Reward Modeling to Online RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the workflow of Online Iterative Reinforcement Learning from Human
Feedback (RLHF) in this technical report, which is widely reported to
outperform its offline counterpart by a large margin in the recent large
language model (LLM) literature. However, existing open-source RLHF projects
are still largely confined to the offline learning setting. In this technical
report, we aim to fill in this gap and provide a detailed recipe that is easy
to reproduce for online iterative RLHF. In particular, since online human
feedback is usually infeasible for open-source communities with limited
resources, we start by constructing preference models using a diverse set of
open-source datasets and use the constructed proxy preference model to
approximate human feedback. Then, we discuss the theoretical insights and
algorithmic principles behind online iterative RLHF, followed by a detailed
practical implementation. Our trained LLM achieves impressive performance on
LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as
well as other academic benchmarks such as HumanEval and TruthfulQA. We have
shown that supervised fine-tuning (SFT) and iterative RLHF can obtain
state-of-the-art performance with fully open-source datasets. Further, we have
made our models, curated datasets, and comprehensive step-by-step code
guidebooks publicly available. Please refer to
https://github.com/RLHFlow/RLHF-Reward-Modeling and
https://github.com/RLHFlow/Online-RLHF for more detailed information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (09/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeKUBE: A Legal Knowledge Update BEnchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyue Wang, Weihang Su, Hu Yiran, Qingyao Ai, Yueyue Wu, Cheng Luo, Yiqun Liu, Min Zhang, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have significantly shaped the
applications of AI in multiple fields, including the studies of legal
intelligence. Trained on extensive legal texts, including statutes and legal
documents, the legal LLMs can capture important legal knowledge/concepts
effectively and provide important support for downstream legal applications
such as legal consultancy. Yet, the dynamic nature of legal statutes and
interpretations also poses new challenges to the use of LLMs in legal
applications. Particularly, how to update the legal knowledge of LLMs
effectively and efficiently has become an important research problem in
practice. Existing benchmarks for evaluating knowledge update methods are
mostly designed for the open domain and cannot address the specific challenges
of the legal domain, such as the nuanced application of new legal knowledge,
the complexity and lengthiness of legal regulations, and the intricate nature
of legal reasoning. To address this gap, we introduce the Legal Knowledge
Update BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for
legal LLMs across five dimensions. Specifically, we categorize the needs of
knowledge updates in the legal domain with the help of legal professionals, and
then hire annotators from law schools to create synthetic updates to the
Chinese Criminal and Civil Code as well as sets of questions of which the
answers would change after the updates. Through a comprehensive evaluation of
state-of-the-art knowledge update methods, we reveal a notable gap between
existing knowledge update methods and the unique needs of the legal domain,
emphasizing the need for further research and development of knowledge update
mechanisms tailored for legal LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Advanced Large Language Models with LLMsuite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Roffo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This tutorial explores the advancements and challenges in the development of
Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent
limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the
generation of incorrect information, proposing solutions like Retrieval
Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks
such as ReAct and LangChain. The integration of these techniques enhances LLM
performance and reliability, especially in multi-step reasoning and complex
task execution. The paper also covers fine-tuning strategies, including
instruction fine-tuning, parameter-efficient methods like LoRA, and
Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced
Self-Training (ReST). Additionally, it provides a comprehensive survey of
transformer architectures and training techniques for LLMs. The source code can
be accessed by contacting the author via email for a request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: Language Model Benchmarking, Pre-Trained LLM Comparison,
  LLM Performance Analysis, NLP Model Evaluation Tools, Public Dataset
  Inference for LLMs, BLEU and ROUGE Metrics for LLM, Open Source LLM Testing
  Tools, Large Language Model Evaluation Software, NLP Benchmarking Suite,
  Comprehensive LLM Evaluation Toolkit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmAgent: A Multi-modal Agent Framework for Complex Video Understanding
  with Task Divide-and-Conquer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, Kyusong Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have expanded their
capabilities to multimodal contexts, including comprehensive video
understanding. However, processing extensive videos such as 24-hour CCTV
footage or full-length films presents significant challenges due to the vast
data and processing demands. Traditional methods, like extracting key frames or
converting frames to text, often result in substantial information loss. To
address these shortcomings, we develop OmAgent, efficiently stores and
retrieves relevant video frames for specific queries, preserving the detailed
content of videos. Additionally, it features an Divide-and-Conquer Loop capable
of autonomous reasoning, dynamically invoking APIs and tools to enhance query
processing and accuracy. This approach ensures robust video understanding,
significantly reducing information loss. Experimental results affirm OmAgent's
efficacy in handling various types of videos and complex tasks. Moreover, we
have endowed it with greater autonomy and a robust tool-calling system,
enabling it to accomplish even more intricate tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciDFM: A Large Language Model with Mixture-of-Experts for Science <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18412v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18412v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangtai Sun, Danyu Luo, Da Ma, Zihan Zhao, Baocai Chen, Zhennan Shen, Su Zhu, Lu Chen, Xin Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a significant upsurge of interest in leveraging
large language models (LLMs) to assist scientific discovery. However, most LLMs
only focus on general science, while they lack domain-specific knowledge, such
as chemical molecules and amino acid sequences. To bridge these gaps, we
introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and
is able to conduct college-level scientific reasoning and understand molecules
and amino acid sequences. We collect a large-scale training corpus containing
numerous scientific papers and books from different disciplines as well as data
from domain-specific databases. We further fine-tune the pre-trained model on
lots of instruction data to improve performances on downstream benchmarks. From
experiment results, we show that SciDFM achieves strong performance on general
scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA
performance on domain-specific benchmarks among models of similar size. We
further analyze the expert layers and show that the results of expert selection
vary with data from different disciplines. To benefit the broader research
community, we open-source SciDFM at
https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 9 tables. Technical Report, accepted by NeurIPS
  2024 Workshop FM4Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does the Textual Information Affect the Retrieval of Multimodal
  In-Context Learning? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Luo, Zangwei Zheng, Zirui Zhu, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increase in parameter size of multimodal large language models (MLLMs)
introduces significant capabilities, particularly in-context learning, where
MLLMs enhance task performance without updating pre-trained parameters. This
effectiveness, however, hinges on the appropriate selection of in-context
examples, a process that is currently biased towards visual data, overlooking
textual information. Furthermore, the area of supervised retrievers for MLLMs,
crucial for optimal in-context example selection, continues to be
uninvestigated. Our study offers an in-depth evaluation of the impact of
textual information on the unsupervised selection of in-context examples in
multimodal contexts, uncovering a notable sensitivity of retriever performance
to the employed modalities. Responding to this, we introduce a novel supervised
MLLM-retriever MSIER that employs a neural network to select examples that
enhance multimodal in-context learning efficiency. This approach is validated
through extensive testing across three distinct tasks, demonstrating the
method's effectiveness. Additionally, we investigate the influence of
modalities on our supervised retrieval method's training and pinpoint factors
contributing to our model's success. This exploration paves the way for future
advancements, highlighting the potential for refined in-context learning in
MLLMs through the strategic use of multimodal data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deception Detection from Linguistic and Physiological Data Streams Using
  Bimodal Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10944v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10944v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Mohamed Abouelenien, Rada Mihalcea, Zhicheng Ding, Qikai Yang, Yiming Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deception detection is gaining increasing interest due to ethical and
security concerns. This paper explores the application of convolutional neural
networks for the purpose of multimodal deception detection. We use a dataset
built by interviewing 104 subjects about two topics, with one truthful and one
falsified response from each subject about each topic. In particular, we make
three main contributions. First, we extract linguistic and physiological
features from this data to train and construct the neural network models.
Second, we propose a fused convolutional neural network model using both
modalities in order to achieve an improved overall performance. Third, we
compare our new approach with earlier methods designed for multimodal deception
detection. We find that our system outperforms regular classification methods;
our results indicate the feasibility of using neural networks for deception
detection even in the presence of limited amounts of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Information Science,
  Parallel and Distributed Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SKVQ: Sliding-window Key and Value Cache Quantization for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can now handle longer sequences of tokens,
enabling complex tasks like book understanding and generating lengthy novels.
However, the key-value (KV) cache required for LLMs consumes substantial memory
as context length increasing, becoming the bottleneck for deployment. In this
paper, we present a strategy called SKVQ, which stands for sliding-window KV
cache quantization, to address the issue of extremely low bitwidth KV cache
quantization. To achieve this, SKVQ rearranges the channels of the KV cache in
order to improve the similarity of channels in quantization groups, and applies
clipped dynamic quantization at the group level. Additionally, SKVQ ensures
that the most recent window tokens in the KV cache are preserved with high
precision. This helps maintain the accuracy of a small but important portion of
the KV cache.SKVQ achieves high compression ratios while maintaining accuracy.
Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization
approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit
values with minimal loss of accuracy. With SKVQ, it is possible to process
context lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7
times faster decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Ni, Shuchen Meng, Xupeng Chen, Ziqing Zhao, Andi Chen, Panfeng Li, Shiyao Zhang, Qifu Yin, Yuanqing Wang, Yuxi Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate stock market predictions following earnings reports are crucial for
investors. Traditional methods, particularly classical machine learning models,
struggle with these predictions because they cannot effectively process and
interpret extensive textual data contained in earnings reports and often
overlook nuances that influence market movements. This paper introduces an
advanced approach by employing Large Language Models (LLMs) instruction
fine-tuned with a novel combination of instruction-based techniques and
quantized low-rank adaptation (QLoRA) compression. Our methodology integrates
'base factors', such as financial metric growth and earnings transcripts, with
'external factors', including recent market indices performances and analyst
grades, to create a rich, supervised dataset. This comprehensive dataset
enables our models to achieve superior predictive performance in terms of
accuracy, weighted F1, and Matthews correlation coefficient (MCC), especially
evident in the comparison with benchmarks such as GPT-4. We specifically
highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases
significant improvements over baseline models. The paper also discusses the
potential of expanding the output capabilities to include a 'Hold' option and
extending the prediction horizon, aiming to accommodate various investment
styles and time frames. This study not only demonstrates the power of
integrating cutting-edge AI with fine-tuned financial data but also paves the
way for future research in enhancing AI-driven financial analysis tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 6th International Conference on Data-driven
  Optimization of Complex Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MASIVE: Open-Ended Affective State Identification in English and Spanish <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Deas, Elsbeth Turcan, Iván Pérez Mejía, Kathleen McKeown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of emotion analysis, much NLP research focuses on identifying a
limited number of discrete emotion categories, often applied across languages.
These basic sets, however, are rarely designed with textual data in mind, and
culture, language, and dialect can influence how particular emotions are
interpreted. In this work, we broaden our scope to a practically unbounded set
of \textit{affective states}, which includes any terms that humans use to
describe their experiences of feeling. We collect and publish MASIVE, a dataset
of Reddit posts in English and Spanish containing over 1,000 unique affective
states each. We then define the new problem of \textit{affective state
identification} for language generation models framed as a masked span
prediction task. On this task, we find that smaller finetuned multilingual
models outperform much larger LLMs, even on region-specific Spanish affective
states. Additionally, we show that pretraining on MASIVE improves model
performance on existing emotion benchmarks. Finally, through machine
translation experiments, we find that native speaker-written data is vital to
good performance on this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Diverse Methods in Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Qikai Yang, Xieming Geng, Wenjing Zhou, Zhicheng Ding, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores innovative methods for improving Visual Question
Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and
attention mechanisms. Leveraging a balanced VQA dataset, we investigate three
distinct strategies. Firstly, GAN-based approaches aim to generate answer
embeddings conditioned on image and question inputs, showing potential but
struggling with more complex tasks. Secondly, autoencoder-based techniques
focus on learning optimal embeddings for questions and images, achieving
comparable results with GAN due to better ability on complex questions. Lastly,
attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB),
address language priors and attention modeling, albeit with a
complexity-performance trade-off. This study underscores the challenges and
opportunities in VQA and suggests avenues for future research, including
alternative GAN formulations and attentional mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Electronic
  Communication and Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAMP: A Language Model on the Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasquale Balsebre, Weiming Huang, Gao Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are poised to play an increasingly important
role in our lives, providing assistance across a wide array of tasks. In the
geospatial domain, LLMs have demonstrated the ability to answer generic
questions, such as identifying a country's capital; nonetheless, their utility
is hindered when it comes to answering fine-grained questions about specific
places, such as grocery stores or restaurants, which constitute essential
aspects of people's everyday lives. This is mainly because the places in our
cities haven't been systematically fed into LLMs, so as to understand and
memorize them. This study introduces a novel framework for fine-tuning a
pre-trained model on city-specific data, to enable it to provide accurate
recommendations, while minimizing hallucinations. We share our model, LAMP, and
the data used to train it. We conduct experiments to analyze its ability to
correctly retrieving spatial objects, and compare it to well-known open- and
closed- source language models, such as GPT-4. Finally, we explore its emerging
capabilities through a case study on day planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Game-theoretic LLM: Agent Workflow for Negotiation Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Ollie Liu, Lingyao Li, Alfonso Amayuelas, Julie Chen, Lucas Jiang, Mingyu Jin, Lizhou Fan, Fei Sun, William Wang, Xintong Wang, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the rationality of large language models (LLMs) in
strategic decision-making contexts, specifically within the framework of game
theory. We evaluate several state-of-the-art LLMs across a spectrum of
complete-information and incomplete-information games. Our findings reveal that
LLMs frequently deviate from rational strategies, particularly as the
complexity of the game increases with larger payoff matrices or deeper
sequential trees.
  To address these limitations, we design multiple game-theoretic workflows
that guide the reasoning and decision-making processes of LLMs. These workflows
aim to enhance the models' ability to compute Nash Equilibria and make rational
choices, even under conditions of uncertainty and incomplete information.
Experimental results demonstrate that the adoption of these workflows
significantly improves the rationality and robustness of LLMs in game-theoretic
tasks. Specifically, with the workflow, LLMs exhibit marked improvements in
identifying optimal strategies, achieving near-optimal allocations in
negotiation scenarios, and reducing susceptibility to exploitation during
negotiations. Furthermore, we explore the meta-strategic considerations of
whether it is rational for agents to adopt such workflows, recognizing that the
decision to use or forgo the workflow constitutes a game-theoretic issue in
itself.
  Our research contributes to a deeper understanding of LLMs' decision-making
capabilities in strategic contexts and provides insights into enhancing their
rationality through structured workflows. The findings have implications for
the development of more robust and strategically sound AI agents capable of
navigating complex interactive environments. Code and data supporting this
study are available at \url{https://github.com/Wenyueh/game_theory}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Adaptive Optimization for Effective Sentiment Analysis
  Fine-Tuning on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Ding, Xuanze Zhao, Shamsul Nahar Abdullah, Deshinta Arrova Dewi, Zixiao Jiang, Xiangyu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis plays a crucial role in various domains, such as business
intelligence and financial forecasting. Large language models (LLMs) have
become a popular paradigm for sentiment analysis, leveraging multi-task
learning to address specific tasks concurrently. However, LLMs with fine-tuning
for sentiment analysis often underperforms due to the inherent challenges in
managing diverse task complexities. Moreover, constant-weight approaches in
multi-task learning struggle to adapt to variations in data characteristics,
further complicating model effectiveness. To address these issues, we propose a
novel multi-task learning framework with a dynamic adaptive optimization (DAO)
module. This module is designed as a plug-and-play component that can be
seamlessly integrated into existing models, providing an effective and flexible
solution for multi-task learning. The key component of the DAO module is
dynamic adaptive loss, which dynamically adjusts the weights assigned to
different tasks based on their relative importance and data characteristics
during training. Sentiment analyses on a standard and customized financial text
dataset demonstrate that the proposed framework achieves superior performance.
Specifically, this work improves the Mean Squared Error (MSE) and Accuracy
(ACC) by 15.58% and 1.24% respectively, compared with previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reminding Multimodal Large Language Models of Object-aware Knowledge
  with Retrieved Tags <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiqing Qi, Handong Zhao, Zijun Wei, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in the general visual instruction-following ability
of Multimodal Large Language Models (MLLMs), they still struggle with critical
problems when required to provide a precise and detailed response to a visual
instruction: (1) failure to identify novel objects or entities, (2) mention of
non-existent objects, and (3) neglect of object's attributed details. Intuitive
solutions include improving the size and quality of data or using larger
foundation models. They show effectiveness in mitigating these issues, but at
an expensive cost of collecting a vast amount of new data and introducing a
significantly larger model. Standing at the intersection of these approaches,
we examine the three object-oriented problems from the perspective of the
image-to-text mapping process by the multimodal connector. In this paper, we
first identify the limitations of multimodal connectors stemming from
insufficient training data. Driven by this, we propose to enhance the mapping
with retrieval-augmented tag tokens, which contain rich object-aware
information such as object names and attributes. With our Tag-grounded visual
instruction tuning with retrieval Augmentation (TUNA), we outperform baselines
that share the same language model and training data on 12 benchmarks.
Furthermore, we show the zero-shot capability of TUNA when provided with
specific datastores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main Conference at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLANG: New Concept Comprehension of Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12585v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12585v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamic nature of language, particularly evident in the realm of slang
and memes on the Internet, poses serious challenges to the adaptability of
large language models (LLMs). Traditionally anchored to static datasets, these
models often struggle to keep up with the rapid linguistic evolution
characteristic of online communities. This research aims to bridge this gap by
enhancing LLMs' comprehension of the evolving new concepts on the Internet,
without the high cost of continual retraining. In pursuit of this goal, we
introduce $\textbf{SLANG}$, a benchmark designed to autonomously integrate
novel data and assess LLMs' ability to comprehend emerging concepts, alongside
$\textbf{FOCUS}$, an approach uses causal inference to enhance LLMs to
understand new phrases and their colloquial context. Our benchmark and approach
involves understanding real-world instances of linguistic shifts, serving as
contextual beacons, to form more precise and contextually relevant connections
between newly emerging expressions and their meanings. The empirical analysis
shows that our causal inference-based approach outperforms the baseline methods
in terms of precision and relevance in the comprehension of Internet slang and
memes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMLongBench-Doc: Benchmarking Long-context Document Understanding with
  Visualizations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding documents with rich layouts and multi-modal components is a
long-standing and practical task. Recent Large Vision-Language Models (LVLMs)
have made remarkable strides in various tasks, particularly in single-page
document understanding (DU). However, their abilities on long-context DU remain
an open problem. This work presents MMLongBench-Doc, a long-context,
multi-modal benchmark comprising 1,062 expert-annotated questions. Distinct
from previous datasets, it is constructed upon 130 lengthy PDF-formatted
documents with an average of 49.4 pages and 20,971 textual tokens. Towards
comprehensive evaluation, answers to these questions rely on pieces of evidence
from (1) different sources (text, image, chart, table, and layout structure)
and (2) various locations (i.e. page number). Moreover, 33.2% of the questions
are cross-page questions requiring evidence across multiple pages. 22.8% of the
questions are designed to be unanswerable for detecting potential
hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU
greatly challenges current models. Notably, the best-performing model, GPT-4o,
achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores
31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse
performance than their LLM counterparts which are fed with lossy-parsed OCR
documents. These results validate the necessity of future research toward more
capable long-context LVLMs. Project Page:
https://mayubo2333.github.io/MMLongBench-Doc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Data Distillation for Recovering Quality in Pruned Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vithursan Thangarasa, Ganesh Venkatesh, Mike Lasby, Nish Sinnadurai, Sean Lie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have driven significant progress in natural language
processing, but their deployment requires substantial compute and memory
resources. As models scale, compression techniques become essential for
balancing model quality with computational efficiency. Structured pruning,
which removes less critical components of the model, is a promising strategy
for reducing complexity. However, one-shot pruning often results in significant
quality degradation, particularly in tasks requiring multi-step reasoning. To
recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it
can lead to catastrophic forgetting by shifting the model's learned data
distribution. Therefore, addressing the degradation from both pruning and SFT
is essential to preserve the original model's quality. In this work, we utilize
self-data distilled fine-tuning to address these challenges. Our approach
leverages the original, unpruned model to generate a distilled dataset that
preserves semantic richness and mitigates catastrophic forgetting by
maintaining alignment with the base model's knowledge. Empirically, we
demonstrate that self-data distillation consistently outperforms standard SFT,
improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard
v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct
(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B
parameters), our method retains 91.2% of the original model's accuracy compared
to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,
combining self-data distilled models through model merging yields enhanced
quality retention. Additionally, leveraging these pruned models in speculative
decoding increases token acceptance rates, thereby improving inference
efficiency in applied settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 6 Tables (Main Paper) + 5 pages (Supplementary
  Material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Active Privacy Auditing in Supervised Fine-tuning for White-Box
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Sun, Hanpeng Wu, Xi Sheryl Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretraining and fine-tuning approach has become the leading technique for
various NLP applications. However, recent studies reveal that fine-tuning data,
due to their sensitive nature, domain-specific characteristics, and
identifiability, pose significant privacy concerns. To help develop more
privacy-resilient fine-tuning models, we introduce a novel active privacy
auditing framework, dubbed Parsing, designed to identify and quantify privacy
leakage risks during the supervised fine-tuning (SFT) of language models (LMs).
The framework leverages improved white-box membership inference attacks (MIAs)
as the core technology, utilizing novel learning objectives and a two-stage
pipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the
exposure of privacy risks. Additionally, we have improved the effectiveness of
MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our
research aims to provide the SFT community of LMs with a reliable, ready-to-use
privacy auditing tool, and to offer valuable insights into safeguarding privacy
during the fine-tuning process. Experimental results confirm the framework's
efficiency across various models and tasks, emphasizing notable privacy
concerns in the fine-tuning process. Project code available for
https://anonymous.4open.science/r/PARSING-4817/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stronger Models are NOT Stronger Teachers for Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning has been widely adopted to ensure large language models
(LLMs) follow user instructions effectively. The resulting
instruction-following capabilities of LLMs heavily rely on the instruction
datasets used for tuning. Recently, synthetic instruction datasets have emerged
as an economically viable solution to provide LLMs diverse and high-quality
instructions. However, existing approaches typically assume that larger or
stronger models are stronger teachers for instruction tuning, and hence simply
adopt these models as response generators to the synthetic instructions. In
this paper, we challenge this commonly-adopted assumption. Our extensive
experiments across five base models and twenty response generators reveal that
larger and stronger models are not necessarily stronger teachers of smaller
models. We refer to this phenomenon as the Larger Models' Paradox. We observe
that existing metrics cannot precisely predict the effectiveness of response
generators since they ignore the compatibility between teachers and base models
being fine-tuned. We thus develop a novel metric, named as
Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response
generators. Our experiments across five base models demonstrate that CAR
outperforms almost all baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model
  with Frozen LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, Long Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapidly developing large language models (LLMs) have brought tremendous
intelligent applications. GPT-4o's excellent duplex speech interaction ability
has recently brought impressive experience to users. Researchers have recently
proposed several multi-modal LLMs in this direction that can achieve
speech-to-speech dialogue. This paper proposes a novel speech-text multimodal
LLM architecture called Freeze-Omni. Our main contribution is that the speech
input and output modalities can be easily connected to a textual LLM while
keeping the LLM's parameters frozen throughout the training process. We
designed 3-stage training strategies both for the modeling of speech input and
output, enabling Freeze-Omni to obtain speech-to-speech dialogue ability using
text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round
text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the
intelligence of the Freeze-Omni in the speech modality is at the same level
compared with that in the text modality of its backbone LLM, while the
end-to-end latency of the spoken response achieves a low level. In addition, we
also designed a method to achieve duplex dialogue ability through multi-task
training, making Freeze-Omni have a more natural style of dialogue ability
between the users. Freeze-Omni mainly provides a possibility for researchers to
conduct multimodal LLM under the condition of a frozen LLM, avoiding various
impacts caused by the catastrophic forgetting of LLM caused by fewer data and
training resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://freeze-omni.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation
  Extraction in Long Sentences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Xinyi Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction as an important natural Language processing (NLP) task is
to identify relations between named entities in text. Recently, graph
convolutional networks over dependency trees have been widely used to capture
syntactic features and achieved attractive performance. However, most existing
dependency-based approaches ignore the positive influence of the words outside
the dependency trees, sometimes conveying rich and useful information on
relation extraction. In this paper, we propose a novel model, Entity-aware
Self-attention Contextualized GCN (ESC-GCN), which efficiently incorporates
syntactic structure of input sentences and semantic context of sequences. To be
specific, relative position self-attention obtains the overall semantic
pairwise correlation related to word position, and contextualized graph
convolutional networks capture rich intra-sentence dependencies between words
by adequately pruning operations. Furthermore, entity-aware attention layer
dynamically selects which token is more decisive to make final relation
prediction. In this way, our proposed model not only reduces the noisy impact
from dependency trees, but also obtains easily-ignored entity-related semantic
representation. Extensive experiments on various tasks demonstrate that our
model achieves encouraging performance as compared to existing dependency-based
and sequence-based models. Specially, our model excels in extracting relations
between entities of long sentences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity-Aware Biaffine Attention Model for Improved Constituent Parsing
  with Reduced Entity Violations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constituency parsing involves analyzing a sentence by breaking it into
sub-phrases, or constituents. While many deep neural models have achieved
state-of-the-art performance in this task, they often overlook the
entity-violating issue, where an entity fails to form a complete sub-tree in
the resultant parsing tree. To address this, we propose an entity-aware
biaffine attention model for constituent parsing. This model incorporates
entity information into the biaffine attention mechanism by using additional
entity role vectors for potential phrases, which enhances the parsing accuracy.
We introduce a new metric, the Entity Violating Rate (EVR), to quantify the
extent of entity violations in parsing results. Experiments on three popular
datasets-ONTONOTES, PTB, and CTB-demonstrate that our model achieves the lowest
EVR while maintaining high precision, recall, and F1-scores comparable to
existing models. Further evaluation in downstream tasks, such as sentence
sentiment analysis, highlights the effectiveness of our model and the validity
of the proposed EVR metric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Large Language Models Decisions Using Shapley Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnam Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large language models (LLMs) has opened up exciting
possibilities for simulating human behavior and cognitive processes, with
potential applications in various domains, including marketing research and
consumer behavior analysis. However, the validity of utilizing LLMs as
stand-ins for human subjects remains uncertain due to glaring divergences that
suggest fundamentally different underlying processes at play and the
sensitivity of LLM responses to prompt variations. This paper presents a novel
approach based on Shapley values from cooperative game theory to interpret LLM
behavior and quantify the relative contribution of each prompt component to the
model's output. Through two applications - a discrete choice experiment and an
investigation of cognitive biases - we demonstrate how the Shapley value method
can uncover what we term "token noise" effects, a phenomenon where LLM
decisions are disproportionately influenced by tokens providing minimal
informative content. This phenomenon raises concerns about the robustness and
generalizability of insights obtained from LLMs in the context of human
behavior simulation. Our model-agnostic approach extends its utility to
proprietary LLMs, providing a valuable tool for practitioners and researchers
to strategically optimize prompts and mitigate apparent cognitive biases. Our
findings underscore the need for a more nuanced understanding of the factors
driving LLM responses before relying on them as substitutes for human subjects
in survey settings. We emphasize the importance of researchers reporting
results conditioned on specific prompt templates and exercising caution when
drawing parallels between human behavior and LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Identification of Hate Speech towards Islam using Graph
  Neural Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04916v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04916v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Islamophobic language on online platforms fosters intolerance, making
detection and elimination crucial for promoting harmony. Traditional hate
speech detection models rely on NLP techniques like tokenization,
part-of-speech tagging, and encoder-decoder models. However, Graph Neural
Networks (GNNs), with their ability to utilize relationships between data
points, offer more effective detection and greater explainability. In this
work, we represent speeches as nodes and connect them with edges based on their
context and similarity to develop the graph. This study introduces a novel
paradigm using GNNs to identify and explain hate speech towards Islam. Our
model leverages GNNs to understand the context and patterns of hate speech by
connecting texts via pretrained NLP-generated word embeddings, achieving
state-of-the-art performance and enhancing detection accuracy while providing
valuable explanations. This highlights the potential of GNNs in combating
online hate speech and fostering a safer, more inclusive online environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in: (i) NeurIPS 2023 : Muslims in ML Workshop (Non-archival)
  (https://www.musiml.org/schedule/#:~:text=Azmine%20Toushik%20Wasi) (ii) EMNLP
  2024 : NLP for Positive Impact Workshop (Archival; ACL Anthology:
  https://aclanthology.org/2024.nlp4pi-1.23/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogErgLLM: Exploring Large Language Model Systems Design Perspective
  Using Cognitive Ergonomics <span class="chip">ICML'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02885v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02885v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Mst Rafia Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating cognitive ergonomics with LLMs is crucial for improving safety,
reliability, and user satisfaction in human-AI interactions. Current LLM
designs often lack this integration, resulting in systems that may not fully
align with human cognitive capabilities and limitations. This oversight
exacerbates biases in LLM outputs and leads to suboptimal user experiences due
to inconsistent application of user-centered design principles. Researchers are
increasingly leveraging NLP, particularly LLMs, to model and understand human
behavior across social sciences, psychology, psychiatry, health, and
neuroscience. Our position paper explores the need to integrate cognitive
ergonomics into LLM design, providing a comprehensive framework and practical
guidelines for ethical development. By addressing these challenges, we aim to
advance safer, more reliable, and ethically sound human-AI interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Page, 3 Figures. Accepted in: (i) ICML'24: LLMs & Cognition
  Workshop (Non-archival; OpenReview:
  https://openreview.net/forum?id=63C9YSc77p) (ii) EMNLP'24 : NLP for Science
  Workshop (Archival; ACL Anthology:
  https://aclanthology.org/2024.nlp4science-1.22/)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">134</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Material Transforms from Disentangled NeRF Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Lopes, Jean-François Lalonde, Raoul de Charette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we first propose a novel method for transferring material
transformations across different scenes. Building on disentangled Neural
Radiance Field (NeRF) representations, our approach learns to map Bidirectional
Reflectance Distribution Functions (BRDF) from pairs of scenes observed in
varying conditions, such as dry and wet. The learned transformations can then
be applied to unseen scenes with similar materials, therefore effectively
rendering the transformation learned with an arbitrary level of intensity.
Extensive experiments on synthetic scenes and real-world objects validate the
effectiveness of our approach, showing that it can learn various
transformations such as wetness, painting, coating, etc. Our results highlight
not only the versatility of our method but also its potential for practical
applications in computer graphics. We publish our method implementation, along
with our synthetic/real datasets on
https://github.com/astra-vision/BRDFTransform
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Properties of Diffusion Models for Perceptual Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Ravishankar, Zeeshan Patel, Jathushan Rajasegaran, Jitendra Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we argue that iterative computation with diffusion models
offers a powerful paradigm for not only generation but also visual perception
tasks. We unify tasks such as depth estimation, optical flow, and segmentation
under image-to-image translation, and show how diffusion models benefit from
scaling training and test-time compute for these perception tasks. Through a
careful analysis of these scaling behaviors, we present various techniques to
efficiently train diffusion models for visual perception tasks. Our models
achieve improved or comparable performance to state-of-the-art methods using
significantly less data and compute. To use our code and models, see
https://scaling-diffusion-perception.github.io .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While 3D content generation has advanced significantly, existing methods
still face challenges with input formats, latent space design, and output
representations. This paper introduces a novel 3D generation framework that
addresses these challenges, offering scalable, high-quality 3D generation with
an interactive Point Cloud-structured Latent space. Our framework employs a
Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal)
renderings as input, using a unique latent space design that preserves 3D shape
information, and incorporates a cascaded latent diffusion model for improved
shape-texture disentanglement. The proposed method, GaussianAnything, supports
multi-modal conditional 3D generation, allowing for point cloud, caption, and
single/multi-view image inputs. Notably, the newly proposed latent space
naturally enables geometry-texture disentanglement, thus allowing 3D-aware
editing. Experimental results demonstrate the effectiveness of our approach on
multiple datasets, outperforming existing methods in both text- and
image-conditioned 3D generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://nirvanalan.github.io/projects/GA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMPhy: Complex Physical Reasoning Using Large Language Models and World
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoop Cherian, Radu Corcodel, Siddarth Jain, Diego Romeres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical reasoning is an important skill needed for robotic agents when
operating in the real world. However, solving such reasoning problems often
involves hypothesizing and reflecting over complex multi-body interactions
under the effect of a multitude of physical forces and thus learning all such
interactions poses a significant hurdle for state-of-the-art machine learning
frameworks, including large language models (LLMs). To study this problem, we
propose a new physical reasoning task and a dataset, dubbed TraySim. Our task
involves predicting the dynamics of several objects on a tray that is given an
external impact -- the domino effect of the ensued object interactions and
their dynamics thus offering a challenging yet controlled setup, with the goal
of reasoning being to infer the stability of the objects after the impact. To
solve this complex physical reasoning task, we present LLMPhy, a zero-shot
black-box optimization framework that leverages the physics knowledge and
program synthesis abilities of LLMs, and synergizes these abilities with the
world models built into modern physics engines. Specifically, LLMPhy uses an
LLM to generate code to iteratively estimate the physical hyperparameters of
the system (friction, damping, layout, etc.) via an implicit
analysis-by-synthesis approach using a (non-differentiable) simulator in the
loop and uses the inferred parameters to imagine the dynamics of the scene
towards solving the reasoning task. To show the effectiveness of LLMPhy, we
present experiments on our TraySim dataset to predict the steady-state poses of
the objects. Our results show that the combination of the LLM and the physics
engine leads to state-of-the-art zero-shot physical reasoning performance,
while demonstrating superior convergence against standard black-box
optimization methods and better estimation of the physical parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model
  with Compact Wavelet Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Sanghi, Aliasghar Khani, Pradyumna Reddy, Arianna Rampini, Derek Cheung, Kamal Rahimi Malekshan, Kanika Madan, Hooman Shayani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale 3D generative models require substantial computational resources
yet often fall short in capturing fine details and complex geometries at high
resolutions. We attribute this limitation to the inefficiency of current
representations, which lack the compactness required to model the generative
models effectively. To address this, we introduce a novel approach called
Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based,
compact latent encodings. Specifically, we compress a $256^3$ signed distance
field into a $12^3 \times 4$ latent grid, achieving an impressive 2427x
compression ratio with minimal loss of detail. This high level of compression
allows our method to efficiently train large-scale generative networks without
increasing the inference time. Our models, both conditional and unconditional,
contain approximately one billion parameters and successfully generate
high-quality 3D shapes at $256^3$ resolution. Moreover, WaLa offers rapid
inference, producing shapes within two to four seconds depending on the
condition, despite the model's scale. We demonstrate state-of-the-art
performance across multiple datasets, with significant improvements in
generation quality, diversity, and computational efficiency. We open-source our
code and, to the best of our knowledge, release the largest pretrained 3D
generative models across different modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artistic Neural Style Transfer Algorithms with Activation Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangtian Li, Han Cao, Zhaoyang Zhang, Jiacheng Hu, Yuhui Jin, Zihao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The works of Gatys et al. demonstrated the capability of Convolutional Neural
Networks (CNNs) in creating artistic style images. This process of transferring
content images in different styles is called Neural Style Transfer (NST). In
this paper, we re-implement image-based NST, fast NST, and arbitrary NST. We
also explore to utilize ResNet with activation smoothing in NST. Extensive
experimental results demonstrate that smoothing transformation can greatly
improve the quality of stylization results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmut S. Gokmen, Cody Bumgardner, Caner Ozcan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary artery disease (CAD), one of the most common cause of mortality in
the world. Coronary artery calcium (CAC) scoring using computed tomography (CT)
is key for risk assessment to prevent coronary disease. Previous studies on
risk assessment and calcification detection in CT scans primarily use
approaches based on UNET architecture, frequently implemented on pre-built
models. However, these models are limited by the availability of annotated CT
scans containing CAC and suffering from imbalanced dataset, decreasing
performance of CAC segmentation and scoring. In this study, we extend this
approach by incorporating the self-supervised learning (SSL) technique of DINO
(self-distillation with no labels) to eliminate limitations of scarce annotated
data in CT scans. The DINO model's ability to train without requiring CAC area
annotations enhances its robustness in generating distinct features. The DINO
model is trained on to focus specifically on calcified areas by using labels,
aiming to generate features that effectively capture and highlight key
characteristics. The label-guided DINO (DINO-LG) enhances classification by
distinguishing CT slices that contain calcification from those that do not,
performing 57% better than the standard DINO model in this task. CAC scoring
and segmentation tasks are performed by a basic U-NET architecture, fed
specifically with CT slices containing calcified areas as identified by the
DINO-LG model. This targeted identification performed by DINO-LG model improves
CAC segmentation performance by approximately 10% and significant increase in
CAC scoring accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Developed by Center for Applied Artificial Intelligence (CAAI),
  University of Kentucky</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified
  Multimodal Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, Chong Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present JanusFlow, a powerful framework that unifies image understanding
and generation in a single model. JanusFlow introduces a minimalist
architecture that integrates autoregressive language models with rectified
flow, a state-of-the-art method in generative modeling. Our key finding
demonstrates that rectified flow can be straightforwardly trained within the
large language model framework, eliminating the need for complex architectural
modifications. To further improve the performance of our unified model, we
adopt two key strategies: (i) decoupling the understanding and generation
encoders, and (ii) aligning their representations during unified training.
Extensive experiments show that JanusFlow achieves comparable or superior
performance to specialized models in their respective domains, while
significantly outperforming existing unified approaches across standard
benchmarks. This work represents a step toward more efficient and versatile
vision-language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Commissioning An All-Sky Infrared Camera Array for Detection Of Airborne
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Dominé, Ankit Biswas, Richard Cloete, Alex Delacroix, Andriy Fedorenko, Lucas Jacaruso, Ezra Kelderman, Eric Keto, Sarah Little, Abraham Loeb, Eric Masson, Mike Prior, Forrest Schultz, Matthew Szenher, Wes Watters, Abby White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To date there is little publicly available scientific data on Unidentified
Aerial Phenomena (UAP) whose properties and kinematics purportedly reside
outside the performance envelope of known phenomena. To address this
deficiency, the Galileo Project is designing, building, and commissioning a
multi-modal ground-based observatory to continuously monitor the sky and
conduct a rigorous long-term aerial census of all aerial phenomena, including
natural and human-made. One of the key instruments is an all-sky infrared
camera array using eight uncooled long-wave infrared FLIR Boson 640 cameras.
Their calibration includes a novel extrinsic calibration method using airplane
positions from Automatic Dependent Surveillance-Broadcast (ADS-B) data. We
establish a first baseline for the system performance over five months of field
operation, using a real-world dataset derived from ADS-B data, synthetic 3-D
trajectories, and a hand-labelled real-world dataset. We report acceptance
rates (e.g. viewable airplanes that are recorded) and detection efficiencies
(e.g. recorded airplanes which are successfully detected) for a variety of
weather conditions, range and aircraft size. We reconstruct $\sim$500,000
trajectories of aerial objects from this commissioning period. A toy outlier
search focused on large sinuosity of the 2-D reconstructed trajectories flags
about 16% of trajectories as outliers. After manual review, 144 trajectories
remain ambiguous: they are likely mundane objects but cannot be elucidated at
this stage of development without distance and kinematics estimation or other
sensor modalities. Our observed count of ambiguous outliers combined with
systematic uncertainties yields an upper limit of 18,271 outliers count for the
five-month interval at a 95% confidence level. This likelihood-based method to
evaluate significance is applicable to all of our future outlier searches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimBase: A Simple Baseline for Temporal Video Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijun Bao, Alex C. Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents SimBase, a simple yet effective baseline for temporal
video grounding. While recent advances in temporal grounding have led to
impressive performance, they have also driven network architectures toward
greater complexity, with a range of methods to (1) capture temporal
relationships and (2) achieve effective multimodal fusion. In contrast, this
paper explores the question: How effective can a simplified approach be? To
investigate, we design SimBase, a network that leverages lightweight,
one-dimensional temporal convolutional layers instead of complex temporal
structures. For cross-modal interaction, SimBase only employs an element-wise
product instead of intricate multimodal fusion. Remarkably, SimBase achieves
state-of-the-art results on two large-scale datasets. As a simple yet powerful
baseline, we hope SimBase will spark new ideas and streamline future
evaluations in temporal video grounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays with
  Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxi Zhang, Yueliang Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography (CT) provides highly detailed three-dimensional (3D)
medical images but is costly, time-consuming, and often inaccessible in
intraoperative settings (Organization et al. 2011). Recent advancements have
explored reconstructing 3D chest volumes from sparse 2D X-rays, such as
single-view or orthogonal double-view images. However, current models tend to
process 2D images in a planar manner, prioritizing visual realism over
structural accuracy. In this work, we introduce DuoLift Generative Adversarial
Networks (DuoLift-GAN), a novel architecture with dual branches that
independently elevate 2D images and their features into 3D representations.
These 3D outputs are merged into a unified 3D feature map and decoded into a
complete 3D chest volume, enabling richer 3D information capture. We also
present a masked loss function that directs reconstruction towards critical
anatomical regions, improving structural accuracy and visual quality. This
paper demonstrates that DuoLift-GAN significantly enhances reconstruction
accuracy while achieving superior visual realism compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic <span class="highlight-title">dataset</span> shift identification to support root cause analysis of
  AI performance drift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélanie Roschewitz, Raghav Mehta, Charles Jones, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shifts in data distribution can substantially harm the performance of
clinical AI models. Hence, various methods have been developed to detect the
presence of such shifts at deployment time. However, root causes of dataset
shifts are varied, and the choice of shift mitigation strategies is highly
dependent on the precise type of shift encountered at test time. As such,
detecting test-time dataset shift is not sufficient: precisely identifying
which type of shift has occurred is critical. In this work, we propose the
first unsupervised dataset shift identification framework, effectively
distinguishing between prevalence shift (caused by a change in the label
distribution), covariate shift (caused by a change in input characteristics)
and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the
importance of self-supervised encoders for detecting subtle covariate shifts
and propose a novel shift detector leveraging both self-supervised encoders and
task model outputs for improved shift detection. We report promising results
for the proposed shift identification framework across three different imaging
modalities (chest radiography, digital mammography, and retinal fundus images)
on five types of real-world dataset shifts, using four large publicly available
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/biomedia-mira/shift_identification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Disentangled Representations for Perceptual Point Cloud Quality
  Assessment via Mutual Information Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Shan, Yujie Zhang, Yipeng Liu, Yiling Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-Reference Point Cloud Quality Assessment (NR-PCQA) aims to objectively
assess the human perceptual quality of point clouds without relying on
pristine-quality point clouds for reference. It is becoming increasingly
significant with the rapid advancement of immersive media applications such as
virtual reality (VR) and augmented reality (AR). However, current NR-PCQA
models attempt to indiscriminately learn point cloud content and distortion
representations within a single network, overlooking their distinct
contributions to quality information. To address this issue, we propose DisPA,
a novel disentangled representation learning framework for NR-PCQA. The
framework trains a dual-branch disentanglement network to minimize mutual
information (MI) between representations of point cloud content and distortion.
Specifically, to fully disentangle representations, the two branches adopt
different philosophies: the content-aware encoder is pretrained by a masked
auto-encoding strategy, which can allow the encoder to capture semantic
information from rendered images of distorted point clouds; the
distortion-aware encoder takes a mini-patch map as input, which forces the
encoder to focus on low-level distortion patterns. Furthermore, we utilize an
MI estimator to estimate the tight upper bound of the actual MI and further
minimize it to achieve explicit representation disentanglement. Extensive
experimental results demonstrate that DisPA outperforms state-of-the-art
methods on multiple PCQA datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Isometric Transformations for Image Augmentation in Mueller Matrix
  Polarimetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Hahne, Omar Rodriguez-Nunez, Éléa Gros, Théotim Lucas, Ekkehard Hewer, Tatiana Novikova, Theoni Maragkou, Philippe Schucht, Richard McKinley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mueller matrix polarimetry captures essential information about polarized
light interactions with a sample, presenting unique challenges for data
augmentation in deep learning due to its distinct structure. While
augmentations are an effective and affordable way to enhance dataset diversity
and reduce overfitting, standard transformations like rotations and flips do
not preserve the polarization properties in Mueller matrix images. To this end,
we introduce a versatile simulation framework that applies physically
consistent rotations and flips to Mueller matrices, tailored to maintain
polarization fidelity. Our experimental results across multiple datasets reveal
that conventional augmentations can lead to misleading results when applied to
polarimetric data, underscoring the necessity of our physics-based approach. In
our experiments, we first compare our polarization-specific augmentations
against real-world captures to validate their physical consistency. We then
apply these augmentations in a semantic segmentation task, achieving
substantial improvements in model generalization and performance. This study
underscores the necessity of physics-informed data augmentation for
polarimetric imaging in deep learning (DL), paving the way for broader adoption
and more robust applications across diverse research in the field. In
particular, our framework unlocks the potential of DL models for polarimetric
datasets with limited sample sizes. Our code implementation is available at
github.com/hahnec/polar_augment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TLDR: Traffic Light Detection using Fourier Domain Adaptation in Hostile
  WeatheR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Gakhar, Aryesh Guha, Aryaman Gupta, Amit Agarwal, Durga Toshniwal, Ujjwal Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of comprehensive datasets in the traffic light detection and
recognition domain and the poor performance of state-of-the-art models under
hostile weather conditions present significant challenges. To address these
issues, this paper proposes a novel approach by merging two widely used
datasets, LISA and S2TLD. The merged dataset is further processed to tackle
class imbalance, a common problem in this domain. This merged dataset becomes
our source domain. Synthetic rain and fog are added to the dataset to create
our target domain. We employ Fourier Domain Adaptation (FDA) to create a final
dataset with a minimized domain gap between the two datasets, helping the model
trained on this final dataset adapt to rainy and foggy weather conditions.
Additionally, we explore Semi-Supervised Learning (SSL) techniques to leverage
the available data more effectively. Experimental results demonstrate that
models trained on FDA-augmented images outperform those trained without FDA
across confidence-dependent and independent metrics, like mAP50, mAP50-95,
Precision, and Recall. The best-performing model, YOLOv8, achieved a Precision
increase of 5.1860%, Recall increase of 14.8009%, mAP50 increase of 9.5074%,
and mAP50-95 increase of 19.5035%. On average, percentage increases of 7.6892%
in Precision, 19.9069% in Recall, 15.8506% in mAP50, and 23.8099% in mAP50-95
were observed across all models, highlighting the effectiveness of FDA in
mitigating the impact of adverse weather conditions on model performance. These
improvements pave the way for real-world applications where reliable
performance in challenging environmental conditions is critical.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review at IEEE Transactions of Artificial Intelligence. 10
  Pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse
  Tensor-based <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Huo, Junhui Ho, Shuai Wan, Fuzheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of 3D visualization techniques has fundamentally transformed
how we interact with digital content. At the forefront of this change is point
cloud technology, offering an immersive experience that surpasses traditional
2D representations. However, the massive data size of point clouds presents
significant challenges in data compression. Current methods for lossy point
cloud attribute compression (PCAC) generally focus on reconstructing the
original point clouds with minimal error. However, for point cloud
visualization scenarios, the reconstructed point clouds with distortion still
need to undergo a complex rendering process, which affects the final
user-perceived quality. In this paper, we propose an end-to-end deep learning
framework that seamlessly integrates PCAC with differentiable rendering,
denoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of
rendered multiview images for viewing. In a differentiable manner, the impact
of the rendering process on the reconstructed point clouds is taken into
account. Moreover, we characterize point clouds as sparse tensors and propose a
sparse tensor-based transformer, called SP-Trans. By aligning with the local
density of the point cloud and utilizing an enhanced local attention mechanism,
SP-Trans captures the intricate relationships within the point cloud, further
improving feature analysis and synthesis within the framework. Extensive
experiments demonstrate that the proposed RO-PCAC achieves state-of-the-art
compression performance, compared to existing reconstruction-oriented methods,
including traditional, learning-based, and hybrid methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint multi-dimensional dynamic attention and <span class="highlight-title">transformer</span> for general
  image restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Zhang, Xu Zhang, Nian Cai, Jianglei Di, Yun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outdoor images often suffer from severe degradation due to rain, haze, and
noise, impairing image quality and challenging high-level tasks. Current image
restoration methods struggle to handle complex degradation while maintaining
efficiency. This paper introduces a novel image restoration architecture that
combines multi-dimensional dynamic attention and self-attention within a U-Net
framework. To leverage the global modeling capabilities of transformers and the
local modeling capabilities of convolutions, we integrate sole CNNs in the
encoder-decoder and sole transformers in the latent layer. Additionally, we
design convolutional kernels with selected multi-dimensional dynamic attention
to capture diverse degraded inputs efficiently. A transformer block with
transposed self-attention further enhances global feature extraction while
maintaining efficiency. Extensive experiments demonstrate that our method
achieves a better balance between performance and computational complexity
across five image restoration tasks: deraining, deblurring, denoising,
dehazing, and enhancement, as well as superior performance for high-level
vision tasks. The source code will be available at
https://github.com/House-yuyu/MDDA-former.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INTRABENCH: Interactive Radiological Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Ulrich, Tassilo Wald, Emily Tempus, Maximilian Rokuss, Paul F. Jaeger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current interactive segmentation approaches, inspired by the success of
META's Segment Anything model, have achieved notable advancements, however,
they come with substantial limitations that hinder their practical application
in real clinical scenarios. These include unrealistic human interaction
requirements, such as slice-by-slice operations for 2D models on 3D data, a
lack of iterative refinement, and insufficient evaluation experiments. These
shortcomings prevent accurate assessment of model performance and lead to
inconsistent outcomes across studies. IntRaBench overcomes these challenges by
offering a comprehensive and reproducible framework for evaluating interactive
segmentation methods in realistic, clinically relevant scenarios. It includes
diverse datasets, target structures, and segmentation models, and provides a
flexible codebase that allows seamless integration of new models and prompting
strategies. Additionally, we introduce advanced techniques to minimize
clinician interaction, ensuring fair comparisons between 2D and 3D models. By
open-sourcing IntRaBench, we invite the research community to integrate their
models and prompting techniques, ensuring continuous and transparent evaluation
of interactive segmentation models in 3D medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Undergoing Peer-Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse capability and scaling of diffusion and auto-regressive models
  when learning abstract rules <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binxu Wang, Jiaqi Shang, Haim Sompolinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans excel at discovering regular structures from limited samples and
applying inferred rules to novel settings. We investigate whether modern
generative models can similarly learn underlying rules from finite samples and
perform reasoning through conditional sampling. Inspired by Raven's Progressive
Matrices task, we designed GenRAVEN dataset, where each sample consists of
three rows, and one of 40 relational rules governing the object position,
number, or attributes applies to all rows. We trained generative models to
learn the data distribution, where samples are encoded as integer arrays to
focus on rule learning. We compared two generative model families: diffusion
(EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their
ability to generate structurally consistent samples and perform panel
completion via unconditional and conditional sampling. We found diffusion
models excel at unconditional generation, producing more novel and consistent
samples from scratch and memorizing less, but performing less well in panel
completion, even with advanced conditional sampling methods. Conversely,
autoregressive models excel at completing missing panels in a rule-consistent
manner but generate less consistent samples unconditionally. We observe diverse
data scaling behaviors: for both model families, rule learning emerges at a
certain dataset size - around 1000s examples per rule. With more training data,
diffusion models improve both their unconditional and conditional generation
capabilities. However, for autoregressive models, while panel completion
improves with more training data, unconditional generation consistency
declines. Our findings highlight complementary capabilities and limitations of
diffusion and autoregressive models in rule learning and reasoning tasks,
suggesting avenues for further research into their mechanisms and potential for
human-like reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures. Accepted to NeurIPS2024 Workshop on System 2
  Reasoning At Scale as long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDXFormer: Boosting Remote Sensing Change Detection with Extended Long
  Short-Term Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenkai Wu, Xiaowen Ma, Rongrong Lian, Zhentao Lin, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In complex scenes and varied conditions, effectively integrating
spatial-temporal context is crucial for accurately identifying changes.
However, current RS-CD methods lack a balanced consideration of performance and
efficiency. CNNs lack global context, Transformers have quadratic computational
complexity, and Mambas are restricted by CUDA acceleration. In this paper, we
propose CDXFormer, with a core component that is a powerful XLSTM-based feature
enhancement layer, integrating the advantages of linear computational
complexity, global context perception, and strong interpret-ability.
Specifically, we introduce a scale-specific Feature Enhancer layer,
incorporating a Cross-Temporal Global Perceptron customized for
semantic-accurate deep features, and a Cross-Temporal Spatial Refiner
customized for detail-rich shallow features. Additionally, we propose a
Cross-Scale Interactive Fusion module to progressively interact global change
representations with spatial responses. Extensive experimental results
demonstrate that CDXFormer achieves state-of-the-art performance across three
benchmark datasets, offering a compelling balance between efficiency and
accuracy. Code is available at https://github.com/xwmaxwma/rschange.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-SLAM for OC-VLN: Natural Language Grounded SLAM for Object-Centric
  VLN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonia Raychaudhuri, Duy Ta, Katrina Ashton, Angel X. Chang, Jiuguang Wang, Bernadette Bucher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Landmark-based navigation (e.g. go to the wooden desk) and relative
positional navigation (e.g. move 5 meters forward) are distinct navigation
challenges solved very differently in existing robotics navigation methodology.
We present a new dataset, OC-VLN, in order to distinctly evaluate grounding
object-centric natural language navigation instructions in a method for
performing landmark-based navigation. We also propose Natural Language grounded
SLAM (NL-SLAM), a method to ground natural language instruction to robot
observations and poses. We actively perform NL-SLAM in order to follow
object-centric natural language navigation instructions. Our methods leverage
pre-trained vision and language foundation models and require no task-specific
training. We construct two strong baselines from state-of-the-art methods on
related tasks, Object Goal Navigation and Vision Language Navigation, and we
show that our approach, NL-SLAM, outperforms these baselines across all our
metrics of success on OC-VLN. Finally, we successfully demonstrate the
effectiveness of NL-SLAM for performing navigation instruction following in the
real world on a Boston Dynamics Spot robot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Vision Mixture of Experts for Wildlife Monitoring on the Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel Azuh Mensah, Anderson Lee, Haoran Zhang, Yitong Shan, Kurtis Heimerl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosion of IoT sensors in industrial, consumer and remote sensing use
cases has come with unprecedented demand for computing infrastructure to
transmit and to analyze petabytes of data. Concurrently, the world is slowly
shifting its focus towards more sustainable computing. For these reasons, there
has been a recent effort to reduce the footprint of related computing
infrastructure, especially by deep learning algorithms, for advanced insight
generation. The `TinyML' community is actively proposing methods to save
communication bandwidth and excessive cloud storage costs while reducing
algorithm inference latency and promoting data privacy. Such proposed
approaches should ideally process multiple types of data, including time
series, audio, satellite images, and video, near the network edge as multiple
data streams has been shown to improve the discriminative ability of learning
algorithms, especially for generating fine grained results. Incidentally, there
has been recent work on data driven conditional computation of subnetworks that
has shown real progress in using a single model to share parameters among very
different types of inputs such as images and text, reducing the computation
requirement of multi-tower multimodal networks. Inspired by such line of work,
we explore similar per patch conditional computation for the first time for
mobile vision transformers (vision only case), that will eventually be used for
single-tower multimodal edge models. We evaluate the model on Cornell Sap
Sucker Woods 60, a fine grained bird species discrimination dataset. Our
initial experiments uses $4X$ fewer parameters compared to MobileViTV2-1.0 with
a $1$% accuracy drop on the iNaturalist '21 birds test data provided as part of
the SSW60 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-scale Remote Sensing Image Target Recognition and Automatic
  Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuzheng Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method for object recognition and automatic labeling in
large-area remote sensing images called LRSAA. The method integrates YOLOv11
and MobileNetV3-SSD object detection algorithms through ensemble learning to
enhance model performance. Furthermore, it employs Poisson disk sampling
segmentation techniques and the EIOU metric to optimize the training and
inference processes of segmented images, followed by the integration of
results. This approach not only reduces the demand for computational resources
but also achieves a good balance between accuracy and speed. The source code
for this project has been made publicly available on
https://github.com/anaerovane/LRSAA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and
  Re-Identification using Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fusaro, Federico Magistri, Jens Behley, Alberto Pretto, Cyrill Stachniss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic fruit monitoring is a key step toward automated agricultural
production systems. Robots can significantly enhance plant and temporal fruit
monitoring by providing precise, high-throughput assessments that overcome the
limitations of traditional manual methods. Fruit monitoring is a challenging
task due to the significant variation in size, shape, orientation, and
occlusion of fruits. Also, fruits may be harvested or newly grown between
recording sessions. Most methods are 2D image-based and they lack the 3D
structure, depth, and spatial information, which represent key aspects of fruit
monitoring. 3D colored point clouds, instead, can offer this information but
they introduce challenges such as their sparsity and irregularity. In this
paper, we present a novel approach for temporal fruit monitoring that addresses
point clouds collected in a greenhouse over time. Our method segments fruits
using a learning-based instance segmentation approach directly on the point
cloud. Each segmented fruit is processed by a 3D sparse convolutional neural
network to extract descriptors, which are used in an attention-based matching
network to associate fruits with their instances from previous data
collections. Experimental results on a real dataset of strawberries demonstrate
that our approach outperforms other methods for fruits re-identification over
time, allowing for precise temporal fruit monitoring in real and complex
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interaction Asymmetry: A General Principle for Learning Composable
  Abstractions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Brady, Julius von Kügelgen, Sébastien Lachapelle, Simon Buchholz, Thomas Kipf, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning disentangled representations of concepts and re-composing them in
unseen ways is crucial for generalizing to out-of-domain situations. However,
the underlying properties of concepts that enable such disentanglement and
compositional generalization remain poorly understood. In this work, we propose
the principle of interaction asymmetry which states: "Parts of the same concept
have more complex interactions than parts of different concepts". We formalize
this via block diagonality conditions on the $(n+1)$th order derivatives of the
generator mapping concepts to observed data, where different orders of
"complexity" correspond to different $n$. Using this formalism, we prove that
interaction asymmetry enables both disentanglement and compositional
generalization. Our results unify recent theoretical results for learning
concepts of objects, which we show are recovered as special cases with
$n\!=\!0$ or $1$. We provide results for up to $n\!=\!2$, thus extending these
prior works to more flexible generator functions, and conjecture that the same
proof strategies generalize to larger $n$. Practically, our theory suggests
that, to disentangle concepts, an autoencoder should penalize its latent
capacity and the interactions between concepts during decoding. We propose an
implementation of these criteria using a flexible Transformer-based VAE, with a
novel regularizer on the attention weights of the decoder. On synthetic image
datasets consisting of objects, we provide evidence that this model can achieve
comparable object disentanglement to existing models that use more explicit
object-centric priors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel View Synthesis with Pixel-Space Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Elata, Bahjat Kawar, Yaron Ostrovsky-Berman, Miriam Farber, Ron Sokolovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing a novel view from a single input image is a challenging task.
Traditionally, this task was approached by estimating scene depth, warping, and
inpainting, with machine learning models enabling parts of the pipeline. More
recently, generative models are being increasingly employed in novel view
synthesis (NVS), often encompassing the entire end-to-end system. In this work,
we adapt a modern diffusion model architecture for end-to-end NVS in the pixel
space, substantially outperforming previous state-of-the-art (SOTA) techniques.
We explore different ways to encode geometric information into the network. Our
experiments show that while these methods may enhance performance, their impact
is minor compared to utilizing improved generative models. Moreover, we
introduce a novel NVS training scheme that utilizes single-view datasets,
capitalizing on their relative abundance compared to their multi-view
counterparts. This leads to improved generalization capabilities to scenes with
out-of-domain content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaSemiCD: An Adaptive Semi-Supervised Change Detection Method Based on
  Pseudo-Label Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Lingyan, Wen Dongcheng, Zhuo Tao, Zhang Shizhou, Zhang Xiuwei, Zhang Yanning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change Detection (CD) is an essential field in remote sensing, with a primary
focus on identifying areas of change in bi-temporal image pairs captured at
varying intervals of the same region by a satellite. The data annotation
process for the CD task is both time-consuming and labor-intensive. To make
better use of the scarce labeled data and abundant unlabeled data, we present
an adaptive dynamic semi-supervised learning method, AdaSemiCD, to improve the
use of pseudo-labels and optimize the training process. Initially, due to the
extreme class imbalance inherent in CD, the model is more inclined to focus on
the background class, and it is easy to confuse the boundary of the target
object. Considering these two points, we develop a measurable evaluation metric
for pseudo-labels that enhances the representation of information entropy by
class rebalancing and amplification of confusing areas to give a larger weight
to prospects change objects. Subsequently, to enhance the reliability of
sample-wise pseudo-labels, we introduce the AdaFusion module, which is capable
of dynamically identifying the most uncertain region and substituting it with
more trustworthy content. Lastly, to ensure better training stability, we
introduce the AdaEMA module, which updates the teacher model using only batches
of trusted samples. Experimental results from LEVIR-CD, WHU-CD, and CDD
datasets validate the efficacy and universality of our proposed adaptive
training framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State
  Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Qian, Jiaran Gao, Yaodan Zhang, Qiquan Zhang, Hexin Liu, Leibny Paola Garcia, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement plays an essential role in various applications, and the
integration of visual information has been demonstrated to bring substantial
advantages. However, the majority of current research concentrates on the
examination of facial and lip movements, which can be compromised or entirely
inaccessible in scenarios where occlusions occur or when the camera view is
distant. Whereas contextual visual cues from the surrounding environment have
been overlooked: for example, when we see a dog bark, our brain has the innate
ability to discern and filter out the barking noise. To this end, in this
paper, we introduce a novel task, i.e. SAV-SE. To our best knowledge, this is
the first proposal to use rich contextual information from synchronized video
as auxiliary cues to indicate the type of noise, which eventually improves the
speech enhancement performance. Specifically, we propose the VC-S$^2$E method,
which incorporates the Conformer and Mamba modules for their complementary
strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and
AudioSet datasets, where the results demonstrate the superiority of VC-S$^2$E
over other competitive methods. We will make the source code publicly
available. Project demo page: https://AVSEPage.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LapGSR: Laplacian Reconstructive Network for Guided Thermal
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kasliwal, Ishaan Gakhar, Aryan Kamani, Pratinav Seth, Ujjwal Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last few years, the fusion of multi-modal data has been widely studied
for various applications such as robotics, gesture recognition, and autonomous
navigation. Indeed, high-quality visual sensors are expensive, and
consumer-grade sensors produce low-resolution images. Researchers have
developed methods to combine RGB color images with non-visual data, such as
thermal, to overcome this limitation to improve resolution. Fusing multiple
modalities to produce visually appealing, high-resolution images often requires
dense models with millions of parameters and a heavy computational load, which
is commonly attributed to the intricate architecture of the model.
  We propose LapGSR, a multimodal, lightweight, generative model incorporating
Laplacian image pyramids for guided thermal super-resolution. This approach
uses a Laplacian Pyramid on RGB color images to extract vital edge information,
which is then used to bypass heavy feature map computation in the higher layers
of the model in tandem with a combined pixel and adversarial loss. LapGSR
preserves the spatial and structural details of the image while also being
efficient and compact. This results in a model with significantly fewer
parameters than other SOTA models while demonstrating excellent results on two
cross-domain datasets viz. ULB17-VT and VGTSR datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constraint Learning for Parametric Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Cheng, Ruiqi Lei, Di Huang, Zhichao Liao, Fengyuan Piao, Yan Chen, Pingfa Feng, Long Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parametric point clouds are sampled from CAD shapes, have become increasingly
prevalent in industrial manufacturing. However, most existing point cloud
learning methods focus on the geometric features, such as local and global
features or developing efficient convolution operations, overlooking the
important attribute of constraints inherent in CAD shapes, which limits these
methods' ability to fully comprehend CAD shapes. To address this issue, we
analyzed the effect of constraints, and proposed its deep learning-friendly
representation, after that, the Constraint Feature Learning Network (CstNet) is
developed to extract and leverage constraints. Our CstNet includes two stages.
The Stage 1 extracts constraints from B-Rep data or point cloud. The Stage 2
leverages coordinates and constraints to enhance the comprehend of CAD shapes.
Additionally, we built up the Parametric 20,000 Multi-modal Dataset for the
scarcity of labeled B-Rep datasets. Experiments demonstrate that our CstNet
achieved state-of-the-art performance on both public and proposed CAD shapes
datasets. To the best of our knowledge, CstNet is the first constraint-based
learning method tailored for CAD shapes analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial
  Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Li, Tianyu Sun, Xueqian Zhang, Zhongdao Wang, Bailan Feng, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies point cloud perception within outdoor environments.
Existing methods face limitations in recognizing objects located at a distance
or occluded, due to the sparse nature of outdoor point clouds. In this work, we
observe a significant mitigation of this problem by accumulating multiple
temporally consecutive LiDAR sweeps, resulting in a remarkable improvement in
perception accuracy. However, the computation cost also increases, hindering
previous approaches from utilizing a large number of LiDAR sweeps. To tackle
this challenge, we find that a considerable portion of points in the
accumulated point cloud is redundant, and discarding these points has minimal
impact on perception accuracy. We introduce a simple yet effective Gumbel
Spatial Pruning (GSP) layer that dynamically prunes points based on a learned
end-to-end sampling. The GSP layer is decoupled from other network components
and thus can be seamlessly integrated into existing point cloud network
architectures. Without incurring additional computational overhead, we increase
the number of LiDAR sweeps from 10, a common practice, to as many as 40.
Consequently, there is a significant enhancement in perception performance. For
instance, in nuScenes 3D object detection and BEV map segmentation tasks, our
pruning strategy improves the vanilla TransL baseline and other baseline
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Focusing-and-Matching Network for Multi-Instance Point Cloud
  Registration <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyuan Zhang, Le Hui, Qi Liu, Bo Li, Yuchao Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-instance point cloud registration aims to estimate the pose of all
instances of a model point cloud in the whole scene. Existing methods all adopt
the strategy of first obtaining the global correspondence and then clustering
to obtain the pose of each instance. However, due to the cluttered and occluded
objects in the scene, it is difficult to obtain an accurate correspondence
between the model point cloud and all instances in the scene. To this end, we
propose a simple yet powerful 3D focusing-and-matching network for
multi-instance point cloud registration by learning the multiple pair-wise
point cloud registration. Specifically, we first present a 3D multi-object
focusing module to locate the center of each object and generate object
proposals. By using self-attention and cross-attention to associate the model
point cloud with structurally similar objects, we can locate potential matching
instances by regressing object centers. Then, we propose a 3D dual masking
instance matching module to estimate the pose between the model point cloud and
each object proposal. It performs instance mask and overlap mask masks to
accurately predict the pair-wise correspondence. Extensive experiments on two
public benchmarks, Scan2CAD and ROBI, show that our method achieves a new
state-of-the-art performance on the multi-instance point cloud registration
task. Code is available at https://github.com/zlynpu/3DFMNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No-Reference Point Cloud Quality Assessment via Graph Convolutional
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wu Chen, Qiuping Jiang, Wei Zhou, Feng Shao, Guangtao Zhai, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional (3D) point cloud, as an emerging visual media format, is
increasingly favored by consumers as it can provide more realistic visual
information than two-dimensional (2D) data. Similar to 2D plane images and
videos, point clouds inevitably suffer from quality degradation and information
loss through multimedia communication systems. Therefore, automatic point cloud
quality assessment (PCQA) is of critical importance. In this work, we propose a
novel no-reference PCQA method by using a graph convolutional network (GCN) to
characterize the mutual dependencies of multi-view 2D projected image contents.
The proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e.,
multi-view projection, graph construction, and GCN-based quality prediction.
First, multi-view projection is performed on the test point cloud to obtain a
set of horizontally and vertically projected images. Then, a
perception-consistent graph is constructed based on the spatial relations among
different projected images. Finally, reasoning on the constructed graph is
performed by GCN to characterize the mutual dependencies and interactions
between different projected images, and aggregate feature information of
multi-view projected images for final quality prediction. Experimental results
on two publicly available benchmark databases show that our proposed GC-PCQA
can achieve superior performance than state-of-the-art quality assessment
metrics. The code will be available at: https://github.com/chenwuwq/GC-PCQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALOcc: Adaptive Lifting-based 3D Semantic Occupancy and Cost
  Volume-based Flow Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dubing Chen, Jin Fang, Wencheng Han, Xinjing Cheng, Junbo Yin, Chenzhong Xu, Fahad Shahbaz Khan, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based semantic occupancy and flow prediction plays a crucial role in
providing spatiotemporal cues for real-world tasks, such as autonomous driving.
Existing methods prioritize higher accuracy to cater to the demands of these
tasks. In this work, we strive to improve performance by introducing a series
of targeted improvements for 3D semantic occupancy prediction and flow
estimation. First, we introduce an occlusion-aware adaptive lifting mechanism
with a depth denoising technique to improve the robustness of 2D-to-3D feature
transformation and reduce the reliance on depth priors. Second, we strengthen
the semantic consistency between 3D features and their original 2D modalities
by utilizing shared semantic prototypes to jointly constrain both 2D and 3D
features. This is complemented by confidence- and category-based sampling
strategies to tackle long-tail challenges in 3D space. To alleviate the feature
encoding burden in the joint prediction of semantics and flow, we propose a BEV
cost volume-based prediction method that links flow and semantic features
through a cost volume and employs a classification-regression supervision
scheme to address the varying flow scales in dynamic scenes. Our purely
convolutional architecture framework, named ALOcc, achieves an optimal tradeoff
between speed and accuracy achieving state-of-the-art results on multiple
benchmarks. On Occ3D and training without the camera visible mask, our ALOcc
achieves an absolute gain of 2.5\% in terms of RayIoU while operating at a
comparable speed compared to the state-of-the-art, using the same input size
(256$\times$704) and ResNet-50 backbone. Our method also achieves 2nd place in
the CVPR24 Occupancy and Flow Prediction Competition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMPERROR: A Flexible Generative Perception Error Model for Probing
  Self-Driving Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Hanselmann, Simon Doll, Marius Cordts, Hendrik P. A. Lensch, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To handle the complexities of real-world traffic, learning planners for
self-driving from data is a promising direction. While recent approaches have
shown great progress, they typically assume a setting in which the ground-truth
world state is available as input. However, when deployed, planning needs to be
robust to the long-tail of errors incurred by a noisy perception system, which
is often neglected in evaluation. To address this, previous work has proposed
drawing adversarial samples from a perception error model (PEM) mimicking the
noise characteristics of a target object detector. However, these methods use
simple PEMs that fail to accurately capture all failure modes of detection. In
this paper, we present EMPERROR, a novel transformer-based generative PEM,
apply it to stress-test an imitation learning (IL)-based planner and show that
it imitates modern detectors more faithfully than previous work. Furthermore,
it is able to produce realistic noisy inputs that increase the planner's
collision rate by up to 85%, demonstrating its utility as a valuable tool for a
more complete evaluation of self-driving planners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lasnik.github.io/emperror/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Classification of Children Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanchayan Vivekananthan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a process for a classification model for the facial
expressions. The proposed process would aid in specific categorisation of
children's emotions from 2 emotions namely 'Happy' and 'Sad'. Since the
existing emotion recognition systems algorithms primarily train on adult faces,
the model developed is achieved by using advanced concepts of models with
Squeeze-andExcitation blocks, Convolutional Block Attention modules, and robust
data augmentation. Stable Diffusion image synthesis was used for expanding and
diversifying the data set generating realistic and various training samples.
The model designed using Batch Normalisation, Dropout, and SE Attention
mechanisms for the classification of children's emotions achieved an accuracy
rate of 89\% due to these methods improving the precision of emotion
recognition in children. The relative importance of this issue is raised in
this study with an emphasis on the call for a more specific model in emotion
detection systems for the young generation with specific direction on how the
young people can be assisted to manage emotions while online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with
  ImageRAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilun Zhang, Haozhan Shen, Tiancheng Zhao, Yuhao Wang, Bin Chen, Yuxiang Cai, Yongheng Shang, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000
$\times$ 100,000 pixels or more) poses a significant challenge for current
Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize
the UHR image to standard input image size, the extensive spatial and
contextual information that UHR images contain will be neglected. Otherwise,
the original size of these images often exceeds the token limits of standard
RSMLLMs, making it difficult to process the entire image and capture long-range
dependencies to answer the query based on the abundant visual context. In this
paper, we introduce ImageRAG for RS, a training-free framework to address the
complexities of analyzing UHR remote sensing imagery. By transforming UHR
remote sensing image analysis task to image's long context selection task, we
design an innovative image contextual retrieval mechanism based on the
Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's
core innovation lies in its ability to selectively retrieve and focus on the
most relevant portions of the UHR image as visual contexts that pertain to a
given query. Fast path and slow path are proposed in this framework to handle
this task efficiently and effectively. ImageRAG allows RSMLLMs to manage
extensive context and spatial information from UHR RSI, ensuring the analysis
is both accurate and efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Disentangled Slim Tensor Learning for Multi-view Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deng Xu, Chao Zhang, Zechao Li, Chunlin Chen, Huaxiong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor-based multi-view clustering has recently received significant
attention due to its exceptional ability to explore cross-view high-order
correlations. However, most existing methods still encounter some limitations.
(1) Most of them explore the correlations among different affinity matrices,
making them unscalable to large-scale data. (2) Although some methods address
it by introducing bipartite graphs, they may result in sub-optimal solutions
caused by an unstable anchor selection process. (3) They generally ignore the
negative impact of latent semantic-unrelated information in each view. To
tackle these issues, we propose a new approach termed fast Disentangled Slim
Tensor Learning (DSTL) for multi-view clustering . Instead of focusing on the
multi-view graph structures, DSTL directly explores the high-order correlations
among multi-view latent semantic representations based on matrix factorization.
To alleviate the negative influence of feature redundancy, inspired by robust
PCA, DSTL disentangles the latent low-dimensional representation into a
semantic-unrelated part and a semantic-related part for each view.
Subsequently, two slim tensors are constructed with tensor-based
regularization. To further enhance the quality of feature disentanglement, the
semantic-related representations are aligned across views through a consensus
alignment indicator. Our proposed model is computationally efficient and can be
solved effectively. Extensive experiments demonstrate the superiority and
efficiency of DSTL over state-of-the-art approaches. The code of DSTL is
available at https://github.com/dengxu-nju/DSTL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,6 figures, will be published to IEEE TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI enhanced diagnosis of Peyronies disease a novel approach using
  Computer Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudara Kularathne, Janitha Prathapa, Prarththanan Sothyrajah, Salomi Arasaratnam, Sithira Ambepitiya, Thanveer Ahamed, Dinuka Wijesundara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents an innovative AI-driven tool for diagnosing Peyronie's
Disease (PD), a condition that affects between 0.3% and 13.1% of men worldwide.
Our method uses key point detection on both images and videos to measure penile
curvature angles, utilizing advanced computer vision techniques. This tool has
demonstrated high accuracy in identifying anatomical landmarks, validated
against conventional goniometer measurements. Traditional PD diagnosis often
involves subjective and invasive methods, which can lead to patient discomfort
and inaccuracies. Our approach offers a precise, reliable, and non-invasive
diagnostic tool to address these drawbacks. The model distinguishes between PD
and normal anatomical changes with a sensitivity of 96.7% and a specificity of
100%. This advancement represents a significant improvement in urological
diagnostics, greatly enhancing the efficacy and convenience of PD assessment
for healthcare providers and patients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Generation of Spatial Relations in Text and Image
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang Hong Sim, Clarence Lee, Alvin Tan, Cheston Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding spatial relations is a crucial cognitive ability for both
humans and AI. While current research has predominantly focused on the
benchmarking of text-to-image (T2I) models, we propose a more comprehensive
evaluation that includes \textit{both} T2I and Large Language Models (LLMs). As
spatial relations are naturally understood in a visuo-spatial manner, we
develop an approach to convert LLM outputs into an image, thereby allowing us
to evaluate both T2I models and LLMs \textit{visually}. We examined the spatial
relation understanding of 8 prominent generative models (3 T2I models and 5
LLMs) on a set of 10 common prepositions, as well as assess the feasibility of
automatic evaluation methods. Surprisingly, we found that T2I models only
achieve subpar performance despite their impressive general image-generation
abilities. Even more surprisingly, our results show that LLMs are significantly
more accurate than T2I models in generating spatial relations, despite being
primarily trained on textual data. We examined reasons for model failures and
highlight gaps that can be filled to enable more spatially faithful
generations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HMIL: Hierarchical Multi-Instance Learning for Fine-Grained Whole Slide
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Jin, Luyang Luo, Huangjing Lin, Jun Hou, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained classification of whole slide images (WSIs) is essential in
precision oncology, enabling precise cancer diagnosis and personalized
treatment strategies. The core of this task involves distinguishing subtle
morphological variations within the same broad category of gigapixel-resolution
images, which presents a significant challenge. While the multi-instance
learning (MIL) paradigm alleviates the computational burden of WSIs, existing
MIL methods often overlook hierarchical label correlations, treating
fine-grained classification as a flat multi-class classification task. To
overcome these limitations, we introduce a novel hierarchical multi-instance
learning (HMIL) framework. By facilitating on the hierarchical alignment of
inherent relationships between different hierarchy of labels at instance and
bag level, our approach provides a more structured and informative learning
process. Specifically, HMIL incorporates a class-wise attention mechanism that
aligns hierarchical information at both the instance and bag levels.
Furthermore, we introduce supervised contrastive learning to enhance the
discriminative capability for fine-grained classification and a
curriculum-based dynamic weighting module to adaptively balance the
hierarchical feature during training. Extensive experiments on our large-scale
cytology cervical cancer (CCC) dataset and two public histology datasets, BRACS
and PANDA, demonstrate the state-of-the-art class-wise and overall performance
of our HMIL framework. Our source code is available at
https://github.com/ChengJin-git/HMIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Audiovisual Deepfake Detection: Techniques, Challenges,
  Human Factors and Perceptual Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning has been successfully applied in diverse fields, and its impact
on deepfake detection is no exception. Deepfakes are fake yet realistic
synthetic content that can be used deceitfully for political impersonation,
phishing, slandering, or spreading misinformation. Despite extensive research
on unimodal deepfake detection, identifying complex deepfakes through joint
analysis of audio and visual streams remains relatively unexplored. To fill
this gap, this survey first provides an overview of audiovisual deepfake
generation techniques, applications, and their consequences, and then provides
a comprehensive review of state-of-the-art methods that combine audio and
visual modalities to enhance detection accuracy, summarizing and critically
analyzing their strengths and limitations. Furthermore, we discuss existing
open source datasets for a deeper understanding, which can contribute to the
research community and provide necessary information to beginners who want to
analyze deep learning-based audiovisual methods for video forensics. By
bridging the gap between unimodal and multimodal approaches, this paper aims to
improve the effectiveness of deepfake detection strategies and guide future
research in cybersecurity and media integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maritime Search and Rescue Missions with Aerial Images: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan P. Martinez-Esteso, Francisco J. Castellanos, Jorge Calvo-Zaragoza, Antonio Javier Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speed of response by search and rescue teams at sea is of vital
importance, as survival may depend on it. Recent technological advancements
have led to the development of more efficient systems for locating individuals
involved in a maritime incident, such as the use of Unmanned Aerial Vehicles
(UAVs) equipped with cameras and other integrated sensors. Over the past
decade, several researchers have contributed to the development of automatic
systems capable of detecting people using aerial images, particularly by
leveraging the advantages of deep learning. In this article, we provide a
comprehensive review of the existing literature on this topic. We analyze the
methods proposed to date, including both traditional techniques and more
advanced approaches based on machine learning and neural networks.
Additionally, we take into account the use of synthetic data to cover a wider
range of scenarios without the need to deploy a team to collect data, which is
one of the major obstacles for these systems. Overall, this paper situates the
reader in the field of detecting people at sea using aerial images by quickly
identifying the most suitable methodology for each scenario, as well as
providing an in-depth discussion and direction for future trends.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xCG: Explainable Cell Graphs for Survival Prediction in Non-Small Cell
  Lung Cancer <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Sextro, Gabriel Dernbach, Kai Standvoss, Simon Schallenberg, Frederick Klauschen, Klaus-Robert Müller, Maximilian Alber, Lukas Ruff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how deep learning models predict oncology patient risk can
provide critical insights into disease progression, support clinical
decision-making, and pave the way for trustworthy and data-driven precision
medicine. Building on recent advances in the spatial modeling of the tumor
microenvironment using graph neural networks, we present an explainable cell
graph (xCG) approach for survival prediction. We validate our model on a public
cohort of imaging mass cytometry (IMC) data for 416 cases of lung
adenocarcinoma. We explain survival predictions in terms of known phenotypes on
the cell level by computing risk attributions over cell graphs, for which we
propose an efficient grid-based layer-wise relevance propagation (LRP) method.
Our ablation studies highlight the importance of incorporating the cancer stage
and model ensembling to improve the quality of risk estimates. Our xCG method,
together with the IMC data, is made publicly available to support further
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings paper presented at Machine Learning for Health (ML4H)
  symposium 2024, December 15-16, 2024, Vancouver, Canada, 11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Low-Rank Dilemma of Linear Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihang Fan, Huaibo Huang, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Softmax attention mechanism in Transformer models is notoriously
computationally expensive, particularly due to its quadratic complexity, posing
significant challenges in vision applications. In contrast, linear attention
provides a far more efficient solution by reducing the complexity to linear
levels. However, compared to Softmax attention, linear attention often
experiences significant performance degradation. Our experiments indicate that
this performance drop is due to the low-rank nature of linear attention's
feature map, which hinders its ability to adequately model complex spatial
information. In this paper, to break the low-rank dilemma of linear attention,
we conduct rank analysis from two perspectives: the KV buffer and the output
features. Consequently, we introduce Rank-Augmented Linear Attention (RALA),
which rivals the performance of Softmax attention while maintaining linear
complexity and high efficiency. Based on RALA, we construct the Rank-Augmented
Vision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT
achieves excellent performance across various vision tasks. Specifically,
without using any additional labels, data, or supervision during training,
RAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters
and 4.6G FLOPs. This result significantly surpasses previous linear attention
mechanisms, fully illustrating the potential of RALA. Code will be available at
https://github.com/qhfan/RALA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Previous Steps: A Training-free Fast Solver for Flow
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Song, Hanjiang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow diffusion models (FDMs) have recently shown potential in generation
tasks due to the high generation quality. However, the current ordinary
differential equation (ODE) solver for FDMs, e.g., the Euler solver, still
suffers from slow generation since ODE solvers need many number function
evaluations (NFE) to keep high-quality generation. In this paper, we propose a
novel training-free flow-solver to reduce NFE while maintaining high-quality
generation. The key insight for the flow-solver is to leverage the previous
steps to reduce the NFE, where a cache is created to reuse these results from
the previous steps. Specifically, the Taylor expansion is first used to
approximate the ODE. To calculate the high-order derivatives of Taylor
expansion, the flow-solver proposes to use the previous steps and a polynomial
interpolation to approximate it, where the number of orders we could
approximate equals the number of previous steps we cached. We also prove that
the flow-solver has a more minor approximation error and faster generation
speed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,
LSUN-Church, ImageNet, and real text-to-image generation prove the efficiency
of the flow-solver. Specifically, the flow-solver improves the FID-30K from
13.79 to 6.75, from 46.64 to 19.49 with $\text{NFE}=10$ on CIFAR-10 and
LSUN-Church, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling the Connections between Flow Matching and Diffusion
  Probabilistic Models in Training-free Conditional Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Song, Hanjiang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training-free conditional generation aims to leverage the unconditional
diffusion models to implement the conditional generation, where flow-matching
(FM) and diffusion probabilistic models (DPMs) are two mature unconditional
diffusion models that achieve high-quality generation. Two questions were asked
in this paper: What are the underlying connections between FM and DPMs in
training-free conditional generation? Can we leverage DPMs to improve the
training-free conditional generation for FM? We first show that a probabilistic
diffusion path can be associated with the FM and DPMs. Then, we reformulate the
ordinary differential equation (ODE) of FM based on the score function of DPMs,
and thus, the conditions in FM can be incorporated as those in DPMs. Finally,
we propose two posterior sampling methods to estimate the conditional term and
achieve a training-free conditional generation of FM. Experimental results show
that our proposed method could be implemented for various conditional
generation tasks. Our method can generate higher-quality results than the
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mix from Failure: Confusion-Pairing Mixup for Long-Tailed Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngseok Yoon, Sangwoo Hong, Hyungjoon Joo, Yao Qin, Haewon Jeong, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-tailed image recognition is a computer vision problem considering a
real-world class distribution rather than an artificial uniform. Existing
methods typically detour the problem by i) adjusting a loss function, ii)
decoupling classifier learning, or iii) proposing a new multi-head architecture
called experts. In this paper, we tackle the problem from a different
perspective to augment a training dataset to enhance the sample diversity of
minority classes. Specifically, our method, namely Confusion-Pairing Mixup
(CP-Mix), estimates the confusion distribution of the model and handles the
data deficiency problem by augmenting samples from confusion pairs in
real-time. In this way, CP-Mix trains the model to mitigate its weakness and
distinguish a pair of classes it frequently misclassifies. In addition, CP-Mix
utilizes a novel mixup formulation to handle the bias in decision boundaries
that originated from the imbalanced dataset. Extensive experiments demonstrate
that CP-Mix outperforms existing methods for long-tailed image recognition and
successfully relieves the confusion of the classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence for Biomedical Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyuan Li, Jianing Qiu, Anujit Saha, Lin Li, Poyuan Li, Mengxian He, Ziyu Guo, Wu Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a prominent subfield of Artificial Intelligence Generated Content (AIGC),
video generation has achieved notable advancements in recent years. The
introduction of Sora-alike models represents a pivotal breakthrough in video
generation technologies, significantly enhancing the quality of synthesized
videos. Particularly in the realm of biomedicine, video generation technology
has shown immense potential such as medical concept explanation, disease
simulation, and biomedical data augmentation. In this article, we thoroughly
examine the latest developments in video generation models and explore their
applications, challenges, and future opportunities in the biomedical sector. We
have conducted an extensive review and compiled a comprehensive list of
datasets from various sources to facilitate the development and evaluation of
video generative models in biomedicine. Given the rapid progress in this field,
we have also created a github repository to regularly update the advances of
biomedical video generation at:
https://github.com/Lee728243228/Biomedical-Video-Generation
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Information-Empowered Graph Neural Network for Hyperspectral
  Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hsiang Lin, Tzu-Hsuan Lin, Jocelyn Chanussot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change detection (CD) is a critical remote sensing technique for identifying
changes in the Earth's surface over time. The outstanding substance
identifiability of hyperspectral images (HSIs) has significantly enhanced the
detection accuracy, making hyperspectral change detection (HCD) an essential
technology. The detection accuracy can be further upgraded by leveraging the
graph structure of HSIs, motivating us to adopt the graph neural networks
(GNNs) in solving HCD. For the first time, this work introduces quantum deep
network (QUEEN) into HCD. Unlike GNN and CNN, both extracting the
affine-computing features, QUEEN provides fundamentally different
unitary-computing features. We demonstrate that through the unitary feature
extraction procedure, QUEEN provides radically new information for deciding
whether there is a change or not. Hierarchically, a graph feature learning
(GFL) module exploits the graph structure of the bitemporal HSIs at the
superpixel level, while a quantum feature learning (QFL) module learns the
quantum features at the pixel level, as a complementary to GFL by preserving
pixel-level detailed spatial information not retained in the superpixels. In
the final classification stage, a quantum classifier is designed to cooperate
with a traditional fully connected classifier. The superior HCD performance of
the proposed QUEEN-empowered GNN (i.e., QUEEN-G) will be experimentally
demonstrated on real hyperspectral datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IEEE Transactions on Geoscience and
  Remote Sensing (TGRS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegQC: a segmentation network-based framework for multi-metric
  segmentation quality control and segmentation error detection in volumetric
  medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bella Specktor-Fadida, Liat Ben-Sira, Dafna Ben-Bashat, Leo Joskowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality control of structures segmentation in volumetric medical images is
important for identifying segmentation errors in clinical practice and for
facilitating model development. This paper introduces SegQC, a novel framework
for segmentation quality estimation and segmentation error detection. SegQC
computes an estimate measure of the quality of a segmentation in volumetric
scans and in their individual slices and identifies possible segmentation error
regions within a slice. The key components include: 1. SegQC-Net, a deep
network that inputs a scan and its segmentation mask and outputs segmentation
error probabilities for each voxel in the scan; 2. three new segmentation
quality metrics, two overlap metrics and a structure size metric, computed from
the segmentation error probabilities; 3. a new method for detecting possible
segmentation errors in scan slices computed from the segmentation error
probabilities. We introduce a new evaluation scheme to measure segmentation
error discrepancies based on an expert radiologist corrections of automatically
produced segmentations that yields smaller observer variability and is closer
to actual segmentation errors. We demonstrate SegQC on three fetal structures
in 198 fetal MRI scans: fetal brain, fetal body and the placenta. To assess the
benefits of SegQC, we compare it to the unsupervised Test Time Augmentation
(TTA)-based quality estimation. Our studies indicate that SegQC outperforms
TTA-based quality estimation in terms of Pearson correlation and MAE for fetal
body and fetal brain structures segmentation. Our segmentation error detection
method achieved recall and precision rates of 0.77 and 0.48 for fetal body, and
0.74 and 0.55 for fetal brain segmentation error detection respectively. SegQC
enhances segmentation metrics estimation for whole scans and individual slices,
as well as provides error regions detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounded Video Caption Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Kazakos, Cordelia Schmid, Josef Sivic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new task, dataset and model for grounded video caption
generation. This task unifies captioning and object grounding in video, where
the objects in the caption are grounded in the video via temporally consistent
bounding boxes. We introduce the following contributions. First, we present a
task definition and a manually annotated test dataset for this task, referred
to as GROunded Video Caption Generation (GROC). Second, we introduce a
large-scale automatic annotation method leveraging an existing model for
grounded still image captioning together with an LLM for summarising
frame-level captions into temporally consistent captions in video. Furthermore,
we prompt the LLM to track by language -- classifying noun phrases from the
frame-level captions into noun phrases of the video-level generated caption. We
apply this approach to videos from the HowTo100M dataset, which results in a
new large-scale training dataset, called HowToGround, with automatically
annotated captions and spatio-temporally consistent bounding boxes with
coherent natural language labels. Third, we introduce a new grounded video
caption generation model, called VideoGround, and train the model on the new
automatically annotated HowToGround dataset. Finally, results of our
VideoGround model set the state of the art for the new task of grounded video
caption generation. We perform extensive ablations and demonstrate the
importance of key technical contributions of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic segmentation on multi-resolution optical and microwave data
  using deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai G Singla, Bakul Vaghela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Presently, deep learning and convolutional neural networks (CNNs) are widely
used in the fields of image processing, image classification, object
identification and many more. In this work, we implemented convolutional neural
network based modified U-Net model and VGG-UNet model to automatically identify
objects from satellite imagery captured using high resolution Indian remote
sensing satellites and then to pixel wise classify satellite data into various
classes. In this paper, Cartosat 2S (~1m spatial resolution) datasets were used
and deep learning models were implemented to detect building shapes and ships
from the test datasets with an accuracy of more than 95%. In another
experiment, microwave data (varied resolution) from RISAT-1 was taken as an
input and ships and trees were detected with an accuracy of >96% from these
datasets. For the classification of images into multiple-classes, deep learning
model was trained on multispectral Cartosat images. Model generated results
were then tested using ground truth. Multi-label classification results were
obtained with an accuracy (IoU) of better than 95%. Total six different
problems were attempted using deep learning models and IoU accuracies in the
range of 85% to 98% were achieved depending on the degree of complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projecting Gaussian Ellipsoids While Avoiding Affine Projection
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Qi, Tao Cai, Xiyue Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its
real-time rendering speed and state-of-the-art rendering quality. However,
during the rendering process, the use of the Jacobian of the affine
approximation of the projection transformation leads to inevitable errors,
resulting in blurriness, artifacts and a lack of scene consistency in the final
rendered images. To address this issue, we introduce an ellipsoid-based
projection method to calculate the projection of Gaussian ellipsoid on the
image plane, witch is the primitive of 3D Gaussian Splatting. As our proposed
ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera
origins inside them or parts lying below $z=0$ plane in the camera space, we
designed a pre-filtering strategy. Experiments over multiple widely adopted
benchmark datasets show that using our ellipsoid-based projection method can
enhance the rendering quality of 3D Gaussian Splatting and its extensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atmospheric turbulence restoration by diffeomorphic image registration
  and blind deconvolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerome Gilles, Tristan Dagobert, Carlo De Franchis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel approach is presented in this paper to improve images which are
altered by atmospheric turbulence. Two new algorithms are presented based on
two combinations of a blind deconvolution block, an elastic registration block
and a temporal filter block. The algorithms are tested on real images acquired
in the desert in New Mexico by the NATO RTG40 group.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IR image databases generation under target intrinsic thermal variability
  constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerome Gilles, Stephane Landeau, Tristan Dagobert, Philippe Chevalier, Christian Bolut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with the problem of infrared image database generation for
ATR assessment purposes. Huge databases are required to have quantitative and
objective performance evaluations. We propose a method which superimpose
targets and occultants on background under image quality metrics constraints to
generate realistic images. We also propose a method to generate target
signatures with intrinsic thermal variability based on 3D models plated with
real infrared textures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2411.06695</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Génération de bases de données images IR sous contraintes avec
  variabilité thermique intrinsèque des cibles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerome Gilles, Stephane Landeau, Tristan Dagobert, Philippe Chevalier, Christian Bolut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this communication, we propose a method which permits to simulate images
of targets in infrared imagery by superimposition of vehicle signatures in
background, eventually with occultants. We develop a principle which authorizes
us to generate different thermal configurations of target signatures. This
method enables us to easily generate huge datasets for ATR algorithms
performance evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language, GRETSI Symposium on Signal and Image Processing,
  Dijon, France, September 2009</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Test-Time Adaptation for Inverse Consistent
  Diffeomorphic Lung Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad F. A. Chaudhary, Stephanie M. Aguilera, Arie Nakhmani, Joseph M. Reinhardt, Surya P. Bhatt, Sandeep Bodduluri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffeomorphic deformable image registration ensures smooth invertible
transformations across inspiratory and expiratory chest CT scans. Yet, in
practice, deep learning-based diffeomorphic methods struggle to capture large
deformations between inspiratory and expiratory volumes, and therefore lack
inverse consistency. Existing methods also fail to account for model
uncertainty, which can be useful for improving performance. We propose an
uncertainty-aware test-time adaptation framework for inverse consistent
diffeomorphic lung registration. Our method uses Monte Carlo (MC) dropout to
estimate spatial uncertainty that is used to improve model performance. We
train and evaluate our method for inspiratory-to-expiratory CT registration on
a large cohort of 675 subjects from the COPDGene study, achieving a higher Dice
similarity coefficient (DSC) between the lung boundaries (0.966) compared to
both VoxelMorph (0.953) and TransMorph (0.953). Our method demonstrates
consistent improvements in the inverse registration direction as well with an
overall DSC of 0.966, higher than VoxelMorph (0.958) and TransMorph (0.956).
Paired t-tests indicate statistically significant improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-task Feature Enhancement Network for No-Reference Image Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the scarcity of labeled samples in Image Quality Assessment (IQA)
datasets, numerous recent studies have proposed multi-task based strategies,
which explore feature information from other tasks or domains to boost the IQA
task. Nevertheless, multi-task strategies based No-Reference Image Quality
Assessment (NR-IQA) methods encounter several challenges. First, existing
methods have not explicitly exploited texture details, which significantly
influence the image quality. Second, multi-task methods conventionally
integrate features through simple operations such as addition or concatenation,
thereby diminishing the network's capacity to accurately represent distorted
features. To tackle these challenges, we introduce a novel multi-task NR-IQA
framework. Our framework consists of three key components: a high-frequency
extraction network, a quality estimation network, and a distortion-aware
network. The high-frequency extraction network is designed to guide the model's
focus towards high-frequency information, which is highly related to the
texture details. Meanwhile, the distortion-aware network extracts
distortion-related features to distinguish different distortion types. To
effectively integrate features from different tasks, a feature fusion module is
developed based on an attention mechanism. Empirical results from five standard
IQA databases confirm that our method not only achieves high performance but
also exhibits robust generalization ability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianCut: Interactive segmentation via graph cut for 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umangi Jain, Ashkan Mirzaei, Igor Gilitschenski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GaussianCut, a new method for interactive multiview segmentation
of scenes represented as 3D Gaussians. Our approach allows for selecting the
objects to be segmented by interacting with a single view. It accepts intuitive
user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian
Splatting (3DGS) as the underlying scene representation simplifies the
extraction of objects of interest which are considered to be a subset of the
scene's Gaussians. Our key idea is to represent the scene as a graph and use
the graph-cut algorithm to minimize an energy function to effectively partition
the Gaussians into foreground and background. To achieve this, we construct a
graph based on scene Gaussians and devise a segmentation-aligned energy
function on the graph to combine user inputs with scene properties. To obtain
an initial coarse segmentation, we leverage 2D image/video segmentation models
and further refine these coarse estimates using our graph construction. Our
empirical evaluations show the adaptability of GaussianCut across a diverse set
of scenes. GaussianCut achieves competitive performance with state-of-the-art
approaches for 3D segmentation without requiring any additional
segmentation-aware training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Language <span class="highlight-title">Prompt</span>ing to Ease False Positives in Medical
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Myung Jin Kim, Hyeong Seok Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A pre-trained visual-language model, contrastive language-image pre-training
(CLIP), successfully accomplishes various downstream tasks with text prompts,
such as finding images or localizing regions within the image. Despite CLIP's
strong multi-modal data capabilities, it remains limited in specialized
environments, such as medical applications. For this purpose, many CLIP
variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives
related to normal regions persist. Thus, we aim to present a simple yet
important goal of reducing false positives in medical anomaly detection. We
introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both
positive and negative text prompts. This straightforward approach identifies
potential lesion regions by visual attention to the positive prompts in the
given image. To reduce false positives, we attenuate attention on normal
regions using negative prompts. Extensive experiments with the BMAD dataset,
including six biomedical benchmarks, demonstrate that CLAP method enhances
anomaly detection performance. Our future plans include developing an automated
fine prompting method for more practical usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depthwise Separable Convolutions with Deep Residual Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Arid Hasan, Krishno Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancement of edge computing enables researchers to optimize
various deep learning architectures to employ them in edge devices. In this
study, we aim to optimize Xception architecture which is one of the most
popular deep learning algorithms for computer vision applications. The Xception
architecture is highly effective for object detection tasks. However, it comes
with a significant computational cost. The computational complexity of Xception
sometimes hinders its deployment on resource-constrained edge devices. To
address this, we propose an optimized Xception architecture tailored for edge
devices, aiming for lightweight and efficient deployment. We incorporate the
depthwise separable convolutions with deep residual convolutions of the
Xception architecture to develop a small and efficient model for edge devices.
The resultant architecture reduces parameters, memory usage, and computational
load. The proposed architecture is evaluated on the CIFAR 10 object detection
dataset. The evaluation result of our experiment also shows the proposed
architecture is smaller in parameter size and requires less training time while
outperforming Xception architecture performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Course Project Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D
  Gaussian Splatting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiankun Gao, Jiarui Meng, Chengxiang Wen, Jie Chen, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The online reconstruction of dynamic scenes from multi-view streaming videos
faces significant challenges in training, rendering and storage efficiency.
Harnessing superior learning speed and real-time rendering capabilities, 3D
Gaussian Splatting (3DGS) has recently demonstrated considerable potential in
this field. However, 3DGS can be inefficient in terms of storage and prone to
overfitting by excessively growing Gaussians, particularly with limited views.
This paper proposes an efficient framework, dubbed HiCoM, with three key
components. First, we construct a compact and robust initial 3DGS
representation using a perturbation smoothing strategy. Next, we introduce a
Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform
distribution and local consistency of 3D Gaussians to swiftly and accurately
learn motions across frames. Finally, we continually refine the 3DGS with
additional Gaussians, which are later merged into the initial 3DGS to maintain
consistency with the evolving scene. To preserve a compact representation, an
equivalent number of low-opacity Gaussians that minimally impact the
representation are removed before processing subsequent frames. Extensive
experiments conducted on two widely used datasets show that our framework
improves learning efficiency of the state-of-the-art methods by about $20\%$
and reduces the data storage by $85\%$, achieving competitive free-viewpoint
video synthesis quality but with higher robustness and stability. Moreover, by
parallel learning multiple frames simultaneously, our HiCoM decreases the
average training wall time to $<2$ seconds per frame with negligible
performance degradation, substantially boosting real-world applicability and
responsiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024; Code is avaliable at
  https://github.com/gqk/HiCoM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparrowVQE: Visual Question Explanation for Course Content Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialu Li, Manish Kumar Thota, Ruslan Gokhman, Radek Holik, Youshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) research seeks to create AI systems to answer
natural language questions in images, yet VQA methods often yield overly
simplistic and short answers. This paper aims to advance the field by
introducing Visual Question Explanation (VQE), which enhances the ability of
VQA to provide detailed explanations rather than brief responses and address
the need for more complex interaction with visual content. We first created an
MLVQE dataset from a 14-week streamed video machine learning course, including
885 slide images, 110,407 words of transcripts, and 9,416 designed
question-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3
billion parameters multimodal model. We trained our model with a three-stage
training mechanism consisting of multimodal pre-training (slide images and
transcripts feature alignment), instruction tuning (tuning the pre-trained
model with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide
image and QA pairs). Eventually, our SparrowVQE can understand and connect
visual information using the SigLIP model with transcripts using the Phi-2
language model with an MLP adapter. Experimental results demonstrate that our
SparrowVQE achieves better performance in our developed MLVQE dataset and
outperforms state-of-the-art methods in the other five benchmark VQA datasets.
The source code is available at
\url{https://github.com/YoushanZhang/SparrowVQE}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Automatic Real-time Motion Tracking Method for Magnetic
  Resonance Imaging-guided Radiotherapy: Leveraging the Enhanced
  Tracking-Learning-Detection Framework with Automatic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengqi Chen, Zilin Wang, Jianrong Dai, Shirui Qin, Ying Cao, Ruiao Zhao, Jiayun Chen, Guohua Wu, Yuan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Ensuring the precision in motion tracking for MRI-guided
Radiotherapy (MRIgRT) is crucial for the delivery of effective treatments. This
study refined the motion tracking accuracy in MRIgRT through the innovation of
an automatic real-time tracking method, leveraging an enhanced
Tracking-Learning-Detection (ETLD) framework coupled with automatic
segmentation. Methods: We developed a novel MRIgRT motion tracking method by
integrating two primary methods: the ETLD framework and an improved Chan-Vese
model (ICV), named ETLD+ICV. The TLD framework was upgraded to suit real-time
cine MRI, including advanced image preprocessing, no-reference image quality
assessment, an enhanced median-flow tracker, and a refined detector with
dynamic search region adjustments. Additionally, ICV was combined for precise
coverage of the target volume, which refined the segmented region frame by
frame using tracking results, with key parameters optimized. Tested on 3.5D MRI
scans from 10 patients with liver metastases, our method ensures precise
tracking and accurate segmentation vital for MRIgRT. Results: An evaluation of
106,000 frames across 77 treatment fractions revealed sub-millimeter tracking
errors of less than 0.8mm, with over 99% precision and 98% recall for all
subjects, underscoring the robustness and efficacy of the ETLD. Moreover, the
ETLD+ICV yielded a dice global score of more than 82% for all subjects,
demonstrating the proposed method's extensibility and precise target volume
coverage. Conclusions: This study successfully developed an automatic real-time
motion tracking method for MRIgRT that markedly surpasses current methods. The
novel method not only delivers exceptional precision in tracking and
segmentation but also demonstrates enhanced adaptability to clinical demands,
positioning it as an indispensable asset in the quest to augment the efficacy
of radiotherapy treatments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAUREL: Learned Augmented Residual Layer <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Menghani, Ravi Kumar, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the core pillars of efficient deep learning methods is architectural
improvements such as the residual/skip connection, which has led to
significantly better model convergence and quality. Since then the residual
connection has become ubiquitous in not just convolutional neural networks but
also transformer-based architectures, the backbone of LLMs.
  In this paper we introduce \emph{Learned Augmented Residual Layer} (LAuReL)
-- a novel generalization of the canonical residual connection -- with the goal
to be an in-situ replacement of the latter while outperforming on both model
quality and footprint metrics. Our experiments show that using \laurel can help
boost performance for both vision and language models. For example, on the
ResNet-50, ImageNet 1K task, it achieves $60\%$ of the gains from adding an
extra layer, while only adding $0.003\%$ more parameters, and matches it while
adding $2.6\times$ fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2nd Efficient Systems for Foundation Models Workshop
  at the International Conference on Machine Learning (ICML) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Knowledge Distillation Using Partial Information
  Decomposition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasan Dissanayake, Faisal Hamman, Barproda Halder, Ilia Sucholutsky, Qiuyi Zhang, Sanghamitra Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation provides an effective method for deploying complex
machine learning models in resource-constrained environments. It typically
involves training a smaller student model to emulate either the probabilistic
outputs or the internal feature representations of a larger teacher model. By
doing so, the student model often achieves substantially better performance on
a downstream task compared to when it is trained independently. Nevertheless,
the teacher's internal representations can also encode noise or additional
information that may not be relevant to the downstream task. This observation
motivates our primary question: What are the information-theoretic limits of
knowledge transfer? To this end, we leverage a body of work in information
theory called Partial Information Decomposition (PID) to quantify the
distillable and distilled knowledge of a teacher's representation corresponding
to a given student and a downstream task. Moreover, we demonstrate that this
metric can be practically used in distillation to address challenges caused by
the complexity gap between the teacher and the student representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Machine Learning and Compression Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUS-IR: Gaussian Splatting with Unified Shading for Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Liang, Hongdong Li, Kui Jia, Kailing Guo, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering the intrinsic physical attributes of a scene from images,
generally termed as the inverse rendering problem, has been a central and
challenging task in computer vision and computer graphics. In this paper, we
present GUS-IR, a novel framework designed to address the inverse rendering
problem for complicated scenes featuring rough and glossy surfaces. This paper
starts by analyzing and comparing two prominent shading techniques popularly
used for inverse rendering, forward shading and deferred shading, effectiveness
in handling complex materials. More importantly, we propose a unified shading
solution that combines the advantages of both techniques for better
decomposition. In addition, we analyze the normal modeling in 3D Gaussian
Splatting (3DGS) and utilize the shortest axis as normal for each particle in
GUS-IR, along with a depth-related regularization, resulting in improved
geometric representation and better shape reconstruction. Furthermore, we
enhance the probe-based baking scheme proposed by GS-IR to achieve more
accurate ambient occlusion modeling to better handle indirect illumination.
Extensive experiments have demonstrated the superior performance of GUS-IR in
achieving precise intrinsic decomposition and geometric representation,
supporting many downstream tasks (such as relighting, retouching) in computer
vision, graphics, and extended reality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Truths: A Large-Scale <span class="highlight-title">Dataset</span> of AI-Augmented Images for Evaluating
  Robustness of AI-Generated Image detectors <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anisha Pal, Julia Kruk, Mansi Phute, Manognya Bhattaram, Diyi Yang, Duen Horng Chau, Judy Hoffman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have impactful applications in art, design,
and entertainment, yet these technologies also pose significant risks by
enabling the creation and dissemination of misinformation. Although recent
advancements have produced AI-generated image detectors that claim robustness
against various augmentations, their true effectiveness remains uncertain. Do
these detectors reliably identify images with different levels of augmentation?
Are they biased toward specific scenes or data distributions? To investigate,
we introduce SEMI-TRUTHS, featuring 27,600 real images, 223,400 masks, and
1,472,700 AI-augmented images that feature targeted and localized perturbations
produced using diverse augmentation techniques, diffusion models, and data
distributions. Each augmented image is accompanied by metadata for standardized
and targeted evaluation of detector robustness. Our findings suggest that
state-of-the-art detectors exhibit varying sensitivities to the types and
degrees of perturbations, data distributions, and augmentation methods used,
offering new insights into their performance and limitations. The code for the
augmentation and evaluation pipeline is available at
https://github.com/J-Kruk/SemiTruths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Track Datasets & Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSEG-VCUQ: Multimodal SEGmentation with Enhanced Vision Foundation
  Models, Convolutional Neural Networks, and Uncertainty Quantification for
  High-Speed Video Phase Detection Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chika Maduabuchi, Ericmoore Jossou, Matteo Bucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: High-speed video (HSV) phase detection (PD) segmentation is vital in
nuclear reactors, chemical processing, and electronics cooling for detecting
vapor, liquid, and microlayer phases. Traditional segmentation models face
pixel-level accuracy and generalization issues in multimodal data. MSEG-VCUQ
introduces VideoSAM, a hybrid framework leveraging convolutional neural
networks (CNNs) and transformer-based vision models to enhance segmentation
accuracy and generalizability across complex multimodal PD tasks. Methods:
VideoSAM combines U-Net CNN and the Segment Anything Model (SAM) for advanced
feature extraction and segmentation across diverse HSV PD modalities, spanning
fluids like water, FC-72, nitrogen, and argon under varied heat flux
conditions. The framework also incorporates uncertainty quantification (UQ) to
assess pixel-based discretization errors, delivering reliable metrics such as
contact line density and dry area fraction under experimental conditions.
Results: VideoSAM outperforms SAM and modality-specific CNN models in
segmentation accuracy, excelling in environments with complex phase boundaries,
overlapping bubbles, and dynamic liquid-vapor interactions. Its hybrid
architecture supports cross-dataset generalization, adapting effectively to
varying modalities. The UQ module provides accurate error estimates, enhancing
the reliability of segmentation outputs for advanced HSV PD research.
Conclusion: MSEG-VCUQ, via VideoSAM, offers a robust solution for HSV PD
segmentation, addressing previous limitations with advanced deep learning and
UQ techniques. The open-source datasets and tools introduced enable scalable,
precise, and adaptable segmentation for multimodal PD datasets, supporting
advancements in HSV analysis and autonomous experimentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in EAAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MureObjectStitch: Multi-reference Image Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Chen, Bo Zhang, Li Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative image composition aims to regenerate the given foreground object
in the background image to produce a realistic composite image. In this work,
we propose an effective finetuning strategy for generative image composition
model, in which we finetune a pretrained model using one or more images
containing the same foreground object. Moreover, we propose a multi-reference
strategy, which allows the model to take in multiple reference images of the
foreground object. The experiments on MureCOM dataset verify the effectiveness
of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anas Awadalla, Le Xue, Manli Shu, An Yan, Jun Wang, Senthil Purushwalkam, Sheng Shen, Hannah Lee, Oscar Lo, Jae Sung Park, Etash Guha, Silvio Savarese, Ludwig Schmidt, Yejin Choi, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that
bridges the gap between descriptive synthetic captions and factual web-scale
alt-text. KALE augments synthetic dense image captions with web-scale alt-text
to generate factually grounded image captions. Our two-stage approach leverages
large vision-language models and language models to create knowledge-augmented
captions, which are then used to train a specialized VLM for scaling up the
dataset. We train vision-language models on KALE and demonstrate improvements
on vision-language tasks. Our experiments show the utility of KALE for training
more capable and knowledgeable multimodal models. We release the KALE dataset
at https://huggingface.co/datasets/Salesforce/blip3-kale
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracing the Roots: Leveraging Temporal Dynamics in Diffusion
  Trajectories for Origin Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have revolutionized image synthesis, garnering significant
research interest in recent years. Diffusion is an iterative algorithm in which
samples are generated step-by-step, starting from pure noise. This process
introduces the notion of diffusion trajectories, i.e., paths from the standard
Gaussian distribution to the target image distribution. In this context, we
study discriminative algorithms operating on these trajectories. Specifically,
given a pre-trained diffusion model, we consider the problem of classifying
images as part of the training dataset, generated by the model or originating
from an external source. Our approach demonstrates the presence of patterns
across steps that can be leveraged for classification. We also conduct ablation
studies, which reveal that using higher-order gradient features to characterize
the trajectories leads to significant performance gains and more robust
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-in-one Weather-degraded Image Restoration via Adaptive
  Degradation-aware Self-<span class="highlight-title">prompt</span>ing Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanbo Wen, Tao Gao, Ziqi Li, Jing Zhang, Kaihao Zhang, Ting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches for all-in-one weather-degraded image restoration suffer
from inefficiencies in leveraging degradation-aware priors, resulting in
sub-optimal performance in adapting to different weather conditions. To this
end, we develop an adaptive degradation-aware self-prompting model (ADSM) for
all-in-one weather-degraded image restoration. Specifically, our model employs
the contrastive language-image pre-training model (CLIP) to facilitate the
training of our proposed latent prompt generators (LPGs), which represent three
types of latent prompts to characterize the degradation type, degradation
property and image caption. Moreover, we integrate the acquired
degradation-aware prompts into the time embedding of diffusion model to improve
degradation perception. Meanwhile, we employ the latent caption prompt to guide
the reverse sampling process using the cross-attention mechanism, thereby
guiding the accurate image reconstruction. Furthermore, to accelerate the
reverse sampling procedure of diffusion model and address the limitations of
frequency perception, we introduce a wavelet-oriented noise estimating network
(WNE-Net). Extensive experiments conducted on eight publicly available datasets
demonstrate the effectiveness of our proposed approach in both task-specific
and all-in-one applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and
  200+ FPS <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17245v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17245v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in real-time neural rendering using point-based techniques
have enabled broader adoption of 3D representations. However, foundational
approaches like 3D Gaussian Splatting impose substantial storage overhead, as
Structure-from-Motion (SfM) points can grow to millions, often requiring
gigabyte-level disk space for a single unbounded scene. This growth presents
scalability challenges and hinders splatting efficiency. To address this, we
introduce LightGaussian, a method for transforming 3D Gaussians into a more
compact format. Inspired by Network Pruning, LightGaussian identifies Gaussians
with minimal global significance on scene reconstruction, and applies a pruning
and recovery process to reduce redundancy while preserving visual quality.
Knowledge distillation and pseudo-view augmentation then transfer spherical
harmonic coefficients to a lower degree, yielding compact representations.
Gaussian Vector Quantization, based on each Gaussian's global significance,
further lowers bitwidth with minimal accuracy loss. LightGaussian achieves an
average 15x compression rate while boosting FPS from 144 to 237 within the
3D-GS framework, enabling efficient complex scene representation on the
Mip-NeRF 360 and Tank & Temple datasets. The proposed Gaussian pruning approach
is also adaptable to other 3D representations (e.g., Scaffold-GS),
demonstrating strong generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Project page: https://lightgaussian.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Odd-One-Out: Anomaly Detection by Comparing with Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Bhunia, Changjian Li, Hakan Bilen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel anomaly detection (AD) problem that focuses on
identifying `odd-looking' objects relative to the other instances in a given
scene. In contrast to the traditional AD benchmarks, anomalies in our task are
scene-specific, defined by the regular instances that make up the majority.
Since object instances may be only partly visible from a single viewpoint, our
setting employs multiple views of each scene as input. To provide a testbed for
future research in this task, we introduce two benchmarks, ToysAD-8K and
PartsAD-15K. We propose a novel method that constructs 3D object-centric
representations from multiple 2D views for each instance and detects the
anomalous ones through a cross-instance comparison. We rigorously analyze our
method quantitatively and qualitatively on the presented benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes & Dataset at https://github.com/VICO-UoE/OddOneOutAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WavShadow: Wavelet Based Shadow Segmentation and Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05747v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05747v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyans Jain, Viraj Vekaria, Karan Gandhi, Aadya Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shadow removal and segmentation remain challenging tasks in computer vision,
particularly in complex real world scenarios. This study presents a novel
approach that enhances the ShadowFormer model by incorporating Masked
Autoencoder (MAE) priors and Fast Fourier Convolution (FFC) blocks, leading to
significantly faster convergence and improved performance. We introduce key
innovations: (1) integration of MAE priors trained on Places2 dataset for
better context understanding, (2) adoption of Haar wavelet features for
enhanced edge detection and multiscale analysis, and (3) implementation of a
modified SAM Adapter for robust shadow segmentation. Extensive experiments on
the challenging DESOBA dataset demonstrate that our approach achieves state of
the art results, with notable improvements in both convergence speed and shadow
removal quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-Learned Modality-Weighted Knowledge Distillation for Robust
  Multi-Modal Learning with Missing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Wang, Salma Hassan, Yuyuan Liu, Congbo Ma, Yuanhong Chen, Yutong Xie, Mostafa Salem, Yu Tian, Jodie Avery, Louise Hull, Ian Reid, Mohammad Yaqub, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-modal learning, some modalities are more influential than others,
and their absence can have a significant impact on classification/segmentation
accuracy. Addressing this challenge, we propose a novel approach called
Meta-learned Modality-weighted Knowledge Distillation (MetaKD), which enables
multi-modal models to maintain high accuracy even when key modalities are
missing. MetaKD adaptively estimates the importance weight of each modality
through a meta-learning process. These learned importance weights guide a
pairwise modality-weighted knowledge distillation process, allowing
high-importance modalities to transfer knowledge to lower-importance ones,
resulting in robust performance despite missing inputs. Unlike previous methods
in the field, which are often task-specific and require significant
modifications, our approach is designed to work in multiple tasks (e.g.,
segmentation and classification) with minimal adaptation. Experimental results
on five prevalent datasets, including three Brain Tumor Segmentation datasets
(BraTS2018, BraTS2019 and BraTS2020), the Alzheimer's Disease Neuroimaging
Initiative (ADNI) classification dataset and the Audiovision-MNIST
classification dataset, demonstrate the proposed model is able to outperform
the compared models by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpret Your Decision: Logical Reasoning Regularization for
  Generalization in Visual Classification <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04492v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04492v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision models excel in image classification but struggle to generalize to
unseen data, such as classifying images from unseen domains or discovering
novel categories. In this paper, we explore the relationship between logical
reasoning and deep learning generalization in visual classification. A logical
regularization termed L-Reg is derived which bridges a logical analysis
framework to image classification. Our work reveals that L-Reg reduces the
complexity of the model in terms of the feature distribution and classifier
weights. Specifically, we unveil the interpretability brought by L-Reg, as it
enables the model to extract the salient features, such as faces to persons,
for classification. Theoretical analysis and experiments demonstrate that L-Reg
enhances generalization across various scenarios, including multi-domain
generalization and generalized category discovery. In complex real-world
scenarios where images span unknown classes and unseen domains, L-Reg
consistently improves generalization, highlighting its practical efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-triplet Guided Few-shot Composed Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Hou, Haoqiang Lin, Haokun Wen, Meng Liu, Mingzhu Xu, Xuemeng Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Image Retrieval (CIR) is a challenging task that aims to retrieve
the target image with a multimodal query, i.e., a reference image, and its
complementary modification text. As previous supervised or zero-shot learning
paradigms all fail to strike a good trade-off between the model's
generalization ability and retrieval performance, recent researchers have
introduced the task of few-shot CIR (FS-CIR) and proposed a textual
inversion-based network based on pretrained CLIP model to realize it. Despite
its promising performance, the approach encounters two key limitations: simply
relying on the few annotated samples for CIR model training and
indiscriminately selecting training triplets for CIR model fine-tuning. To
address these two limitations, we propose a novel two-stage pseudo triplet
guided few-shot CIR scheme, dubbed PTG-FSCIR. In the first stage, we propose an
attentive masking and captioning-based pseudo triplet generation method, to
construct pseudo triplets from pure image data and use them to fulfill the
CIR-task specific pertaining. In the second stage, we propose a challenging
triplet-based CIR fine-tuning method, where we design a pseudo modification
text-based sample challenging score estimation strategy and a robust top
range-based random sampling strategy for sampling robust challenging triplets
to promote the model fine-tuning. Notably, our scheme is plug-and-play and
compatible with any existing supervised CIR models. We test our scheme across
two backbones on three public datasets (i.e., FashionIQ, CIRR, and
Birds-to-Words), achieving maximum improvements of 13.3%, 22.2%, and 17.4%
respectively, demonstrating our scheme's efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Reinforcement Learning with Imitation for Vision-Based
  Agile Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12203v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12203v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxu Xing, Angel Romero, Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning visuomotor policies for agile quadrotor flight presents significant
difficulties, primarily from inefficient policy exploration caused by
high-dimensional visual inputs and the need for precise and low-latency
control. To address these challenges, we propose a novel approach that combines
the performance of Reinforcement Learning (RL) and the sample efficiency of
Imitation Learning (IL) in the task of vision-based autonomous drone racing.
While RL provides a framework for learning high-performance controllers through
trial and error, it faces challenges with sample efficiency and computational
demands due to the high dimensionality of visual inputs. Conversely, IL
efficiently learns from visual expert demonstrations, but it remains limited by
the expert's performance and state distribution. To overcome these limitations,
our policy learning framework integrates the strengths of both approaches. Our
framework contains three phases: training a teacher policy using RL with
privileged state information, distilling it into a student policy via IL, and
adaptive fine-tuning via RL. Testing in both simulated and real-world scenarios
shows our approach can not only learn in scenarios where RL from scratch fails
but also outperforms existing IL methods in both robustness and performance,
successfully navigating a quadrotor through a race course using only visual
information. Videos of the experiments are available at
https://rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8th Annual Conference on Robot Learning (CoRL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REVEX: A Unified Framework for Removal-Based Explainable Artificial
  Intelligence in Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. Xavier Gaya-Morey, Jose M. Buades-Rubio, I. Scott MacKenzie, Cristina Manresa-Yee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We developed REVEX, a removal-based video explanations framework. This work
extends fine-grained explanation frameworks for computer vision data and adapts
six existing techniques to video by adding temporal information and local
explanations. The adapted methods were evaluated across networks, datasets,
image classes, and evaluation metrics. By decomposing explanation into steps,
strengths and weaknesses were revealed in the studied methods, for example, on
pixel clustering and perturbations in the input. Video LIME outperformed other
methods with deletion values up to 31\% lower and insertion up to 30\% higher,
depending on method and network. Video RISE achieved superior performance in
the average drop metric, with values 10\% lower. In contrast,
localization-based metrics revealed low performance across all methods, with
significant variation depending on network. Pointing game accuracy reached
53\%, and IoU-based metrics remained below 20\%. Drawing on the findings across
XAI methods, we further examine the limitations of the employed XAI evaluation
metrics and highlight their suitability in different applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transfer Learning for Wildlife Classification: Evaluating YOLOv8 against
  DenseNet, ResNet, and VGGNet on a Custom <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subek Sharma, Sisir Dhakal, Mansi Bhavsar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the performance of various deep learning models,
specifically DenseNet, ResNet, VGGNet, and YOLOv8, for wildlife species
classification on a custom dataset. The dataset comprises 575 images of 23
endangered species sourced from reputable online repositories. The study
utilizes transfer learning to fine-tune pre-trained models on the dataset,
focusing on reducing training time and enhancing classification accuracy. The
results demonstrate that YOLOv8 outperforms other models, achieving a training
accuracy of 97.39% and a validation F1-score of 96.50%. These findings suggest
that YOLOv8, with its advanced architecture and efficient feature extraction
capabilities, holds great promise for automating wildlife monitoring and
conservation efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is published in Journal of Artificial Intelligence and Capsule
  Networks, December 2024, Volume 6, Issue 4, Pages 415-435</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Can Evolve Continually on Modality for X-Modal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazuo Yu, Haomiao Xiong, Lu Zhang, Haiwen Diao, Yunzhi Zhuge, Lanqing Hong, Dong Wang, Huchuan Lu, You He, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have gained significant attention
due to their impressive capabilities in multimodal understanding. However,
existing methods rely heavily on extensive modal-specific pretraining and
joint-modal tuning, leading to significant computational burdens when expanding
to new modalities. In this paper, we propose PathWeave, a flexible and scalable
framework with modal-Path sWitching and ExpAnsion abilities that enables MLLMs
to continually EVolve on modalities for $\mathbb{X}$-modal reasoning. We
leverage the concept of Continual Learning and develop an incremental training
strategy atop pre-trained MLLMs, enabling their expansion to new modalities
using uni-modal data, without executing joint-modal pretraining. In detail, a
novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and
cross-modal adapters are seamlessly integrated to facilitate efficient modality
alignment and collaboration. Additionally, an MoE-based gating module is
applied between two types of adapters to further enhance the multimodal
interaction. To investigate the proposed method, we establish a challenging
benchmark called Continual Learning of Modality (MCL), which consists of
high-quality QA data from five distinct modalities: image, video, audio, depth
and point cloud. Extensive experiments demonstrate the effectiveness of the
proposed AnA framework on learning plasticity and memory stability during
continual learning. Furthermore, PathWeave performs comparably to
state-of-the-art MLLMs while concurrently reducing parameter training burdens
by 98.73%. Our code locates at https://github.com/JiazuoYu/PathWeave
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03677v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03677v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Huang, Ziyu Xu, Hai Wu, Jinlong Wang, Qiming Xia, Yan Xia, Jonathan Li, Kyle Gao, Chenglu Wen, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based vision systems are integral for 3D object detection, which is
crucial for autonomous navigation. However, they suffer from performance
degradation in adverse weather conditions due to the quality deterioration of
LiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is
expected to solve this problem. However, the fusion of LiDAR and 4D radar is
challenging because they differ significantly in terms of data quality and the
degree of degradation in adverse weather. To address these issues, we introduce
L4DR, a weather-robust 3D object detection method that effectively achieves
LiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and
Foreground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is
the first exploration of the complementarity of early fusion between LiDAR and
4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )
parallel feature extraction backbone coupled with a Multi-Scale Gated Fusion
(MSGF) module to counteract the varying degrees of sensor degradation under
adverse weather conditions. Experimental evaluation on a VoD dataset with
simulated fog proves that L4DR is more adaptable to changing weather
conditions. It delivers a significant performance increase under different fog
levels, improving the 3D mAP by up to 20.0% over the traditional LiDAR-only
approach. Moreover, the results on the K-Radar dataset validate the consistent
performance improvement of L4DR in real-world adverse weather conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Segment Anything Model to Multi-modal Salient Object Detection
  with Semantic Feature Fusion Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15063v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15063v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunpeng Wang, Danying Lin, Chenglong Li, Zhengzheng Tu, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although most existing multi-modal salient object detection (SOD) methods
demonstrate effectiveness through training models from scratch, the limited
multi-modal data hinders these methods from reaching optimality. In this paper,
we propose a novel framework to explore and exploit the powerful feature
representation and zero-shot generalization ability of the pre-trained Segment
Anything Model (SAM) for multi-modal SOD. Despite serving as a recent vision
fundamental model, driving the class-agnostic SAM to comprehend and detect
salient objects accurately is non-trivial, especially in challenging scenes. To
this end, we develop \underline{SAM} with se\underline{m}antic
f\underline{e}ature fu\underline{s}ion guidanc\underline{e} (Sammese), which
incorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to
multi-modal SOD tasks. However, it is difficult for SAM trained on single-modal
data to directly mine the complementary benefits of multi-modal inputs and
comprehensively utilize them to achieve accurate saliency prediction. To
address these issues, we first design a multi-modal complementary fusion module
to extract robust multi-modal semantic features by integrating information from
visible and thermal or depth image pairs. Then, we feed the extracted
multi-modal semantic features into both the SAM image encoder and mask decoder
for fine-tuning and prompting, respectively. Specifically, in the image
encoder, a multi-modal adapter is proposed to adapt the single-modal SAM to
multi-modal information. In the mask decoder, a semantic-geometric prompt
generation strategy is proposed to produce corresponding embeddings with
various saliency cues. Extensive experiments on both RGB-D and RGB-T SOD
benchmarks show the effectiveness of the proposed framework. The code will be
available at \url{https://github.com/Angknpng/Sammese}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for
  Adversarial Defense <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite ongoing efforts to defend neural classifiers from adversarial
attacks, they remain vulnerable, especially to unseen attacks. In contrast,
humans are difficult to be cheated by subtle manipulations, since we make
judgments only based on essential factors. Inspired by this observation, we
attempt to model label generation with essential label-causative factors and
incorporate label-non-causative factors to assist data generation. For an
adversarial example, we aim to discriminate the perturbations as non-causative
factors and make predictions only based on the label-causative factors.
Concretely, we propose a casual diffusion model (CausalDiff) that adapts
diffusion models for conditional data generation and disentangles the two types
of casual factors by learning towards a novel casual information bottleneck
objective. Empirically, CausalDiff has significantly outperformed
state-of-the-art defense methods on various unseen attacks, achieving an
average robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on
CIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition
Benchmark).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Act in Collusion: A Persistent Distributed Multi-Target Backdoor in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Liu, Wu Yang, Chen Xu, Jiguang Lv, Huanran Wang, Yuhang Zhang, Shuchun Xu, Dapeng Man
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning, a novel paradigm designed to protect data privacy, is
vulnerable to backdoor attacks due to its distributed nature. Current research
often designs attacks based on a single attacker with a single backdoor,
overlooking more realistic and complex threats in federated learning. We
propose a more practical threat model for federated learning: the distributed
multi-target backdoor. In this model, multiple attackers control different
clients, embedding various triggers and targeting different classes,
collaboratively implanting backdoors into the global model via central
aggregation. Empirical validation shows that existing methods struggle to
maintain the effectiveness of multiple backdoors in the global model. Our key
insight is that similar backdoor triggers cause parameter conflicts and
injecting new backdoors disrupts gradient directions, significantly weakening
some backdoors performance. To solve this, we propose a Distributed
Multi-Target Backdoor Attack (DMBA), ensuring efficiency and persistence of
backdoors from different malicious clients. To avoid parameter conflicts, we
design a multi-channel dispersed frequency trigger strategy to maximize trigger
differences. To mitigate gradient interference, we introduce backdoor replay in
local training to neutralize conflicting gradients. Extensive validation shows
that 30 rounds after the attack, Attack Success Rates of three different
backdoors from various clients remain above 93%. The code will be made publicly
available after the review period.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhyTracker: An Online Tracker for Phytoplankton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Qingxuan Lv, Yuezun Li, Zhiqiang Wei, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phytoplankton, a crucial component of aquatic ecosystems, requires efficient
monitoring to understand marine ecological processes and environmental
conditions. Traditional phytoplankton monitoring methods, relying on non-in
situ observations, are time-consuming and resource-intensive, limiting timely
analysis. To address these limitations, we introduce PhyTracker, an intelligent
in situ tracking framework designed for automatic tracking of phytoplankton.
PhyTracker overcomes significant challenges unique to phytoplankton monitoring,
such as constrained mobility within water flow, inconspicuous appearance, and
the presence of impurities. Our method incorporates three innovative modules: a
Texture-enhanced Feature Extraction (TFE) module, an Attention-enhanced
Temporal Association (ATA) module, and a Flow-agnostic Movement Refinement
(FMR) module. These modules enhance feature capture, differentiate between
phytoplankton and impurities, and refine movement characteristics,
respectively. Extensive experiments on the PMOT dataset validate the
superiority of PhyTracker in phytoplankton tracking, and additional tests on
the MOT dataset demonstrate its general applicability, outperforming
conventional tracking methods. This work highlights key differences between
phytoplankton and traditional objects, offering an effective solution for
phytoplankton monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13pages,eleven figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical
  Phase Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabel Funke, Dominik Rivoir, Stefanie Krell, Stefanie Speidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enable context-aware computer assistance in the operating room of the
future, cognitive systems need to understand automatically which surgical phase
is being performed by the medical team. The primary source of information for
surgical phase recognition is typically video, which presents two challenges:
extracting meaningful features from the video stream and effectively modeling
temporal information in the sequence of visual features. For temporal modeling,
attention mechanisms have gained popularity due to their ability to capture
long-range dependencies. In this paper, we explore design choices for attention
in existing temporal models for surgical phase recognition and propose a novel
approach that uses attention more effectively and does not require hand-crafted
constraints: TUNeS, an efficient and simple temporal model that incorporates
self-attention at the core of a convolutional U-Net structure. In addition, we
propose to train the feature extractor, a standard CNN, together with an LSTM
on preferably long video segments, i.e., with long temporal context. In our
experiments, almost all temporal models performed better on top of feature
extractors that were trained with longer temporal context. On these
contextualized features, TUNeS achieves state-of-the-art results on the
Cholec80 dataset. This study offers new insights on how to use attention
mechanisms to build accurate and efficient temporal models for surgical phase
recognition. Implementing automatic surgical phase recognition is essential to
automate the analysis and optimization of surgical workflows and to enable
context-aware computer assistance during surgery, thus ultimately improving
patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extreme Rotation Estimation in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hana Bezalel, Dotan Ankri, Ruojin Cai, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a technique and benchmark dataset for estimating the relative 3D
orientation between a pair of Internet images captured in an extreme setting,
where the images have limited or non-overlapping field of views. Prior work
targeting extreme rotation estimation assume constrained 3D environments and
emulate perspective images by cropping regions from panoramic views. However,
real images captured in the wild are highly diverse, exhibiting variation in
both appearance and camera intrinsics. In this work, we propose a
Transformer-based method for estimating relative rotations in extreme
real-world settings, and contribute the ExtremeLandmarkPairs dataset, assembled
from scene-level Internet photo collections. Our evaluation demonstrates that
our approach succeeds in estimating the relative rotations in a wide variety of
extreme-view Internet image pairs, outperforming various baselines, including
dedicated rotation estimation techniques and contemporary 3D reconstruction
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage:
  https://tau-vailab.github.io/ExtremeRotationsInTheWild/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Viti, Franz Thaler, Kathrin Lisa Kapper, Martin Urschler, Martin Holler, Elias Karabelas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of cardiac magnetic resonance images (MRI) is crucial for the
analysis and assessment of cardiac function, helping to diagnose and treat
various cardiovascular diseases. Most recent techniques rely on deep learning
and usually require an extensive amount of labeled data. To overcome this
problem, few-shot learning has the capability of reducing data dependency on
labeled data. In this work, we introduce a new method that merges few-shot
learning with a U-Net architecture and Gaussian Process Emulators (GPEs),
enhancing data integration from a support set for improved performance. GPEs
are trained to learn the relation between the support images and the
corresponding masks in latent space, facilitating the segmentation of unseen
query images given only a small labeled support set at inference. We test our
model with the M&Ms-2 public dataset to assess its ability to segment the heart
in cardiac magnetic resonance imaging from different orientations, and compare
it with state-of-the-art unsupervised and few-shot methods. Our architecture
shows higher DICE coefficients compared to these methods, especially in the
more challenging setups where the size of the support set is considerably
small.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Statistical Atlases and Computational Modeling of the
  Heart (STACOM) Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MEGA-Bench, an evaluation suite that scales multimodal evaluation
to over 500 real-world tasks, to address the highly heterogeneous daily use
cases of end users. Our objective is to optimize for a set of high-quality data
samples that cover a highly diverse and rich set of multimodal tasks, while
enabling cost-effective and accurate model evaluation. In particular, we
collected 505 realistic tasks encompassing over 8,000 samples from 16 expert
annotators to extensively cover the multimodal task space. Instead of unifying
these problems into standard multi-choice questions (like MMMU, MMBench, and
MMT-Bench), we embrace a wide range of output formats like numbers, phrases,
code, \LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats,
we developed over 40 metrics to evaluate these tasks. Unlike existing
benchmarks, MEGA-Bench offers a fine-grained capability report across multiple
dimensions (e.g., application, input type, output format, skill), allowing
users to interact with and visualize model capabilities in depth. We evaluate a
wide variety of frontier vision-language models on MEGA-Bench to understand
their capabilities across these dimensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Project page:
  https://tiger-ai-lab.github.io/MEGA-Bench/. v2 includes more evaluated models
  and a single-image setting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalar Function Topology Divergence: Comparing Topology of 3D Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Trofimov, Daria Voronkova, Eduard Tulchinskii, Evgeny Burnaev, Serguei Barannikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new topological tool for computer vision - Scalar Function
Topology Divergence (SFTD), which measures the dissimilarity of multi-scale
topology between sublevel sets of two functions having a common domain.
Functions can be defined on an undirected graph or Euclidean space of any
dimensionality. Most of the existing methods for comparing topology are based
on Wasserstein distance between persistence barcodes and they don't take into
account the localization of topological features. The minimization of SFTD
ensures that the corresponding topological features of scalar functions are
located in the same places. The proposed tool provides useful visualizations
depicting areas where functions have topological dissimilarities. We provide
applications of the proposed method to 3D computer vision. In particular,
experiments demonstrate that SFTD as an additional loss improves the
reconstruction of cellular 3D shapes from 2D fluorescence microscopy images,
and helps to identify topological errors in 3D segmentation. Additionally, we
show that SFTD outperforms Betti matching loss in 2D segmentation problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Adversarial Robustness of Vision Language Models: a
  Multimodal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19287v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19287v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Zhou, Shuanghao Bai, Danilo P. Mandic, Qibin Zhao, Badong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained vision-language models (VLMs) like CLIP exhibit exceptional
generalization across diverse downstream tasks. While recent studies reveal
their vulnerability to adversarial attacks, research to date has primarily
focused on enhancing the robustness of image encoders against image-based
attacks, with defenses against text-based and multimodal attacks remaining
largely unexplored. To this end, this work presents the first comprehensive
study on improving the adversarial robustness of VLMs against attacks targeting
image, text, and multimodal inputs. This is achieved by proposing multimodal
contrastive adversarial training (MMCoA). Such an approach strengthens the
robustness of both image and text encoders by aligning the clean text
embeddings with adversarial image embeddings, and adversarial text embeddings
with clean image embeddings. The robustness of the proposed MMCoA is examined
against existing defense methods over image, text, and multimodal attacks on
the CLIP model. Extensive experiments on 15 datasets across two tasks reveal
the characteristics of different adversarial defense methods under distinct
distribution shifts and dataset complexities across the three attack types.
This paves the way for a unified framework of adversarial robustness against
different modality attacks, opening up new possibilities for securing VLMs
against multimodal attacks. The code is available at
https://github.com/ElleZWQ/MMCoA.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Advanced Large Language Models with LLMsuite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Roffo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This tutorial explores the advancements and challenges in the development of
Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent
limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the
generation of incorrect information, proposing solutions like Retrieval
Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks
such as ReAct and LangChain. The integration of these techniques enhances LLM
performance and reliability, especially in multi-step reasoning and complex
task execution. The paper also covers fine-tuning strategies, including
instruction fine-tuning, parameter-efficient methods like LoRA, and
Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced
Self-Training (ReST). Additionally, it provides a comprehensive survey of
transformer architectures and training techniques for LLMs. The source code can
be accessed by contacting the author via email for a request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: Language Model Benchmarking, Pre-Trained LLM Comparison,
  LLM Performance Analysis, NLP Model Evaluation Tools, Public Dataset
  Inference for LLMs, BLEU and ROUGE Metrics for LLM, Open Source LLM Testing
  Tools, Large Language Model Evaluation Software, NLP Benchmarking Suite,
  Comprehensive LLM Evaluation Toolkit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmAgent: A Multi-modal Agent Framework for Complex Video Understanding
  with Task Divide-and-Conquer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, Kyusong Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have expanded their
capabilities to multimodal contexts, including comprehensive video
understanding. However, processing extensive videos such as 24-hour CCTV
footage or full-length films presents significant challenges due to the vast
data and processing demands. Traditional methods, like extracting key frames or
converting frames to text, often result in substantial information loss. To
address these shortcomings, we develop OmAgent, efficiently stores and
retrieves relevant video frames for specific queries, preserving the detailed
content of videos. Additionally, it features an Divide-and-Conquer Loop capable
of autonomous reasoning, dynamically invoking APIs and tools to enhance query
processing and accuracy. This approach ensures robust video understanding,
significantly reducing information loss. Experimental results affirm OmAgent's
efficacy in handling various types of videos and complex tasks. Moreover, we
have endowed it with greater autonomy and a robust tool-calling system,
enabling it to accomplish even more intricate tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory-Efficient Pseudo-Labeling for Online Source-Free Universal Domain
  Adaptation using a Gaussian Mixture Model <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Schlachter, Simon Wagner, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In practice, domain shifts are likely to occur between training and test
data, necessitating domain adaptation (DA) to adjust the pre-trained source
model to the target domain. Recently, universal domain adaptation (UniDA) has
gained attention for addressing the possibility of an additional category
(label) shift between the source and target domain. This means new classes can
appear in the target data, some source classes may no longer be present, or
both at the same time. For practical applicability, UniDA methods must handle
both source-free and online scenarios, enabling adaptation without access to
the source data and performing batch-wise updates in parallel with prediction.
In an online setting, preserving knowledge across batches is crucial. However,
existing methods often require substantial memory, which is impractical because
memory is limited and valuable, in particular on embedded systems. Therefore,
we consider memory-efficiency as an additional constraint. To achieve
memory-efficient online source-free universal domain adaptation (SF-UniDA), we
propose a novel method that continuously captures the distribution of known
classes in the feature space using a Gaussian mixture model (GMM). This
approach, combined with entropy-based out-of-distribution detection, allows for
the generation of reliable pseudo-labels. Finally, we combine a contrastive
loss with a KL divergence loss to perform the adaptation. Our approach not only
achieves state-of-the-art results in all experiments on the DomainNet and
Office-Home datasets but also significantly outperforms the existing methods on
the challenging VisDA-C dataset, setting a new benchmark for online SF-UniDA.
Our code is available at https://github.com/pascalschlachter/GMM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HYPNOS : Highly Precise Foreground-focused Diffusion Finetuning for
  Inanimate Objects <span class="chip">ACCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliverio Theophilus Nathanael, Jonathan Samuel Lumentut, Nicholas Hans Muliawan, Edbert Valencio Angky, Felix Indra Kurniadi, Alfi Yusrotis Zakiyyah, Jeklin Harefa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, personalized diffusion-based text-to-image generative tasks
have been a hot topic in computer vision studies. A robust diffusion model is
determined by its ability to perform near-perfect reconstruction of certain
product outcomes given few related input samples. Unfortunately, the current
prominent diffusion-based finetuning technique falls short in maintaining the
foreground object consistency while being constrained to produce diverse
backgrounds in the image outcome. In the worst scenario, the overfitting issue
may occur, meaning that the foreground object is less controllable due to the
condition above, for example, the input prompt information is transferred
ambiguously to both foreground and background regions, instead of the supposed
background region only. To tackle the issues above, we proposed Hypnos, a
highly precise foreground-focused diffusion finetuning technique. On the image
level, this strategy works best for inanimate object generation tasks, and to
do so, Hypnos implements two main approaches, namely: (i) a content-centric
prompting strategy and (ii) the utilization of our additional
foreground-focused discriminative module. The utilized module is connected with
the diffusion model and finetuned with our proposed set of supervision
mechanism. Combining the strategies above yielded to the foreground-background
disentanglement capability of the diffusion model. Our experimental results
showed that the proposed strategy gave a more robust performance and visually
pleasing results compared to the former technique. For better elaborations, we
also provided extensive studies to assess the fruitful outcomes above, which
reveal how personalization behaves in regard to several training conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 12 figures, to appear on the Rich Media with Generative AI
  workshop in conjunction with Asian Conference on Computer Vision (ACCV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEO: Generative Latent Image Animator for Human Video Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaohui Wang, Xin Ma, Xinyuan Chen, Cunjian Chen, Antitza Dantcheva, Bo Dai, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal coherency is a major challenge in synthesizing high quality
videos, particularly in synthesizing human videos that contain rich global and
local deformations. To resolve this challenge, previous approaches have
resorted to different features in the generation process aimed at representing
appearance and motion. However, in the absence of strict mechanisms to
guarantee such disentanglement, a separation of motion from appearance has
remained challenging, resulting in spatial distortions and temporal jittering
that break the spatio-temporal coherency. Motivated by this, we here propose
LEO, a novel framework for human video synthesis, placing emphasis on
spatio-temporal coherency. Our key idea is to represent motion as a sequence of
flow maps in the generation process, which inherently isolate motion from
appearance. We implement this idea via a flow-based image animator and a Latent
Motion Diffusion Model (LMDM). The former bridges a space of motion codes with
the space of flow maps, and synthesizes video frames in a warp-and-inpaint
manner. LMDM learns to capture motion prior in the training data by
synthesizing sequences of motion codes. Extensive quantitative and qualitative
analysis suggests that LEO significantly improves coherent synthesis of human
videos over previous methods on the datasets TaichiHD, FaceForensics and
CelebV-HQ. In addition, the effective disentanglement of appearance and motion
in LEO allows for two additional tasks, namely infinite-length human video
synthesis, as well as content-preserving video editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCV 2024, Project webpage: https://wyhsirius.github.io/LEO-project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupling Fine Detail and Global Geometry for Compressed Depth Map
  Super-Resolution <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Zheng, Wencheng Han, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering high-quality depth maps from compressed sources has gained
significant attention due to the limitations of consumer-grade depth cameras
and the bandwidth restrictions during data transmission. However, current
methods still suffer from two challenges. First, bit-depth compression produces
a uniform depth representation in regions with subtle variations, hindering the
recovery of detailed information. Second, densely distributed random noise
reduces the accuracy of estimating the global geometric structure of the scene.
To address these challenges, we propose a novel framework, termed
geometry-decoupled network (GDNet), for compressed depth map super-resolution
that decouples the high-quality depth map reconstruction process by handling
global and detailed geometric features separately. To be specific, we propose
the fine geometry detail encoder (FGDE), which is designed to aggregate fine
geometry details in high-resolution low-level image features while
simultaneously enriching them with complementary information from
low-resolution context-level image features. In addition, we develop the global
geometry encoder (GGE) that aims at suppressing noise and extracting global
geometric information effectively via constructing compact feature
representation in a low-rank space. We conduct experiments on multiple
benchmark datasets, demonstrating that our GDNet significantly outperforms
current methods in terms of geometric consistency and detail recovery. In the
ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st
place award. Our codes will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 1st place award for the ECCV 2024 AIM Compressed Depth Upsampling
  Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-throughput 3D shape completion of potato tubers on a harvester 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21341v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21341v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pieter M. Blok, Federico Magistri, Cyrill Stachniss, Haozhou Wang, James Burridge, Wei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Potato yield is an important metric for farmers to further optimize their
cultivation practices. Potato yield can be estimated on a harvester using an
RGB-D camera that can estimate the three-dimensional (3D) volume of individual
potato tubers. A challenge, however, is that the 3D shape derived from RGB-D
images is only partially completed, underestimating the actual volume. To
address this issue, we developed a 3D shape completion network, called CoRe++,
which can complete the 3D shape from RGB-D images. CoRe++ is a deep learning
network that consists of a convolutional encoder and a decoder. The encoder
compresses RGB-D images into latent vectors that are used by the decoder to
complete the 3D shape using the deep signed distance field network (DeepSDF).
To evaluate our CoRe++ network, we collected partial and complete 3D point
clouds of 339 potato tubers on an operational harvester in Japan. On the 1425
RGB-D images in the test set (representing 51 unique potato tubers), our
network achieved a completion accuracy of 2.8 mm on average. For volumetric
estimation, the root mean squared error (RMSE) was 22.6 ml, and this was better
than the RMSE of the linear regression (31.1 ml) and the base model (36.9 ml).
We found that the RMSE can be further reduced to 18.2 ml when performing the 3D
shape completion in the center of the RGB-D image. With an average 3D shape
completion time of 10 milliseconds per tuber, we can conclude that CoRe++ is
both fast and accurate enough to be implemented on an operational harvester for
high-throughput potato yield estimation. CoRe++'s high-throughput and accurate
processing allows it to be applied to other tuber, fruit and vegetable crops,
thereby enabling versatile, accurate and real-time yield monitoring in
precision agriculture. Our code, network weights and dataset are publicly
available at https://github.com/UTokyo-FieldPhenomics-Lab/corepp.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransAgent: Transfer Vision-Language Foundation Models with
  Heterogeneous Agent Collaboration <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Guo, Shaobin Zhuang, Kunchang Li, Yu Qiao, Yali Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language foundation models (such as CLIP) have recently shown their
power in transfer learning, owing to large-scale image-text pre-training.
However, target domain data in the downstream tasks can be highly different
from the pre-training phase, which makes it hard for such a single model to
generalize well. Alternatively, there exists a wide range of expert models that
contain diversified vision and/or language knowledge pre-trained on different
modalities, tasks, networks, and datasets. Unfortunately, these models are
"isolated agents" with heterogeneous structures, and how to integrate their
knowledge for generalizing CLIP-like models has not been fully explored. To
bridge this gap, we propose a general and concise TransAgent framework, which
transports the knowledge of the isolated agents in a unified manner, and
effectively guides CLIP to generalize with multi-source knowledge distillation.
With such a distinct framework, we flexibly collaborate with 11 heterogeneous
agents to empower vision-language foundation models, without further cost in
the inference phase. Finally, our TransAgent achieves state-of-the-art
performance on 11 visual recognition datasets. Under the same low-shot setting,
it outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT
which contains large domain shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does the Textual Information Affect the Retrieval of Multimodal
  In-Context Learning? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Luo, Zangwei Zheng, Zirui Zhu, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increase in parameter size of multimodal large language models (MLLMs)
introduces significant capabilities, particularly in-context learning, where
MLLMs enhance task performance without updating pre-trained parameters. This
effectiveness, however, hinges on the appropriate selection of in-context
examples, a process that is currently biased towards visual data, overlooking
textual information. Furthermore, the area of supervised retrievers for MLLMs,
crucial for optimal in-context example selection, continues to be
uninvestigated. Our study offers an in-depth evaluation of the impact of
textual information on the unsupervised selection of in-context examples in
multimodal contexts, uncovering a notable sensitivity of retriever performance
to the employed modalities. Responding to this, we introduce a novel supervised
MLLM-retriever MSIER that employs a neural network to select examples that
enhance multimodal in-context learning efficiency. This approach is validated
through extensive testing across three distinct tasks, demonstrating the
method's effectiveness. Additionally, we investigate the influence of
modalities on our supervised retrieval method's training and pinpoint factors
contributing to our model's success. This exploration paves the way for future
advancements, highlighting the potential for refined in-context learning in
MLLMs through the strategic use of multimodal data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot NAS via the Suppression of Local Entropy Decrease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Wu, Han Huang, Yueting Xu, Zhifeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architecture performance evaluation is the most time-consuming part of neural
architecture search (NAS). Zero-Shot NAS accelerates the evaluation by
utilizing zero-cost proxies instead of training. Though effective, existing
zero-cost proxies require invoking backpropagations or running networks on
input data, making it difficult to further accelerate the computation of
proxies. To alleviate this issue, architecture topologies are used to evaluate
the performance of networks in this study. We prove that particular
architectural topologies decrease the local entropy of feature maps, which
degrades specific features to a bias, thereby reducing network performance.
Based on this proof, architectural topologies are utilized to quantify the
suppression of local entropy decrease (SED) as a data-free and running-free
proxy. Experimental results show that SED outperforms most state-of-the-art
proxies in terms of architecture selection on five benchmarks, with computation
time reduced by three orders of magnitude. We further compare the SED-based NAS
with state-of-the-art proxies. SED-based NAS selects the architecture with
higher accuracy and fewer parameters in only one second. The theoretical
analyses of local entropy and experimental results demonstrate that the
suppression of local entropy decrease facilitates selecting optimal
architectures in Zero-Shot NAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures. Corrected typos and latex template</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CALoR: Towards Comprehensive Model Inversion Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyao Yu, Yixiang Qiu, Hao Fang, Bin Chen, Sijin Yu, Bin Wang, Shu-Tao Xia, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Inversion Attacks (MIAs) aim at recovering privacy-sensitive training
data from the knowledge encoded in the released machine learning models. Recent
advances in the MIA field have significantly enhanced the attack performance
under multiple scenarios, posing serious privacy risks of Deep Neural Networks
(DNNs). However, the development of defense strategies against MIAs is
relatively backward to resist the latest MIAs and existing defenses fail to
achieve further trade-off between model utility and model robustness. In this
paper, we provide an in-depth analysis from the perspective of intrinsic
vulnerabilities of MIAs, comprehensively uncovering the weaknesses inherent in
the basic pipeline, which are partially investigated in the previous defenses.
Building upon these new insights, we propose a robust defense mechanism,
integrating Confidence Adaptation and Low-Rank compression(CALoR). Our method
includes a novel robustness-enhanced classification loss specially-designed for
model inversion defenses and reveals the extraordinary effectiveness of
compressing the classification header. With CALoR, we can mislead the
optimization objective, reduce the leaked information and impede the
backpropagation of MIAs, thus mitigating the risk of privacy leakage. Extensive
experimental results demonstrate that our method achieves state-of-the-art
(SOTA) defense performance against MIAs and exhibits superior generalization to
existing defenses across various scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>-Based Tooth Alignment Prediction With Occlusion And
  Collision Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20806v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20806v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ZhenXing Dong, JiaZhou Chen, YangHui Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The planning of digital orthodontic treatment requires providing tooth
alignment, which not only consumes a lot of time and labor to determine
manually but also relays clinical experiences heavily. In this work, we
proposed a lightweight tooth alignment neural network based on
Swin-transformer. We first re-organized 3D point clouds based on virtual arch
lines and converted them into order-sorted multi-channel textures, which
improves the accuracy and efficiency simultaneously. We then designed two new
occlusal loss functions that quantitatively evaluate the occlusal relationship
between the upper and lower jaws. They are important clinical constraints,
first introduced to the best of our knowledge, and lead to cutting-edge
prediction accuracy. To train our network, we collected a large digital
orthodontic dataset that has 591 clinical cases, including various complex
clinical cases. This dataset will benefit the community after its release since
there is no open dataset so far. Furthermore, we also proposed two new
orthodontic dataset augmentation methods considering tooth spatial distribution
and occlusion. We evaluated our method with this dataset and extensive
experiments, including comparisons with STAT methods and ablation studies, and
demonstrate the high prediction accuracy of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Modify formatting errors, optimize content layout</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Open-Domain Continual Learning via Leveraging Intra-domain
  Category-aware Prototype 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadong Lu, Shitian Zhao, Boxiang Yun, Dongsheng Jiang, Yin Li, Qingli Li, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in enhancing the efficacy of Open-Domain Continual
Learning (ODCL) in Vision-Language Models (VLM), failing to (1) correctly
identify the Task-ID of a test image and (2) use only the category set
corresponding to the Task-ID, while preserving the knowledge related to each
domain, cannot address the two primary challenges of ODCL: forgetting old
knowledge and maintaining zero-shot capabilities, as well as the confusions
caused by category-relatedness between domains. In this paper, we propose a
simple yet effective solution: leveraging intra-domain category-aware
prototypes for ODCL in CLIP (DPeCLIP), where the prototype is the key to
bridging the above two processes. Concretely, we propose a training-free
Task-ID discriminator method, by utilizing prototypes as classifiers for
identifying Task-IDs. Furthermore, to maintain the knowledge corresponding to
each domain, we incorporate intra-domain category-aware prototypes as domain
prior prompts into the training process. Extensive experiments conducted on 11
different datasets demonstrate the effectiveness of our approach, achieving
2.37% and 1.14% average improvement in class-incremental and task-incremental
settings, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCSA: Exploring the Synergistic Effects Between Spatial and Channel
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhong Si, Huiying Xu, Xinzhong Zhu, Wenhao Zhang, Yao Dong, Yuxing Chen, Hongbo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Channel and spatial attentions have respectively brought significant
improvements in extracting feature dependencies and spatial structure relations
for various downstream vision tasks. While their combination is more beneficial
for leveraging their individual strengths, the synergy between channel and
spatial attentions has not been fully explored, lacking in fully harness the
synergistic potential of multi-semantic information for feature guidance and
mitigation of semantic disparities. Our study attempts to reveal the
synergistic relationship between spatial and channel attention at multiple
semantic levels, proposing a novel Spatial and Channel Synergistic Attention
module (SCSA). Our SCSA consists of two parts: the Shareable Multi-Semantic
Spatial Attention (SMSA) and the Progressive Channel-wise Self-Attention
(PCSA). SMSA integrates multi-semantic information and utilizes a progressive
compression strategy to inject discriminative spatial priors into PCSA's
channel self-attention, effectively guiding channel recalibration.
Additionally, the robust feature interactions based on the self-attention
mechanism in PCSA further mitigate the disparities in multi-semantic
information among different sub-features within SMSA. We conduct extensive
experiments on seven benchmark datasets, including classification on
ImageNet-1K, object detection on MSCOCO 2017, segmentation on ADE20K, and four
other complex scene detection datasets. Our results demonstrate that our
proposed SCSA not only surpasses the current state-of-the-art attention but
also exhibits enhanced generalization capabilities across various task
scenarios. The code and models are available at:
https://github.com/HZAI-ZJNU/SCSA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We added experiments for the classification task and updated the
  corresponding sections accordingly. The paper formatting has also been
  revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Training-free Conditional Diffusion Model via Fisher
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Song, Hanjiang Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training-free conditional diffusion models have received great attention in
conditional image generation tasks. However, they require a computationally
expensive conditional score estimator to let the intermediate results of each
step in the reverse process toward the condition, which causes slow conditional
generation. In this paper, we propose a novel Fisher information-based
conditional diffusion (FICD) model to generate high-quality samples according
to the condition. In particular, we further explore the conditional term from
the perspective of Fisher information, where we show Fisher information can act
as a weight to measure the informativeness of the condition in each generation
step. According to this new perspective, we can control and gain more
information along the conditional direction in the generation space. Thus, we
propose the upper bound of the Fisher information to reformulate the
conditional term, which increases the information gain and decreases the time
cost. Experimental results also demonstrate that the proposed FICD can offer up
to 2x speed-ups under the same sampling steps as most baselines. Meanwhile,
FICD can improve the generation quality in various tasks compared to the
baselines with a low computation cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CIMIL-CRC: a clinically-informed multiple instance learning framework
  for patient-level colorectal cancer molecular subtypes classification from
  H\&E stained images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadar Hezi, Matan Gelber, Alexander Balabanov, Yosef E. Maruvka, Moti Freiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Treatment approaches for colorectal cancer (CRC) are highly dependent on the
molecular subtype, as immunotherapy has shown efficacy in cases with
microsatellite instability (MSI) but is ineffective for the microsatellite
stable (MSS) subtype. There is promising potential in utilizing deep neural
networks (DNNs) to automate the differentiation of CRC subtypes by analyzing
Hematoxylin and Eosin (H\&E) stained whole-slide images (WSIs). Due to the
extensive size of WSIs, Multiple Instance Learning (MIL) techniques are
typically explored. However, existing MIL methods focus on identifying the most
representative image patches for classification, which may result in the loss
of critical information. Additionally, these methods often overlook clinically
relevant information, like the tendency for MSI class tumors to predominantly
occur on the proximal (right side) colon. We introduce `CIMIL-CRC', a DNN
framework that: 1) solves the MSI/MSS MIL problem by efficiently combining a
pre-trained feature extraction model with principal component analysis (PCA) to
aggregate information from all patches, and 2) integrates clinical priors,
particularly the tumor location within the colon, into the model to enhance
patient-level classification accuracy. We assessed our CIMIL-CRC method using
the average area under the curve (AUC) from a 5-fold cross-validation
experimental setup for model development on the TCGA-CRC-DX cohort, contrasting
it with a baseline patch-level classification, MIL-only approach, and
Clinically-informed patch-level classification approach. Our CIMIL-CRC
outperformed all methods (AUROC: $0.92\pm0.002$ (95\% CI 0.91-0.92), vs.
$0.79\pm0.02$ (95\% CI 0.76-0.82), $0.86\pm0.01$ (95\% CI 0.85-0.88), and
$0.87\pm0.01$ (95\% CI 0.86-0.88), respectively). The improvement was
statistically significant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the journal 'Computer Methods and Programs in
  Biomedicine'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging <span class="highlight-title">Pre-train</span>ed Models for FF-to-FFPE Histopathological Image
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilai Zhang, Jiawen Li, Peiran Liao, Jiali Hu, Tian Guan, Anjia Han, Yonghong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The two primary types of Hematoxylin and Eosin (H&E) slides in histopathology
are Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF). FFPE slides
offer high quality histopathological images but require a labor-intensive
acquisition process. In contrast, FF slides can be prepared quickly, but the
image quality is relatively poor. Our task is to translate FF images into FFPE
style, thereby improving the image quality for diagnostic purposes. In this
paper, we propose Diffusion-FFPE, a method for FF-to-FFPE histopathological
image translation using a pre-trained diffusion model. Specifically, we employ
a one-step diffusion model as the generator and fine-tune it with LoRA adapters
using adversarial learning objectives. To ensure that the model effectively
captures both global structural information and local details, we propose a
multi-scale feature fusion (MFF) module. This module utilizes two VAE encoders
to extract features of varying image sizes and performs feature fusion before
feeding them into the UNet. Furthermore, we utilize a pre-trained
vision-language model for histopathology as the backbone for the discriminator
to further improve performance We conducted FF-to-FFPE translation experiments
on the TCGA-NSCLC datasets, and our method achieved better performance compared
to other methods. The code and models are released at
https://github.com/QilaiZhang/Diffusion-FFPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Add-it: Training-Free Object Insertion in Images With <span class="highlight-title">Pretrain</span>ed
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adding Object into images based on text instructions is a challenging task in
semantic image editing, requiring a balance between preserving the original
scene and seamlessly integrating the new object in a fitting location. Despite
extensive efforts, existing models often struggle with this balance,
particularly with finding a natural location for adding an object in complex
scenes. We introduce Add-it, a training-free approach that extends diffusion
models' attention mechanisms to incorporate information from three key sources:
the scene image, the text prompt, and the generated image itself. Our weighted
extended-attention mechanism maintains structural consistency and fine details
while ensuring natural object placement. Without task-specific fine-tuning,
Add-it achieves state-of-the-art results on both real and generated image
insertion benchmarks, including our newly constructed "Additing Affordance
Benchmark" for evaluating object placement plausibility, outperforming
supervised methods. Human evaluations show that Add-it is preferred in over 80%
of cases, and it also demonstrates improvements in various automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is at https://research.nvidia.com/labs/par/addit/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Style Transfer: From Stitching to Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00606v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00606v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhe Xu, Zhuoer Wang, Yihan Zhang, Yizhou Liu, Zhaoyue Wang, Zhihao Xu, Muhan Zhao, Huaiying Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article compares two style transfer methods in image processing: the
traditional method, which synthesizes new images by stitching together small
patches from existing images, and a modern machine learning-based approach that
uses a segmentation network to isolate foreground objects and apply style
transfer solely to the background. The traditional method excels in creating
artistic abstractions but can struggle with seamlessness, whereas the machine
learning method preserves the integrity of foreground elements while enhancing
the background, offering improved aesthetic quality and computational
efficiency. Our study indicates that machine learning-based methods are more
suited for real-world applications where detail preservation in foreground
elements is essential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Diverse Methods in Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Qikai Yang, Xieming Geng, Wenjing Zhou, Zhicheng Ding, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores innovative methods for improving Visual Question
Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and
attention mechanisms. Leveraging a balanced VQA dataset, we investigate three
distinct strategies. Firstly, GAN-based approaches aim to generate answer
embeddings conditioned on image and question inputs, showing potential but
struggling with more complex tasks. Secondly, autoencoder-based techniques
focus on learning optimal embeddings for questions and images, achieving
comparable results with GAN due to better ability on complex questions. Lastly,
attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB),
address language priors and attention modeling, albeit with a
complexity-performance trade-off. This study underscores the challenges and
opportunities in VQA and suggests avenues for future research, including
alternative GAN formulations and attentional mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Electronic
  Communication and Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SplatFormer: Point <span class="highlight-title">Transformer</span> for Robust 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin, Siyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently transformed photorealistic
reconstruction, achieving high visual fidelity and real-time performance.
However, rendering quality significantly deteriorates when test views deviate
from the camera angles used during training, posing a major challenge for
applications in immersive free-viewpoint rendering and navigation. In this
work, we conduct a comprehensive evaluation of 3DGS and related novel view
synthesis methods under out-of-distribution (OOD) test camera scenarios. By
creating diverse test cases with synthetic and real-world datasets, we
demonstrate that most existing methods, including those incorporating various
regularization techniques and data-driven priors, struggle to generalize
effectively to OOD views. To address this limitation, we introduce SplatFormer,
the first point transformer model specifically designed to operate on Gaussian
splats. SplatFormer takes as input an initial 3DGS set optimized under limited
training views and refines it in a single forward pass, effectively removing
potential artifacts in OOD test views. To our knowledge, this is the first
successful application of point transformers directly on 3DGS sets, surpassing
the limitations of previous multi-scene training methods, which could handle
only a restricted number of input views during inference. Our model
significantly improves rendering quality under extreme novel views, achieving
state-of-the-art performance in these challenging scenarios and outperforming
various 3DGS regularization techniques, multi-scene models tailored for sparse
view synthesis, and diffusion-based frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and dataset: https://github.com/ChenYutongTHU/SplatFormer
  Project page: https://sergeyprokudin.github.io/splatformer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhance Image-to-Image Generation with LLaVA-generated <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01956v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01956v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Ding, Panfeng Li, Qikai Yang, Siyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to enhance image-to-image generation by
leveraging the multimodal capabilities of the Large Language and Vision
Assistant (LLaVA). We propose a framework where LLaVA analyzes input images and
generates textual descriptions, hereinafter LLaVA-generated prompts. These
prompts, along with the original image, are fed into the image-to-image
generation pipeline. This enriched representation guides the generation process
towards outputs that exhibit a stronger resemblance to the input image.
Extensive experiments demonstrate the effectiveness of LLaVA-generated prompts
in promoting image similarity. We observe a significant improvement in the
visual coherence between the generated and input images compared to traditional
methods. Future work will explore fine-tuning LLaVA prompts for increased
control over the creative process. By providing more specific details within
the prompts, we aim to achieve a delicate balance between faithfulness to the
original image and artistic expression in the generated outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Information Science,
  Parallel and Distributed Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal-Mapping Photography for Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Bao, Lei Sun, Yuqin Ma, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras, or Dynamic Vision Sensors (DVS) are novel neuromorphic sensors
that capture brightness changes as a continuous stream of "events" rather than
traditional intensity frames. Converting sparse events to dense intensity
frames faithfully has long been an ill-posed problem. Previous methods have
primarily focused on converting events to video in dynamic scenes or with a
moving camera. In this paper, for the first time, we realize events to dense
intensity image conversion using a stationary event camera in static scenes
with a transmittance adjustment device for brightness modulation. Different
from traditional methods that mainly rely on event integration, the proposed
Event-Based Temporal Mapping Photography (EvTemMap) measures the time of event
emitting for each pixel. Then, the resulting Temporal Matrix is converted to an
intensity frame with a temporal mapping neural network. At the hardware level,
the proposed EvTemMap is implemented by combining a transmittance adjustment
device with a DVS, named Adjustable Transmittance Dynamic Vision Sensor
(AT-DVS). Additionally, we collected TemMat dataset under various conditions
including low-light and high dynamic range scenes. The experimental results
showcase the high dynamic range, fine-grained details, and high-grayscale
resolution of the proposed EvTemMap. The code and dataset are available in
https://github.com/YuHanBaozju/EvTemMap
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures, 1 Supplementary materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenRec: Unifying Video Generation and Recognition with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zejia Weng, Xitong Yang, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video diffusion models are able to generate high-quality videos by learning
strong spatial-temporal priors on large-scale datasets. In this paper, we aim
to investigate whether such priors derived from a generative process are
suitable for video recognition, and eventually joint optimization of generation
and recognition. Building upon Stable Video Diffusion, we introduce GenRec, the
first unified framework trained with a random-frame conditioning process so as
to learn generalized spatial-temporal representations. The resulting framework
can naturally supports generation and recognition, and more importantly is
robust even when visual inputs contain limited information. Extensive
experiments demonstrate the efficacy of GenRec for both recognition and
generation. In particular, GenRec achieves competitive recognition performance,
offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also
performs the best on class-conditioned image-to-video generation, achieving
46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore, GenRec
demonstrates extraordinary robustness in scenarios that only limited frames can
be observed. Code will be available at https://github.com/wengzejia1/GenRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reminding Multimodal Large Language Models of Object-aware Knowledge
  with Retrieved Tags <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiqing Qi, Handong Zhao, Zijun Wei, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in the general visual instruction-following ability
of Multimodal Large Language Models (MLLMs), they still struggle with critical
problems when required to provide a precise and detailed response to a visual
instruction: (1) failure to identify novel objects or entities, (2) mention of
non-existent objects, and (3) neglect of object's attributed details. Intuitive
solutions include improving the size and quality of data or using larger
foundation models. They show effectiveness in mitigating these issues, but at
an expensive cost of collecting a vast amount of new data and introducing a
significantly larger model. Standing at the intersection of these approaches,
we examine the three object-oriented problems from the perspective of the
image-to-text mapping process by the multimodal connector. In this paper, we
first identify the limitations of multimodal connectors stemming from
insufficient training data. Driven by this, we propose to enhance the mapping
with retrieval-augmented tag tokens, which contain rich object-aware
information such as object names and attributes. With our Tag-grounded visual
instruction tuning with retrieval Augmentation (TUNA), we outperform baselines
that share the same language model and training data on 12 benchmarks.
Furthermore, we show the zero-shot capability of TUNA when provided with
specific datastores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main Conference at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-in-the-Loop Segmentation of Multi-species Coral Imagery <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scarlett Raine, Ross Marchant, Brano Kusy, Frederic Maire, Niko Suenderhauf, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Marine surveys by robotic underwater and surface vehicles result in
substantial quantities of coral reef imagery, however labeling these images is
expensive and time-consuming for domain experts. Point label propagation is a
technique that uses existing images labeled with sparse points to create
augmented ground truth data, which can be used to train a semantic segmentation
model. In this work, we show that recent advances in large foundation models
facilitate the creation of augmented ground truth masks using only features
extracted by the denoised version of the DINOv2 foundation model and K-Nearest
Neighbors (KNN), without any pre-training. For images with extremely sparse
labels, we present a labeling method based on human-in-the-loop principles,
which greatly enhances annotation efficiency: in the case that there are 5
point labels per image, our human-in-the-loop method outperforms the prior
state-of-the-art by 14.2% for pixel accuracy and 19.7% for mIoU; and by 8.9%
and 18.3% if there are 10 point labels. When human-in-the-loop labeling is not
available, using the denoised DINOv2 features with a KNN still improves on the
prior state-of-the-art by 2.7% for pixel accuracy and 5.8% for mIoU (5 grid
points). On the semantic segmentation task, we outperform the prior
state-of-the-art by 8.8% for pixel accuracy and by 13.5% for mIoU when only 5
point labels are used for point label propagation. Additionally, we perform a
comprehensive study into the impacts of the point label placement style and the
number of points on the point label propagation quality, and make several
recommendations for improving the efficiency of labeling images with points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal article preprint of extended paper, 30 pages, 11 figures.
  Original conference paper (v2) accepted at the CVPR2024 3rd Workshop on
  Learning with Limited Labelled Data for Image and Video Understanding
  (L3D-IVU)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMLongBench-Doc: Benchmarking Long-context Document Understanding with
  Visualizations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding documents with rich layouts and multi-modal components is a
long-standing and practical task. Recent Large Vision-Language Models (LVLMs)
have made remarkable strides in various tasks, particularly in single-page
document understanding (DU). However, their abilities on long-context DU remain
an open problem. This work presents MMLongBench-Doc, a long-context,
multi-modal benchmark comprising 1,062 expert-annotated questions. Distinct
from previous datasets, it is constructed upon 130 lengthy PDF-formatted
documents with an average of 49.4 pages and 20,971 textual tokens. Towards
comprehensive evaluation, answers to these questions rely on pieces of evidence
from (1) different sources (text, image, chart, table, and layout structure)
and (2) various locations (i.e. page number). Moreover, 33.2% of the questions
are cross-page questions requiring evidence across multiple pages. 22.8% of the
questions are designed to be unanswerable for detecting potential
hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU
greatly challenges current models. Notably, the best-performing model, GPT-4o,
achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores
31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse
performance than their LLM counterparts which are fed with lossy-parsed OCR
documents. These results validate the necessity of future research toward more
capable long-context LVLMs. Project Page:
https://mayubo2333.github.io/MMLongBench-Doc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for
  Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing robotic manipulation tasks as constraints that associate the
robot and the environment is a promising way to encode desired robot behaviors.
However, it remains unclear how to formulate the constraints such that they are
1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable
by off-the-shelf solvers to produce robot actions in real-time. In this work,
we introduce Relational Keypoint Constraints (ReKep), a visually-grounded
representation for constraints in robotic manipulation. Specifically, ReKep is
expressed as Python functions mapping a set of 3D keypoints in the environment
to a numerical cost. We demonstrate that by representing a manipulation task as
a sequence of Relational Keypoint Constraints, we can employ a hierarchical
optimization procedure to solve for robot actions (represented by a sequence of
end-effector poses in SE(3)) with a perception-action loop at a real-time
frequency. Furthermore, in order to circumvent the need for manual
specification of ReKep for each new task, we devise an automated procedure that
leverages large vision models and vision-language models to produce ReKep from
free-form language instructions and RGB-D observations. We present system
implementations on a wheeled single-arm platform and a stationary dual-arm
platform that can perform a large variety of manipulation tasks, featuring
multi-stage, in-the-wild, bimanual, and reactive behaviors, all without
task-specific data or environment models. Website at
https://rekep-robot.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRAGE: Multimodal Identification and Recognition of Annotations in
  Indian General Prescriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tavish Mankash, V. S. Chaithanya Kota, Anish De, Praveen Prakash, Kshitij Jadhav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hospitals in India still rely on handwritten medical records despite the
availability of Electronic Medical Records (EMR), complicating statistical
analysis and record retrieval. Handwritten records pose a unique challenge,
requiring specialized data for training models to recognize medications and
their recommendation patterns. While traditional handwriting recognition
approaches employ 2-D LSTMs, recent studies have explored using Multimodal
Large Language Models (MLLMs) for OCR tasks. Building on this approach, we
focus on extracting medication names and dosages from simulated medical
records. Our methodology MIRAGE (Multimodal Identification and Recognition of
Annotations in indian GEneral prescriptions) involves fine-tuning the QWEN VL,
LLaVA 1.6 and Idefics2 models on 743,118 high resolution simulated medical
record images-fully annotated from 1,133 doctors across India. Our approach
achieves 82% accuracy in extracting medication names and dosages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures, 3 tables, submitted to ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in text-to-3D creation has been propelled by integrating the
potent prior of Diffusion Models from text-to-image generation into the 3D
domain. Nevertheless, generating 3D scenes characterized by multiple instances
and intricate arrangements remains challenging. In this study, we present
DreamScape, a method for creating highly consistent 3D scenes solely from
textual descriptions, leveraging the strong 3D representation capabilities of
Gaussian Splatting and the complex arrangement abilities of large language
models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene
representation, consisting of semantic primitives (objects) and their spatial
transformations and relationships derived directly from text prompts using
LLMs. This compositional representation allows for local-to-global optimization
of the entire scene. A progressive scale control is tailored during local
object generation, ensuring that objects of different sizes and densities adapt
to the scene, which addresses training instability issue arising from simple
blending in the subsequent global optimization stage. To mitigate potential
biases of LLM priors, we model collision relationships between objects at the
global level, enhancing physical correctness and overall realism. Additionally,
to generate pervasive objects like rain and snow distributed extensively across
the scene, we introduce a sparse initialization and densification strategy.
Experiments demonstrate that DreamScape offers high usability and
controllability, enabling the generation of high-fidelity 3D scenes from only
text prompts and achieving state-of-the-art performance compared to other
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated
  Image Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17465v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17465v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Diffusion Models has dramatically improved image generation
quality, making it increasingly difficult to differentiate between real and
generated images. This development, while impressive, also raises significant
privacy and security concerns. In response to this, we propose a novel Latent
REconstruction error guided feature REfinement method (LaRE^2) for detecting
the diffusion-generated images. We come up with the Latent Reconstruction Error
(LaRE), the first reconstruction-error based feature in the latent space for
generated image detection. LaRE surpasses existing methods in terms of feature
extraction efficiency while preserving crucial cues required to differentiate
between the real and the fake. To exploit LaRE, we propose an Error-Guided
feature REfinement module (EGRE), which can refine the image feature guided by
LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an
align-then-refine mechanism, which effectively refines the image feature for
generated-image detection from both spatial and channel perspectives. Extensive
experiments on the large-scale GenImage benchmark demonstrate the superiority
of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%
average ACC/AP across 8 different image generators. LaRE also surpasses
existing methods in terms of feature extraction cost, delivering an impressive
speed enhancement of 8 times. Code is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code is available at https://github.com/luo3300612/LaRE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WildScenes: A Benchmark for 2D and 3D Semantic Segmentation in
  Large-scale Natural Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavisha Vidanapathirana, Joshua Knights, Stephen Hausler, Mark Cox, Milad Ramezani, Jason Jooste, Ethan Griffiths, Shaheer Mohamed, Sridha Sridharan, Clinton Fookes, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in semantic scene understanding has primarily been enabled by
the availability of semantically annotated bi-modal (camera and LiDAR) datasets
in urban environments. However, such annotated datasets are also needed for
natural, unstructured environments to enable semantic perception for
applications, including conservation, search and rescue, environment
monitoring, and agricultural automation. Therefore, we introduce $WildScenes$,
a bi-modal benchmark dataset consisting of multiple large-scale, sequential
traversals in natural environments, including semantic annotations in
high-resolution 2D images and dense 3D LiDAR point clouds, and accurate 6-DoF
pose information. The data is (1) trajectory-centric with accurate localization
and globally aligned point clouds, (2) calibrated and synchronized to support
bi-modal training and inference, and (3) containing different natural
environments over 6 months to support research on domain adaptation. Our 3D
semantic labels are obtained via an efficient, automated process that transfers
the human-annotated 2D labels from multiple views into 3D point cloud
sequences, thus circumventing the need for expensive and time-consuming human
annotation in 3D. We introduce benchmarks on 2D and 3D semantic segmentation
and evaluate a variety of recent deep-learning techniques to demonstrate the
challenges in semantic segmentation in natural environments. We propose
train-val-test splits for standard benchmarks as well as domain adaptation
benchmarks and utilize an automated split generation technique to ensure the
balance of class label distributions. The $WildScenes$ benchmark webpage is
https://csiro-robotics.github.io/WildScenes, and the data is publicly available
at https://data.csiro.au/collection/csiro:61541 .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the The International Journal of Robotics Research (IJRR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Graph Generation for Enhanced Domain Adaptive Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of Domain Adaptive in the field of Object Detection involves the
transfer of object detection models from labeled source domains to unannotated
target domains. Recent advancements in this field aim to address domain
discrepancies by aligning pixel-pairs across domains within a non-Euclidean
graphical space, thereby minimizing semantic distribution variance. Despite
their remarkable achievements, these methods often use coarse semantic
representations to model graphs, mainly due to ignoring non-informative
elements and failing to focus on precise semantic alignment. Additionally, the
generation of coarse graphs inherently introduces abnormal nodes, posing
challenges and potentially biasing domain adaptation outcomes. Consequently, we
propose a framework, which utilizes the Graph Generation to enhance the quality
of DAOD (\method{}). Specifically, we introduce a Node Refinement module that
utilizes a memory bank to reconstruct noisy sampled nodes while applying
contrastive regularization to noisy features. To enhance semantic alignment, we
propose separating domain-specific styles from category invariance encoded
within graph covariances, which allows us to selectively remove domain-specific
styles while preserving category-invariant information, thus facilitating more
accurate semantic alignment across different domains. Furthermore, we propose a
Graph Optimization adaptor, leveraging variational inference to mitigate the
impact of abnormal nodes. Extensive experimentation across three adaptation
benchmarks validates that \method{} achieves state-of-the-art performance in
the task of unsupervised domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Gaffer: Relighting Any Object via Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07520v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07520v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, Noah Snavely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-image relighting is a challenging task that involves reasoning about
the complex interplay between geometry, materials, and lighting. Many prior
methods either support only specific categories of images, such as portraits,
or require special capture conditions, like using a flashlight. Alternatively,
some methods explicitly decompose a scene into intrinsic components, such as
normals and BRDFs, which can be inaccurate or under-expressive. In this work,
we propose a novel end-to-end 2D relighting diffusion model, called Neural
Gaffer, that takes a single image of any object and can synthesize an accurate,
high-quality relit image under any novel environmental lighting condition,
simply by conditioning an image generator on a target environment map, without
an explicit scene decomposition. Our method builds on a pre-trained diffusion
model, and fine-tunes it on a synthetic relighting dataset, revealing and
harnessing the inherent understanding of lighting present in the diffusion
model. We evaluate our model on both synthetic and in-the-wild Internet imagery
and demonstrate its advantages in terms of generalization and accuracy.
Moreover, by combining with other generative methods, our model enables many
downstream 2D tasks, such as text-based relighting and object insertion. Our
model can also operate as a strong relighting prior for 3D tasks, such as
relighting a radiance field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://neural-gaffer.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Diffusion Models are Training-free Motion Interpreter and
  Controller <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14864v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14864v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqi Xiao, Yifan Zhou, Shuai Yang, Xingang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation primarily aims to model authentic and customized motion
across frames, making understanding and controlling the motion a crucial topic.
Most diffusion-based studies on video motion focus on motion customization with
training-based paradigms, which, however, demands substantial training
resources and necessitates retraining for diverse models. Crucially, these
approaches do not explore how video diffusion models encode cross-frame motion
information in their features, lacking interpretability and transparency in
their effectiveness. To answer this question, this paper introduces a novel
perspective to understand, localize, and manipulate motion-aware features in
video diffusion models. Through analysis using Principal Component Analysis
(PCA), our work discloses that robust motion-aware feature already exists in
video diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating
content correlation information and filtering motion channels. MOFT provides a
distinct set of benefits, including the ability to encode comprehensive motion
information with clear interpretability, extraction without the need for
training, and generalizability across diverse architectures. Leveraging MOFT,
we propose a novel training-free video motion control framework. Our method
demonstrates competitive performance in generating natural and faithful motion,
providing architecture-agnostic insights and applicability in a variety of
downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project Page:
  https://xizaoqu.github.io/moft/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models Meet Remote Sensing: Principles, Methods, and
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08926v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08926v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Liu, Jun Yue, Shaobo Xia, Pedram Ghamisi, Weiying Xie, Leyuan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a newly emerging advance in deep generative models, diffusion models have
achieved state-of-the-art results in many fields, including computer vision,
natural language processing, and molecule design. The remote sensing (RS)
community has also noticed the powerful ability of diffusion models and quickly
applied them to a variety of tasks for image processing. Given the rapid
increase in research on diffusion models in the field of RS, it is necessary to
conduct a comprehensive review of existing diffusion model-based RS papers, to
help researchers recognize the potential of diffusion models and provide some
directions for further exploration. Specifically, this article first introduces
the theoretical background of diffusion models, and then systematically reviews
the applications of diffusion models in RS, including image generation,
enhancement, and interpretation. Finally, the limitations of existing RS
diffusion models and worthy research directions for further exploration are
discussed and summarized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Miao, Zhengyuan Yang, Kevin Lin, Ze Wang, Zicheng Liu, Lijuan Wang, Qiang Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in timestep-distilled diffusion models have enabled
high-quality image generation that rivals non-distilled multi-step models, but
with significantly fewer inference steps. While such models are attractive for
applications due to the low inference cost and latency, fine-tuning them with a
naive diffusion objective would result in degraded and blurry outputs. An
intuitive alternative is to repeat the diffusion distillation process with a
fine-tuned teacher model, which produces good results but is cumbersome and
computationally intensive; the distillation training usually requires magnitude
higher of training compute compared to fine-tuning for specific image styles.
In this paper, we present an algorithm named pairwise sample optimization
(PSO), which enables the direct fine-tuning of an arbitrary timestep-distilled
diffusion model. PSO introduces additional reference images sampled from the
current time-step distilled model, and increases the relative likelihood margin
between the training images and reference images. This enables the model to
retain its few-step generation ability, while allowing for fine-tuning of its
output distribution. We also demonstrate that PSO is a generalized formulation
which can be flexibly extended to both offline-sampled and online-sampled
pairwise data, covering various popular objectives for diffusion model
preference optimization. We evaluate PSO in both preference optimization and
other fine-tuning tasks, including style transfer and concept customization. We
show that PSO can directly adapt distilled models to human-preferred generation
with both offline and online-generated pairwise preference image data. PSO also
demonstrates effectiveness in style transfer and concept customization by
directly tuning timestep-distilled diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Imaging Constrained Diffusion for Brain PET Synthesis from
  Structural MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02504v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02504v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhui Yu, Mengqi Wu, Ling Yue, Andrea Bozoki, Mingxia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance imaging (MRI) and positron emission tomography (PET) are
increasingly used in multimodal analysis of neurodegenerative disorders. While
MRI is broadly utilized in clinical settings, PET is less accessible. Many
studies have attempted to use deep generative models to synthesize PET from MRI
scans. However, they often suffer from unstable training and inadequately
preserve brain functional information conveyed by PET. To this end, we propose
a functional imaging constrained diffusion (FICD) framework for 3D brain PET
image synthesis with paired structural MRI as input condition, through a new
constrained diffusion model (CDM). The FICD introduces noise to PET and then
progressively removes it with CDM, ensuring high output fidelity throughout a
stable training phase. The CDM learns to predict denoised PET with a functional
imaging constraint introduced to ensure voxel-wise alignment between each
denoised PET and its ground truth. Quantitative and qualitative analyses
conducted on 293 subjects with paired T1-weighted MRI and
18F-fluorodeoxyglucose (FDG)-PET scans suggest that FICD achieves superior
performance in generating FDG-PET data compared to state-of-the-art methods. We
further validate the effectiveness of the proposed FICD on data from a total of
1,262 subjects through three downstream tasks, with experimental results
suggesting its utility and generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TraceFL: Interpretability-Driven Debugging in Federated Learning via
  Neuron Provenance <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waris Gill, Ali Anwar, Muhammad Ali Gulzar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Federated Learning, clients train models on local data and send updates to
a central server, which aggregates them into a global model using a fusion
algorithm. This collaborative yet privacy-preserving training comes at a
cost--FL developers face significant challenges in attributing global model
predictions to specific clients. Localizing responsible clients is a crucial
step towards (a) excluding clients primarily responsible for incorrect
predictions and (b) encouraging clients who contributed high-quality models to
continue participating in the future. Existing ML explainability approaches are
inherently inapplicable as they are designed for single-model, centralized
training.
  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism
that identifies clients responsible for the global model's prediction by
tracking the flow of information from individual clients to the global model.
Since inference on different inputs activates a different set of neurons of the
global model, TraceFL dynamically quantifies the significance of the global
model's neurons in a given prediction. It then selectively picks a slice of the
most crucial neurons in the global model and maps them to the corresponding
neurons in every participating client to determine each client's contribution,
ultimately localizing the responsible client. We evaluate TraceFL on six
datasets, including two real-world medical imaging datasets and four neural
networks, including advanced models such as GPT. TraceFL achieves 99% accuracy
in localizing the responsible client in FL tasks spanning both image and text
classification tasks. At a time when state-of-the-art ML debugging approaches
are mostly domain-specific (e.g., image classification only), TraceFL is the
first technique to enable highly accurate automated reasoning across a wide
range of FL applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2025 IEEE/ACM 47th International Conference on Software
  Engineering (ICSE)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query Optimization for Parametric Knowledge Refinement in
  Retrieval-Augmented Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youan Cong, Cheng Wang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the \textit{Extract-Refine-Retrieve-Read} (ERRR) framework, a
novel approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theoretical Analysis of Recommendation Loss Functions under Negative
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Di Teodoro, Federico Siciliano, Nicola Tonellotto, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RSs) are pivotal in diverse domains such as e-commerce,
music streaming, and social media. This paper conducts a comparative analysis
of prevalent loss functions in RSs: Binary Cross-Entropy (BCE), Categorical
Cross-Entropy (CCE), and Bayesian Personalized Ranking (BPR). Exploring the
behaviour of these loss functions across varying negative sampling settings, we
reveal that BPR and CCE are equivalent when one negative sample is used.
Additionally, we demonstrate that all losses share a common global minimum.
Evaluation of RSs mainly relies on ranking metrics known as Normalized
Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR). We produce
bounds of the different losses for negative sampling settings to establish a
probabilistic lower bound for NDCG. We show that the BPR bound on NDCG is
weaker than that of BCE, contradicting the common assumption that BPR is
superior to BCE in RSs training. Experiments on five datasets and four models
empirically support these theoretical findings. Our code is available at
\url{https://anonymous.4open.science/r/recsys_losses} .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper 8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Legal Knowledge with Multi-Layered Embedding-Based Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Alberto de Oliveira Lima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the challenge of capturing the complexities of legal
knowledge by proposing a multi-layered embedding-based retrieval method for
legal and legislative texts. Creating embeddings not only for individual
articles but also for their components (paragraphs, clauses) and structural
groupings (books, titles, chapters, etc), we seek to capture the subtleties of
legal information through the use of dense vectors of embeddings, representing
it at varying levels of granularity. Our method meets various information needs
by allowing the Retrieval Augmented Generation system to provide accurate
responses, whether for specific segments or entire sections, tailored to the
user's query. We explore the concepts of aboutness, semantic chunking, and
inherent hierarchy within legal texts, arguing that this method enhances the
legal information retrieval. Despite the focus being on Brazil's legislative
methods and the Brazilian Constitution, which follow a civil law tradition, our
findings should in principle be applicable across different legal systems,
including those adhering to common law traditions. Furthermore, the principles
of the proposed method extend beyond the legal domain, offering valuable
insights for organizing and retrieving information in any field characterized
by information encoded in hierarchical text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Sustainability via Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhou, Lei Zhang, Honglei Zhang, Yixin Zhang, Xiaoxiong Zhang, Jie Zhang, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human behavioral patterns and consumption paradigms have emerged as pivotal
determinants in environmental degradation and climate change, with quotidian
decisions pertaining to transportation, energy utilization, and resource
consumption collectively precipitating substantial ecological impacts.
Recommender systems, which generate personalized suggestions based on user
preferences and historical interaction data, exert considerable influence on
individual behavioral trajectories. However, conventional recommender systems
predominantly optimize for user engagement and economic metrics, inadvertently
neglecting the environmental and societal ramifications of their
recommendations, potentially catalyzing over-consumption and reinforcing
unsustainable behavioral patterns. Given their instrumental role in shaping
user decisions, there exists an imperative need for sustainable recommender
systems that incorporate sustainability principles to foster eco-conscious and
socially responsible choices. This comprehensive survey addresses this critical
research gap by presenting a systematic analysis of sustainable recommender
systems. As these systems can simultaneously advance multiple sustainability
objectives--including resource conservation, sustainable consumer behavior, and
social impact enhancement--examining their implementations across distinct
application domains provides a more rigorous analytical framework. Through a
methodological analysis of domain-specific implementations encompassing
transportation, food, buildings, and auxiliary sectors, we can better elucidate
how these systems holistically advance sustainability objectives while
addressing sector-specific constraints and opportunities. Moreover, we
delineate future research directions for evolving recommender systems beyond
sustainability advocacy toward fostering environmental resilience and social
consciousness in society.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20pages, 10 figures. Working paper: https://github.com/enoche/SusRec</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overhead-free User-side Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, recommendation algorithms have been designed for service
developers. But recently, a new paradigm called user-side recommender systems
has been proposed. User-side recommender systems are built and used by end
users, in sharp contrast to traditional provider-side recommender systems. Even
if the official recommender system offered by the provider is not fair, end
users can create and enjoy their own user-side recommender systems by
themselves. Although the concept of user-side recommender systems is
attractive, the problem is they require tremendous communication costs between
the user and the official system. Even the most efficient user-side recommender
systems require about 5 times more costs than provider-side recommender
systems. Such high costs hinder the adoption of user-side recommender systems.
In this paper, we propose overhead-free user-side recommender systems,
RecCycle, which realizes user-side recommender systems without any
communication overhead. The main idea of RecCycle is to recycle past
recommendation results offered by the provider's recommender systems. The
ingredients of RecCycle can be retrieved ``for free,'' and it greatly reduces
the cost of user-side recommendations. In the experiments, we confirm that
RecCycle performs as well as state-of-the-art user-side recommendation
algorithms while RecCycle reduces costs significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2208.09864,
  arXiv:2403.15757</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automated Model Design on Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Yudong Liu, Feng Cheng, Yufan Cao, Feng Yan, Hai Li, Yiran Chen, Wei Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing popularity of deep learning models has created new
opportunities for developing AI-based recommender systems. Designing
recommender systems using deep neural networks requires careful architecture
design, and further optimization demands extensive co-design efforts on jointly
optimizing model architecture and hardware. Design automation, such as
Automated Machine Learning (AutoML), is necessary to fully exploit the
potential of recommender model design, including model choices and
model-hardware co-design strategies. We introduce a novel paradigm that
utilizes weight sharing to explore abundant solution spaces. Our paradigm
creates a large supernet to search for optimal architectures and co-design
strategies to address the challenges of data multi-modality and heterogeneity
in the recommendation domain. From a model perspective, the supernet includes a
variety of operators, dense connectivity, and dimension search options. From a
co-design perspective, it encompasses versatile Processing-In-Memory (PIM)
configurations to produce hardware-efficient models. Our solution space's
scale, heterogeneity, and complexity pose several challenges, which we address
by proposing various techniques for training and evaluating the supernet. Our
crafted models show promising results on three Click-Through Rates (CTR)
prediction benchmarks, outperforming both manually designed and AutoML-crafted
models with state-of-the-art performance when focusing solely on architecture
search. From a co-design perspective, we achieve 2x FLOPs efficiency, 1.8x
energy efficiency, and 1.5x performance improvements in recommender models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACM Transactions on Recommender Systems. arXiv admin
  note: substantial text overlap with arXiv:2207.07187</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Interaction Fusion Self-Distillation Network For CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Sang, Qiuze Ru, Honghao Li, Yiwen Zhang, Xindong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-Through Rate (CTR) prediction plays a vital role in recommender
systems, online advertising, and search engines. Most of the current approaches
model feature interactions through stacked or parallel structures, with some
employing knowledge distillation for model compression. However, we observe
some limitations with these approaches: (1) In parallel structure models, the
explicit and implicit components are executed independently and simultaneously,
which leads to insufficient information sharing within the feature set. (2) The
introduction of knowledge distillation technology brings about the problems of
complex teacher-student framework design and low knowledge transfer efficiency.
(3) The dataset and the process of constructing high-order feature interactions
contain significant noise, which limits the model's effectiveness. To address
these limitations, we propose FSDNet, a CTR prediction framework incorporating
a plug-and-play fusion self-distillation module. Specifically, FSDNet forms
connections between explicit and implicit feature interactions at each layer,
enhancing the sharing of information between different features. The deepest
fused layer is then used as the teacher model, utilizing self-distillation to
guide the training of shallow layers. Empirical evaluation across four
benchmark datasets validates the framework's efficacy and generalization
capabilities. The code is available on https://github.com/coder-qiu/FSDNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaS&S: a One-Shot Supernet Approach for Automatic Embedding Size Search
  in Deep Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Wei, Yuekui Yang, Yang Zhang, Haiyang Wu, Meixi Liu, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning Recommendation Model(DLRM)s utilize the embedding layer to
represent various categorical features. Traditional DLRMs adopt unified
embedding size for all features, leading to suboptimal performance and
redundant parameters. Thus, lots of Automatic Embedding size Search (AES) works
focus on obtaining mixed embedding sizes with strong model performance.
However, previous AES works can hardly address several challenges together: (1)
The search results of embedding sizes are unstable; (2) Recommendation effect
with AES results is unsatisfactory; (3) Memory cost of embeddings is
uncontrollable. To address these challenges, we propose a novel one-shot AES
framework called AdaS&S, in which a supernet encompassing various candidate
embeddings is built and AES is performed as searching network architectures
within it. Our framework contains two main stages: In the first stage, we
decouple training parameters from searching embedding sizes, and propose the
Adaptive Sampling method to yield a well-trained supernet, which further helps
to produce stable AES results. In the second stage, to obtain embedding sizes
that benefits the model effect, we design a reinforcement learning search
process which utilizes the supernet trained previously. Meanwhile, to adapt
searching to specific resource constraint, we introduce the resource
competition penalty to balance the model effectiveness and memory cost of
embeddings. We conduct extensive experiments on public datasets to show the
superiority of AdaS&S. Our method could improve AUC by about 0.3% while saving
about 20% of model parameters. Empirical analysis also shows that the stability
of searching results in AdaS&S significantly exceeds other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Link Prediction with Fuzzy Graph Attention Networks and
  Dynamic Negative Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction is crucial for understanding complex networks but traditional
Graph Neural Networks (GNNs) often rely on random negative sampling, leading to
suboptimal performance. This paper introduces Fuzzy Graph Attention Networks
(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative
sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)
systematically selects high-quality negative edges based on fuzzy similarities,
improving training efficiency. FGAT layer incorporates fuzzy rough set
principles, enabling robust and discriminative node representations.
Experiments on two research collaboration networks demonstrate FGAT's superior
link prediction accuracy, outperforming state-of-the-art baselines by
leveraging the power of fuzzy rough sets for effective negative sampling and
node feature learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicit and Implicit Semantic Ranking Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Zhu, Thomas Lin, Vishal Anand, Matthew Calderwood, Eric Clausen-Brown, Gord Lueck, Wen-wai Yim, Cheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core challenge in numerous real-world applications is to match an inquiry
to the best document from a mutable and finite set of candidates. Existing
industry solutions, especially latency-constrained services, often rely on
similarity algorithms that sacrifice quality for speed. In this paper we
introduce a generic semantic learning-to-rank framework, Self-training Semantic
Cross-attention Ranking (sRank). This transformer-based framework uses linear
pairwise loss with mutable training batch sizes and achieves quality gains and
high efficiency, and has been applied effectively to show gains on two industry
tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and
Ambient Clinical Intelligence (ACI). In Smart Reply, sRank assists live
customers with technical support by selecting the best reply from predefined
solutions based on consumer and support agent messages. It achieves 11.7% gain
in offline top-one accuracy on the SR task over the previous system, and has
enabled 38.7% time reduction in composing messages in telemetry recorded since
its general release in January 2021. In the ACI task, sRank selects relevant
historical physician templates that serve as guidance for a text summarization
model to generate higher quality medical notes. It achieves 35.5% top-one
accuracy gain, along with 46% relative ROUGE-L gain in generated medical notes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Early FIRST Reproduction and Improvements to Single-Token Decoding
  for Fast Listwise Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Chen, Ronak Pradeep, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have demonstrated that large language models (LLMs) excel as
listwise rerankers, but their high computational demands remain a barrier to
widespread adoption. Further, the traditional language modeling (LM) objective
is not ideally suited for reranking tasks. FIRST is a novel approach that
addresses these challenges by integrating a learning-to-rank objective and
leveraging the logits of only the first generated token, thereby significantly
reducing inference latency compared to traditional LLM rerankers. In this
study, we extend the evaluation of FIRST to the TREC Deep Learning datasets
(DL19-22), validating its robustness across diverse domains. We investigate the
influence of different first-stage retrievers on FIRST rerankers, observing
diminishing returns and patterns consistent with traditional LLM rerankers.
Through applying the FIRST objective to a broader range of backbone models, we
achieve effectiveness surpassing the original implementation. Our experiments
confirm that fast reranking with single-token logits does not compromise
out-of-domain reranking quality. To better quantify the computational savings
in the original study, we measure and compare latency to find a 21%-42% gain
across various models and benchmarks. Moreover, while LM training implicitly
improves zero-shot single-token reranking, our experiments also raise questions
about whether LM pre-training may hinder subsequent fine-tuning with the FIRST
objective. These findings pave the way for more efficient and effective
listwise reranking in future applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Content-Based Collaborative Generation for Recommender Systems <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Wang, Zhaochun Ren, Weiwei Sun, Jiyuan Yang, Zhixiang Liang, Xin Chen, Ruobing Xie, Su Yan, Xu Zhang, Pengjie Ren, Zhumin Chen, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have emerged as a promising utility to enhance recommender
systems. It is essential to model both item content and user-item collaborative
interactions in a unified generative framework for better recommendation.
Although some existing large language model (LLM)-based methods contribute to
fusing content information and collaborative signals, they fundamentally rely
on textual language generation, which is not fully aligned with the
recommendation task. How to integrate content knowledge and collaborative
interaction signals in a generative framework tailored for item recommendation
is still an open research challenge.
  In this paper, we propose content-based collaborative generation for
recommender systems, namely ColaRec. ColaRec is a sequence-to-sequence
framework which is tailored for directly generating the recommended item
identifier. Precisely, the input sequence comprises data pertaining to the
user's interacted items, and the output sequence represents the generative
identifier (GID) for the suggested item. To model collaborative signals, the
GIDs are constructed from a pretrained collaborative filtering model, and the
user is represented as the content aggregation of interacted items. To this
end, ColaRec captures both collaborative signals and content information in a
unified framework. Then an item indexing task is proposed to conduct the
alignment between the content-based semantic space and the interaction-based
collaborative space. Besides, a contrastive loss is further introduced to
ensure that items with similar collaborative GIDs have similar content
representations. To verify the effectiveness of ColaRec, we conduct experiments
on four benchmark datasets. Empirical results demonstrate the superior
performance of ColaRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CIKM 2024; GitHub:
  https://github.com/Junewang0614/ColaRec</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Enhancing Prediction in Social Network
  Advertisement through Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qikai Yang, Panfeng Li, Xinhe Xu, Zhicheng Ding, Wenjing Zhou, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving landscape of social network advertising, the volume and
accuracy of data play a critical role in the performance of predictive models.
However, the development of robust predictive algorithms is often hampered by
the limited size and potential bias present in real-world datasets. This study
presents and explores a generative augmentation framework of social network
advertising data. Our framework explores three generative models for data
augmentation - Generative Adversarial Networks (GANs), Variational Autoencoders
(VAEs), and Gaussian Mixture Models (GMMs) - to enrich data availability and
diversity in the context of social network advertising analytics effectiveness.
By performing synthetic extensions of the feature space, we find that through
data augmentation, the performance of various classifiers has been
quantitatively improved. Furthermore, we compare the relative performance gains
brought by each data augmentation technique, providing insights for
practitioners to select appropriate techniques to enhance model performance.
This paper contributes to the literature by showing that synthetic data
augmentation alleviates the limitations imposed by small or imbalanced datasets
in the field of social network advertising. At the same time, this article also
provides a comparative perspective on the practicality of different data
augmentation methods, thereby guiding practitioners to choose appropriate
techniques to enhance model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 4th International Conference on Machine Learning and
  Intelligent Systems Engineering (MLISE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Identification of Hate Speech towards Islam using Graph
  Neural Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04916v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04916v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Islamophobic language on online platforms fosters intolerance, making
detection and elimination crucial for promoting harmony. Traditional hate
speech detection models rely on NLP techniques like tokenization,
part-of-speech tagging, and encoder-decoder models. However, Graph Neural
Networks (GNNs), with their ability to utilize relationships between data
points, offer more effective detection and greater explainability. In this
work, we represent speeches as nodes and connect them with edges based on their
context and similarity to develop the graph. This study introduces a novel
paradigm using GNNs to identify and explain hate speech towards Islam. Our
model leverages GNNs to understand the context and patterns of hate speech by
connecting texts via pretrained NLP-generated word embeddings, achieving
state-of-the-art performance and enhancing detection accuracy while providing
valuable explanations. This highlights the potential of GNNs in combating
online hate speech and fostering a safer, more inclusive online environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in: (i) NeurIPS 2023 : Muslims in ML Workshop (Non-archival)
  (https://www.musiml.org/schedule/#:~:text=Azmine%20Toushik%20Wasi) (ii) EMNLP
  2024 : NLP for Positive Impact Workshop (Archival; ACL Anthology:
  https://aclanthology.org/2024.nlp4pi-1.23/)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMPhy: Complex Physical Reasoning Using Large Language Models and World
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoop Cherian, Radu Corcodel, Siddarth Jain, Diego Romeres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical reasoning is an important skill needed for robotic agents when
operating in the real world. However, solving such reasoning problems often
involves hypothesizing and reflecting over complex multi-body interactions
under the effect of a multitude of physical forces and thus learning all such
interactions poses a significant hurdle for state-of-the-art machine learning
frameworks, including large language models (LLMs). To study this problem, we
propose a new physical reasoning task and a dataset, dubbed TraySim. Our task
involves predicting the dynamics of several objects on a tray that is given an
external impact -- the domino effect of the ensued object interactions and
their dynamics thus offering a challenging yet controlled setup, with the goal
of reasoning being to infer the stability of the objects after the impact. To
solve this complex physical reasoning task, we present LLMPhy, a zero-shot
black-box optimization framework that leverages the physics knowledge and
program synthesis abilities of LLMs, and synergizes these abilities with the
world models built into modern physics engines. Specifically, LLMPhy uses an
LLM to generate code to iteratively estimate the physical hyperparameters of
the system (friction, damping, layout, etc.) via an implicit
analysis-by-synthesis approach using a (non-differentiable) simulator in the
loop and uses the inferred parameters to imagine the dynamics of the scene
towards solving the reasoning task. To show the effectiveness of LLMPhy, we
present experiments on our TraySim dataset to predict the steady-state poses of
the objects. Our results show that the combination of the LLM and the physics
engine leads to state-of-the-art zero-shot physical reasoning performance,
while demonstrating superior convergence against standard black-box
optimization methods and better estimation of the physical parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leonardo vindicated: Pythagorean trees for minimal reconstruction of the
  natural branching structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dymitr Ruta, Corrado Mio, Ernesto Damiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trees continue to fascinate with their natural beauty and as engineering
masterpieces optimal with respect to several independent criteria. Pythagorean
tree is a well-known fractal design that realistically mimics the natural tree
branching structures. We study various types of Pythagorean-like fractal trees
with different shapes of the base, branching angles and relaxed scales in an
attempt to identify and explain which variants are the closest match to the
branching structures commonly observed in the natural world. Pursuing
simultaneously the realism and minimalism of the fractal tree model, we have
developed a flexibly parameterised and fast algorithm to grow and visually
examine deep Pythagorean-inspired fractal trees with the capability to orderly
over- or underestimate the Leonardo da Vinci's tree branching rule as well as
control various imbalances and branching angles. We tested the realism of the
generated fractal tree images by means of the classification accuracy of
detecting natural tree with the transfer-trained deep Convolutional Neural
Networks (CNNs). Having empirically established the parameters of the fractal
trees that maximize the CNN's natural tree class classification accuracy we
have translated them back to the scales and angles of branches and came to the
interesting conclusions that support the da Vinci branching rule and golden
ratio based scaling for both the shape of the branch and imbalance between the
child branches, and claim the flexibly parameterized fractal trees can be used
to generate artificial examples to train robust detectors of different species
of trees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, lots of hi res figures I had to reduce quality of,
  submitting as a requirement to the Theory of Computing Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models as Causal Effect Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucius E. J. Bynum, Kyunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for large language model (LLM) based data generation
with controllable causal structure. In particular, we define a procedure for
turning any language model and any directed acyclic graph (DAG) into a
sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM
is a causal model with user-defined structure and LLM-defined structural
equations. We characterize how an SD-SCM allows sampling from observational,
interventional, and counterfactual distributions according to the desired
causal structure. We then leverage this procedure to propose a new type of
benchmark for causal inference methods, generating individual-level
counterfactual data without needing to manually specify functional
relationships between variables. We create an example benchmark consisting of
thousands of datasets, and test a suite of popular estimation methods on these
datasets for average, conditional average, and individual treatment effect
estimation, both with and without hidden confounding. Apart from generating
data, the same procedure also allows us to test for the presence of a causal
effect that might be encoded in an LLM. This procedure can underpin auditing
LLMs for misinformation, discrimination, or otherwise undesirable behavior. We
believe SD-SCMs can serve as a useful tool in any application that would
benefit from sequential data with controllable causal structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model
  with Compact Wavelet Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Sanghi, Aliasghar Khani, Pradyumna Reddy, Arianna Rampini, Derek Cheung, Kamal Rahimi Malekshan, Kanika Madan, Hooman Shayani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale 3D generative models require substantial computational resources
yet often fall short in capturing fine details and complex geometries at high
resolutions. We attribute this limitation to the inefficiency of current
representations, which lack the compactness required to model the generative
models effectively. To address this, we introduce a novel approach called
Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based,
compact latent encodings. Specifically, we compress a $256^3$ signed distance
field into a $12^3 \times 4$ latent grid, achieving an impressive 2427x
compression ratio with minimal loss of detail. This high level of compression
allows our method to efficiently train large-scale generative networks without
increasing the inference time. Our models, both conditional and unconditional,
contain approximately one billion parameters and successfully generate
high-quality 3D shapes at $256^3$ resolution. Moreover, WaLa offers rapid
inference, producing shapes within two to four seconds depending on the
condition, despite the model's scale. We demonstrate state-of-the-art
performance across multiple datasets, with significant improvements in
generation quality, diversity, and computational efficiency. We open-source our
code and, to the best of our knowledge, release the largest pretrained 3D
generative models across different modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Effectiveness of Explainability Methods in Parkinson's
  Detection from Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Mancini, Francesco Paissan, Paolo Torroni, Cem Subakan, Mirco Ravanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech impairments in Parkinson's disease (PD) provide significant early
indicators for diagnosis. While models for speech-based PD detection have shown
strong performance, their interpretability remains underexplored. This study
systematically evaluates several explainability methods to identify PD-specific
speech features, aiming to support the development of accurate, interpretable
models for clinical decision-making in PD diagnosis and monitoring. Our
methodology involves (i) obtaining attributions and saliency maps using
mainstream interpretability techniques, (ii) quantitatively evaluating the
faithfulness of these maps and their combinations obtained via union and
intersection through a range of established metrics, and (iii) assessing the
information conveyed by the saliency maps for PD detection from an auxiliary
classifier. Our results reveal that, while explanations are aligned with the
classifier, they often fail to provide valuable information for domain experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this research: author
  order is alphabetical</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivational Morphology Reveals Analogical Generalization in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Hofmann, Leonie Weissweiler, David Mortensen, Hinrich Schütze, Janet Pierrehumbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What mechanisms underlie linguistic generalization in large language models
(LLMs)? This question has attracted considerable attention, with most studies
analyzing the extent to which the language skills of LLMs resemble rules. As of
yet, it is not known whether linguistic generalization in LLMs could equally
well be explained as the result of analogical processes, which can be
formalized as similarity operations on stored exemplars. A key shortcoming of
prior research is its focus on linguistic phenomena with a high degree of
regularity, for which rule-based and analogical approaches make the same
predictions. Here, we instead examine derivational morphology, specifically
English adjective nominalization, which displays notable variability. We
introduce a new method for investigating linguistic generalization in LLMs:
focusing on GPT-J, we fit cognitive models that instantiate rule-based and
analogical learning to the LLM training data and compare their predictions on a
set of nonce adjectives with those of the LLM, allowing us to draw direct
conclusions regarding underlying mechanisms. As expected, rule-based and
analogical models explain the predictions of GPT-J equally well for adjectives
with regular nominalization patterns. However, for adjectives with variable
nominalization patterns, the analogical model provides a much better match.
Furthermore, GPT-J's behavior is sensitive to the individual word frequencies,
even for regular forms, a behavior that is consistent with an analogical
account of regular forms but not a rule-based one. These findings refute the
hypothesis that GPT-J's linguistic generalization on adjective nominalization
involves rules, suggesting similarity operations on stored exemplars as the
underlying mechanism. Overall, our study suggests that analogical processes
play a bigger role in the linguistic generalization of LLMs than previously
thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact, Tractable Gauss-Newton Optimization in Deep Reversible
  Architectures Reveal Poor Generalization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Buffelli, Jamie McGowan, Wangkun Xu, Alexandru Cioba, Da-shan Shiu, Guillaume Hennequin, Alberto Bernacchia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Second-order optimization has been shown to accelerate the training of deep
neural networks in many applications, often yielding faster progress per
iteration on the training loss compared to first-order optimizers.However, the
generalization properties of second-order methods are still being debated.
Theoretical investigations have proved difficult to carry out outside the
tractable settings of heavily simplified model classes -- thus, the relevance
of existing theories to practical deep learning applications remains unclear.
Similarly, empirical studies in large-scale models and real datasets are
significantly confounded by the necessity to approximate second-order updates
in practice. It is often unclear whether the observed generalization behaviour
arises specifically from the second-order nature of the parameter updates, or
instead reflects the specific structured (e.g.\ Kronecker) approximations used
or any damping-based interpolation towards first-order updates. Here, we show
for the first time that exact Gauss-Newton (GN) updates take on a tractable
form in a class of deep reversible architectures that are sufficiently
expressive to be meaningfully applied to common benchmark datasets. We exploit
this novel setting to study the training and generalization properties of the
GN optimizer. We find that exact GN generalizes poorly. In the mini-batch
training setting, this manifests as rapidly saturating progress even on the
\emph{training} loss, with parameter updates found to overfit each
mini-batchatch without producing the features that would support generalization
to other mini-batches. We show that our experiments run in the ``lazy'' regime,
in which the neural tangent kernel (NTK) changes very little during the course
of training. This behaviour is associated with having no significant changes in
neural representations, explaining the lack of generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly Robust Regression Discontinuity Designs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a doubly robust (DR) estimator for regression
discontinuity (RD) designs. In RD designs, treatment effects are estimated in a
quasi-experimental setting where treatment assignment depends on whether a
running variable surpasses a predefined cutoff. A common approach in RD
estimation is to apply nonparametric regression methods, such as local linear
regression. In such an approach, the validity relies heavily on the consistency
of nonparametric estimators and is limited by the nonparametric convergence
rate, thereby preventing $\sqrt{n}$-consistency. To address these issues, we
propose the DR-RD estimator, which combines two distinct estimators for the
conditional expected outcomes. If either of these estimators is consistent, the
treatment effect estimator remains consistent. Furthermore, due to the
debiasing effect, our proposed estimator achieves $\sqrt{n}$-consistency if
both regression estimators satisfy certain mild conditions, which also
simplifies statistical inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Control of Mechanical Ventilators with Learned Respiratory
  Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Ronald Ward, Dylan M. Asmar, Mansur Arief, Jana Krystofova Mike, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deciding on appropriate mechanical ventilator management strategies
significantly impacts the health outcomes for patients with respiratory
diseases. Acute Respiratory Distress Syndrome (ARDS) is one such disease that
requires careful ventilator operation to be effectively treated. In this work,
we frame the management of ventilators for patients with ARDS as a sequential
decision making problem using the Markov decision process framework. We
implement and compare controllers based on clinical guidelines contained in the
ARDSnet protocol, optimal control theory, and learned latent dynamics
represented as neural networks. The Pulse Physiology Engine's respiratory
dynamics simulator is used to establish a repeatable benchmark, gather
simulated data, and quantitatively compare these controllers. We score
performance in terms of measured improvement in established ARDS health markers
(pertaining to improved respiratory rate, oxygenation, and vital signs). Our
results demonstrate that techniques leveraging neural networks and optimal
control can automatically discover effective ventilation management strategies
without access to explicit ventilator management procedures or guidelines (such
as those defined in the ARDSnet protocol).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE 37th International Symposium on Computer-Based Medical
  Systems (CBMS), 7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sleep Staging from Airflow Signals Using Fourier Approximations of
  Persistence Curves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Manjunath, Hau-Tieng Wu, Aarti Sathyanarayana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sleep staging is a challenging task, typically manually performed by sleep
technologists based on electroencephalogram and other biosignals of patients
taken during overnight sleep studies. Recent work aims to leverage automated
algorithms to perform sleep staging not based on electroencephalogram signals,
but rather based on the airflow signals of subjects. Prior work uses ideas from
topological data analysis (TDA), specifically Hermite function expansions of
persistence curves (HEPC) to featurize airflow signals. However, finite order
HEPC captures only partial information. In this work, we propose Fourier
approximations of persistence curves (FAPC), and use this technique to perform
sleep staging based on airflow signals. We analyze performance using an XGBoost
model on 1155 pediatric sleep studies taken from the Nationwide Children's
Hospital Sleep DataBank (NCHSDB), and find that FAPC methods provide
complimentary information to HEPC methods alone, leading to a 4.9% increase in
performance over baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Convergence of Continual Federated Learning Using Incrementally
  Aggregated Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satish Kumar Keshri, Nazreen Shah, Ranjitha Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The holy grail of machine learning is to enable Continual Federated Learning
(CFL) to enhance the efficiency, privacy, and scalability of AI systems while
learning from streaming data. The primary challenge of a CFL system is to
overcome global catastrophic forgetting, wherein the accuracy of the global
model trained on new tasks declines on the old tasks. In this work, we propose
Continual Federated Learning with Aggregated Gradients (C-FLAG), a novel
replay-memory based federated strategy consisting of edge-based gradient
updates on memory and aggregated gradients on the current data. We provide
convergence analysis of the C-FLAG approach which addresses forgetting and bias
while converging at a rate of $O(1/\sqrt{T})$ over $T$ communication rounds. We
formulate an optimization sub-problem that minimizes catastrophic forgetting,
translating CFL into an iterative algorithm with adaptive learning rates that
ensure seamless learning across tasks. We empirically show that C-FLAG
outperforms several state-of-the-art baselines on both task and
class-incremental settings with respect to metrics such as accuracy and
forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tukey g-and-h neural network regression for non-Gaussian data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur P. Guillaumin, Natalia Efremova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses non-Gaussian regression with neural networks via the use
of the Tukey g-and-h distribution.The Tukey g-and-h transform is a flexible
parametric transform with two parameters $g$ and $h$ which, when applied to a
standard normal random variable, introduces both skewness and kurtosis,
resulting in a distribution commonly called the Tukey g-and-h distribution.
Specific values of $g$ and $h$ produce good approximations to other families of
distributions, such as the Cauchy and student-t distributions. The flexibility
of the Tukey g-and-h distribution has driven its popularity in the statistical
community, in applied sciences and finance. In this work we consider the
training of a neural network to predict the parameters of a Tukey g-and-h
distribution in a regression framework via the minimization of the
corresponding negative log-likelihood, despite the latter having no closed-form
expression. We demonstrate the efficiency of our procedure in simulated
examples and apply our method to a real-world dataset of global crop yield for
several types of crops. Finally, we show how we can carry out a goodness-of-fit
analysis between the predicted distributions and the test data. A Pytorch
implementation is made available on Github and as a Pypi package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Memory Mechanisms for Decision Making through Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Yue, Bo Liu, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Partially Observable Markov Decision Processes, integrating an agent's
history into memory poses a significant challenge for decision-making.
Traditional imitation learning, relying on observation-action pairs for expert
demonstrations, fails to capture the expert's memory mechanisms used in
decision-making. To capture memory processes as demonstrations, we introduce
the concept of \textbf{memory dependency pairs} $(p, q)$ indicating that events
at time $p$ are recalled for decision-making at time $q$. We introduce
\textbf{AttentionTuner} to leverage memory dependency pairs in Transformers and
find significant improvements across several tasks compared to standard
Transformers when evaluated on Memory Gym and the Long-term Memory Benchmark.
Code is available at https://github.com/WilliamYue37/AttentionTuner .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Low-bit Communication for Tensor Parallel LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry Dong, Tyler Johnson, Minsik Cho, Emad Soroush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor parallelism provides an effective way to increase server large
language model (LLM) inference efficiency despite adding an additional
communication cost. However, as server LLMs continue to scale in size, they
will need to be distributed across more devices, magnifying the communication
cost. One way to approach this problem is with quantization, but current
methods for LLMs tend to avoid quantizing the features that tensor parallelism
needs to communicate. Taking advantage of consistent outliers in communicated
features, we introduce a quantization method that reduces communicated values
on average from 16 bits to 4.2 bits while preserving nearly all of the original
performance. For instance, our method maintains around 98.0% and 99.5% of Gemma
2 27B's and Llama 2 13B's original performance, respectively, averaged across
all tasks we evaluated on.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly Mild Generalization for Offline Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiu Mao, Qi Wang, Yun Qu, Yuhang Jiang, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) suffers from the extrapolation error and
value overestimation. From a generalization perspective, this issue can be
attributed to the over-generalization of value functions or policies towards
out-of-distribution (OOD) actions. Significant efforts have been devoted to
mitigating such generalization, and recent in-sample learning approaches have
further succeeded in entirely eschewing it. Nevertheless, we show that mild
generalization beyond the dataset can be trusted and leveraged to improve
performance under certain conditions. To appropriately exploit generalization
in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild
action generalization and (ii) mild generalization propagation. The former
refers to selecting actions in a close neighborhood of the dataset to maximize
the Q values. Even so, the potential erroneous generalization can still be
propagated, accumulated, and exacerbated by bootstrapping. In light of this,
the latter concept is introduced to mitigate the generalization propagation
without impeding the propagation of RL learning signals. Theoretically, DMG
guarantees better performance than the in-sample optimal policy in the oracle
generalization scenario. Even under worst-case generalization, DMG can still
control value overestimation at a certain level and lower bound the
performance. Empirically, DMG achieves state-of-the-art performance across
Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting
from its flexibility in both generalization aspects, DMG enjoys a seamless
transition from offline to online learning and attains strong online
fine-tuning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. arXiv admin note: substantial text overlap
  with arXiv:2410.19400</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction of Acoustic Communication Performance for AUVs using Gaussian
  Process Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Gao, Harun Yetkin, McMahon James, Daniel J. Stilwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperating autonomous underwater vehicles (AUVs) often rely on acoustic
communication to coordinate their actions effectively. However, the reliability
of underwater acoustic communication decreases as the communication range
between vehicles increases. Consequently, teams of cooperating AUVs typically
make conservative assumptions about the maximum range at which they can
communicate reliably. To address this limitation, we propose a novel approach
that involves learning a map representing the probability of successful
communication based on the locations of the transmitting and receiving
vehicles. This probabilistic communication map accounts for factors such as the
range between vehicles, environmental noise, and multi-path effects at a given
location. In pursuit of this goal, we investigate the application of Gaussian
process binary classification to generate the desired communication map. We
specialize existing results to this specific binary classification problem and
explore methods to incorporate uncertainty in vehicle location into the mapping
process. Furthermore, we compare the prediction performance of the probability
communication map generated using binary classification with that of a
signal-to-noise ratio (SNR) communication map generated using Gaussian process
regression. Our approach is experimentally validated using communication and
navigation data collected during trials with a pair of Virginia Tech 690 AUVs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Stochastic Optimization Framework for Private and Fair Learning From
  Decentralized Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh Gupta, A. S. Poornash, Andrew Lowy, Meisam Razaviyayn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are often trained on sensitive data (e.g., medical
records and race/gender) that is distributed across different "silos" (e.g.,
hospitals). These federated learning models may then be used to make
consequential decisions, such as allocating healthcare resources. Two key
challenges emerge in this setting: (i) maintaining the privacy of each person's
data, even if other silos or an adversary with access to the central server
tries to infer this data; (ii) ensuring that decisions are fair to different
demographic groups (e.g., race/gender). In this paper, we develop a novel
algorithm for private and fair federated learning (FL). Our algorithm satisfies
inter-silo record-level differential privacy (ISRL-DP), a strong notion of
private FL requiring that silo i's sent messages satisfy record-level
differential privacy for all i. Our framework can be used to promote different
fairness notions, including demographic parity and equalized odds. We prove
that our algorithm converges under mild smoothness assumptions on the loss
function, whereas prior work required strong convexity for convergence. As a
byproduct of our analysis, we obtain the first convergence guarantee for
ISRL-DP nonconvex-strongly concave min-max FL. Experiments demonstrate the
state-of-the-art fairness-accuracy tradeoffs of our algorithm across different
privacy levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INTRABENCH: Interactive Radiological Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Ulrich, Tassilo Wald, Emily Tempus, Maximilian Rokuss, Paul F. Jaeger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current interactive segmentation approaches, inspired by the success of
META's Segment Anything model, have achieved notable advancements, however,
they come with substantial limitations that hinder their practical application
in real clinical scenarios. These include unrealistic human interaction
requirements, such as slice-by-slice operations for 2D models on 3D data, a
lack of iterative refinement, and insufficient evaluation experiments. These
shortcomings prevent accurate assessment of model performance and lead to
inconsistent outcomes across studies. IntRaBench overcomes these challenges by
offering a comprehensive and reproducible framework for evaluating interactive
segmentation methods in realistic, clinically relevant scenarios. It includes
diverse datasets, target structures, and segmentation models, and provides a
flexible codebase that allows seamless integration of new models and prompting
strategies. Additionally, we introduce advanced techniques to minimize
clinician interaction, ensuring fair comparisons between 2D and 3D models. By
open-sourcing IntRaBench, we invite the research community to integrate their
models and prompting techniques, ensuring continuous and transparent evaluation
of interactive segmentation models in 3D medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Undergoing Peer-Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse capability and scaling of diffusion and auto-regressive models
  when learning abstract rules <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binxu Wang, Jiaqi Shang, Haim Sompolinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans excel at discovering regular structures from limited samples and
applying inferred rules to novel settings. We investigate whether modern
generative models can similarly learn underlying rules from finite samples and
perform reasoning through conditional sampling. Inspired by Raven's Progressive
Matrices task, we designed GenRAVEN dataset, where each sample consists of
three rows, and one of 40 relational rules governing the object position,
number, or attributes applies to all rows. We trained generative models to
learn the data distribution, where samples are encoded as integer arrays to
focus on rule learning. We compared two generative model families: diffusion
(EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their
ability to generate structurally consistent samples and perform panel
completion via unconditional and conditional sampling. We found diffusion
models excel at unconditional generation, producing more novel and consistent
samples from scratch and memorizing less, but performing less well in panel
completion, even with advanced conditional sampling methods. Conversely,
autoregressive models excel at completing missing panels in a rule-consistent
manner but generate less consistent samples unconditionally. We observe diverse
data scaling behaviors: for both model families, rule learning emerges at a
certain dataset size - around 1000s examples per rule. With more training data,
diffusion models improve both their unconditional and conditional generation
capabilities. However, for autoregressive models, while panel completion
improves with more training data, unconditional generation consistency
declines. Our findings highlight complementary capabilities and limitations of
diffusion and autoregressive models in rule learning and reasoning tasks,
suggesting avenues for further research into their mechanisms and potential for
human-like reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures. Accepted to NeurIPS2024 Workshop on System 2
  Reasoning At Scale as long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDXFormer: Boosting Remote Sensing Change Detection with Extended Long
  Short-Term Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenkai Wu, Xiaowen Ma, Rongrong Lian, Zhentao Lin, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In complex scenes and varied conditions, effectively integrating
spatial-temporal context is crucial for accurately identifying changes.
However, current RS-CD methods lack a balanced consideration of performance and
efficiency. CNNs lack global context, Transformers have quadratic computational
complexity, and Mambas are restricted by CUDA acceleration. In this paper, we
propose CDXFormer, with a core component that is a powerful XLSTM-based feature
enhancement layer, integrating the advantages of linear computational
complexity, global context perception, and strong interpret-ability.
Specifically, we introduce a scale-specific Feature Enhancer layer,
incorporating a Cross-Temporal Global Perceptron customized for
semantic-accurate deep features, and a Cross-Temporal Spatial Refiner
customized for detail-rich shallow features. Additionally, we propose a
Cross-Scale Interactive Fusion module to progressively interact global change
representations with spatial responses. Extensive experimental results
demonstrate that CDXFormer achieves state-of-the-art performance across three
benchmark datasets, offering a compelling balance between efficiency and
accuracy. Code is available at https://github.com/xwmaxwma/rschange.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tucano: Advancing Neural Text Generation for Portuguese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Kluge Corrêa, Aniket Sen, Sophia Falk, Shiza Fatimah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advances have been made in natural language processing in recent
years. However, our current deep learning approach to language modeling
requires substantial resources in terms of data and computation. One of the
side effects of this data-hungry paradigm is the current schism between
languages, separating those considered high-resource, where most of the
development happens and resources are available, and the low-resource ones,
which struggle to attain the same level of performance and autonomy. This study
aims to introduce a new set of resources to stimulate the future development of
neural text generation in Portuguese. In this work, we document the development
of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting
to 200 billion tokens. Via this corpus, we trained a series of
decoder-transformers named Tucano. Our models perform equal or superior to
other Portuguese and multilingual language models of similar size in several
Portuguese benchmarks. The evaluation of our models also reveals that model
performance on many currently available benchmarks used by the Portuguese NLP
community has little to no correlation with the scaling of token ingestion
during training, highlighting the limitations of such evaluations when it comes
to the assessment of Portuguese generative language models. All derivatives of
our study are openly released on GitHub and Hugging Face. See
https://nkluge-correa.github.io/Tucano/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidential time-to-event prediction model with well-calibrated
  uncertainty estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Huang, Yucheng Xing, Swapnil Mishra, Thierry Denoeux, Mengling Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-to-event analysis, or Survival analysis, provides valuable insights into
clinical prognosis and treatment recommendations. However, this task is
typically more challenging than other regression tasks due to the censored
observations. Moreover, concerns regarding the reliability of predictions
persist among clinicians, mainly attributed to the absence of confidence
assessment, robustness, and calibration of prediction. To address those
challenges, we introduce an evidential regression model designed especially for
time-to-event prediction tasks, with which the most plausible event time, is
directly quantified by aggregated Gaussian random fuzzy numbers (GRFNs). The
GRFNs are a newly introduced family of random fuzzy subsets of the real line
that generalizes both Gaussian random variables and Gaussian possibility
distributions. Different from conventional methods that construct models based
on strict data distribution, e.g., proportional hazard function, our model only
assumes the event time is encoded in a real line GFRN without any strict
distribution assumption, therefore offering more flexibility in complex data
scenarios. Furthermore, the epistemic and aleatory uncertainty regarding the
event time is quantified within the aggregated GRFN as well. Our model can,
therefore, provide more detailed clinical decision-making guidance with two
more degrees of information. The model is fit by minimizing a generalized
negative log-likelihood function that accounts for data censoring based on
uncertainty evidence reasoning. Experimental results on simulated datasets with
varying data distributions and censoring scenarios, as well as on real-world
datasets across diverse clinical settings and tasks, demonstrate that our model
achieves both accurate and reliable performance, outperforming state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for
  Scalable Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Zmushko, Aleksandr Beznosikov, Martin Takáč, Samuel Horváth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in the number of parameters in large language models, the
process of pre-training and fine-tuning increasingly demands larger volumes of
GPU memory. A significant portion of this memory is typically consumed by the
optimizer state. To overcome this challenge, recent approaches such as low-rank
adaptation (LoRA (Hu et al., 2021)), low-rank gradient projection (GaLore (Zhao
et al., 2024)), and blockwise optimization (BAdam (Luo et al., 2024)) have been
proposed. However, in all these algorithms, the $\textit{effective rank of the
weight updates remains low-rank}$, which can lead to a substantial loss of
information from the gradient. This loss can be critically important,
especially during the pre-training stage. In this paper, we introduce
$\texttt{FRUGAL}$ ($\textbf{F}$ull-$\textbf{R}$ank $\textbf{U}$pdates with
$\textbf{G}$r$\textbf{A}$dient sp$\textbf{L}$itting), a new memory-efficient
optimization framework. $\texttt{FRUGAL}$ leverages gradient splitting to
perform low-dimensional updates using advanced algorithms (such as Adam), while
updates along the remaining directions are executed via state-free methods like
SGD or signSGD (Bernstein et al., 2018). Our framework can be integrated with
various low-rank update selection techniques, including GaLore and BAdam. We
provide theoretical convergence guarantees for our framework when using SGDM
for low-dimensional updates and SGD for state-free updates. Additionally, our
method consistently outperforms concurrent approaches across various fixed
memory budgets, achieving state-of-the-art results in pre-training and
fine-tuning tasks while balancing memory efficiency and performance metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamical-VAE-based Hindsight to Learn the Causal Dynamics of
  Factored-POMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Han, Debabrota Basu, Michael Mangan, Eleni Vasilaki, Aditya Gilra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations of underlying environmental dynamics from partial
observations is a critical challenge in machine learning. In the context of
Partially Observable Markov Decision Processes (POMDPs), state representations
are often inferred from the history of past observations and actions. We
demonstrate that incorporating future information is essential to accurately
capture causal dynamics and enhance state representations. To address this, we
introduce a Dynamical Variational Auto-Encoder (DVAE) designed to learn causal
Markovian dynamics from offline trajectories in a POMDP. Our method employs an
extended hindsight framework that integrates past, current, and multi-step
future information within a factored-POMDP setting. Empirical results reveal
that this approach uncovers the causal graph governing hidden state transitions
more effectively than history-based and typical hindsight-based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suite-IN: Aggregating Motion Features from Apple Suite for Robust
  Inertial Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Sun, Songpengcheng Xia, Junyuan Deng, Jiarui Yang, Zengyuan Lai, Qi Wu, Ling Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of wearable technology, devices like smartphones,
smartwatches, and headphones equipped with IMUs have become essential for
applications such as pedestrian positioning. However, traditional pedestrian
dead reckoning (PDR) methods struggle with diverse motion patterns, while
recent data-driven approaches, though improving accuracy, often lack robustness
due to reliance on a single device.In our work, we attempt to enhance the
positioning performance using the low-cost commodity IMUs embedded in the
wearable devices. We propose a multi-device deep learning framework named
Suite-IN, aggregating motion data from Apple Suite for inertial navigation.
Motion data captured by sensors on different body parts contains both local and
global motion information, making it essential to reduce the negative effects
of localized movements and extract global motion representations from multiple
devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Federated Finetuning of Tiny <span class="highlight-title">Transformer</span>s with
  Resource-Constrained Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kilian Pfeiffer, Mohamed Aboelenien Ahmed, Ramin Khalili, Jörg Henkel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) through Transformer structures
have dominated many machine learning tasks, especially text processing.
However, these models require massive amounts of data for training and induce
high resource requirements, particularly in terms of the large number of
Floating Point Operations (FLOPs) and the high amounts of memory needed. To
fine-tune such a model in a parameter-efficient way, techniques like Adapter or
LoRA have been developed. However, we observe that the application of LoRA,
when used in federated learning (FL), while still being parameter-efficient, is
memory and FLOP inefficient. Based on that observation, we develop a novel
layer finetuning scheme that allows devices in cross-device FL to make use of
pretrained neural networks (NNs) while adhering to given resource constraints.
We show that our presented scheme outperforms the current state of the art when
dealing with homogeneous or heterogeneous computation and memory constraints
and is on par with LoRA regarding limited communication, thereby achieving
significantly higher accuracies in FL training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Criterion Model Aggregation in Federated Learning: Balancing Data
  Quantity and Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haizhou Zhang, Xianjia Yu, Tomi Westerlund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has become one of the key methods for
privacy-preserving collaborative learning, as it enables the transfer of models
without requiring local data exchange. Within the FL framework, an aggregation
algorithm is recognized as one of the most crucial components for ensuring the
efficacy and security of the system. Existing average aggregation algorithms
typically assume that all client-trained data holds equal value or that weights
are based solely on the quantity of data contributed by each client. In
contrast, alternative approaches involve training the model locally after
aggregation to enhance adaptability. However, these approaches fundamentally
ignore the inherent heterogeneity between different clients' data and the
complexity of variations in data at the aggregation stage, which may lead to a
suboptimal global model.
  To address these issues, this study proposes a novel dual-criterion weighted
aggregation algorithm involving the quantity and quality of data from the
client node. Specifically, we quantify the data used for training and perform
multiple rounds of local model inference accuracy evaluation on a specialized
dataset to assess the data quality of each client. These two factors are
utilized as weights within the aggregation process, applied through a
dynamically weighted summation of these two factors. This approach allows the
algorithm to adaptively adjust the weights, ensuring that every client can
contribute to the global model, regardless of their data's size or initial
quality. Our experiments show that the proposed algorithm outperforms several
existing state-of-the-art aggregation approaches on both a general-purpose
open-source dataset, CIFAR-10, and a dataset specific to visual obstacle
avoidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Low-Rank Adaptation with Differential Privacy over Wireless
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqu Kang, Zixin Wang, Hengtao He, Jun Zhang, Shenghui Song, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large pre-trained foundation models (FMs) on distributed edge
devices presents considerable computational and privacy challenges. Federated
fine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative
model training without the need to share raw data. To lessen the computational
burden on resource-limited devices, combining low-rank adaptation (LoRA) with
federated learning enables parameter-efficient fine-tuning. Additionally, the
split FedFT architecture partitions an FM between edge devices and a central
server, reducing the necessity for complete model deployment on individual
devices. However, the risk of privacy eavesdropping attacks in FedFT remains a
concern, particularly in sensitive areas such as healthcare and finance. In
this paper, we propose a split FedFT framework with differential privacy (DP)
over wireless networks, where the inherent wireless channel noise in the uplink
transmission is utilized to achieve DP guarantees without adding an extra
artificial noise. We shall investigate the impact of the wireless noise on
convergence performance of the proposed framework. We will also show that by
updating only one of the low-rank matrices in the split FedFT with DP, the
proposed method can mitigate the noise amplification effect. Simulation results
will demonstrate that the proposed framework achieves higher accuracy under
strict privacy budgets compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, submitted to IEEE ICC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel-based retrieval models for hyperspectral image data optimized
  with Kernel Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zina-Sabrina Duma, Tuomas Sihvonen, Jouni Susiluoto, Otto Lamminpää, Heikki Haario, Satu-Pia Reinikainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel-based statistical methods are efficient, but their performance depends
heavily on the selection of kernel parameters. In literature, the optimization
studies on kernel-based chemometric methods is limited and often reduced to
grid searching. Previously, the authors introduced Kernel Flows (KF) to learn
kernel parameters for Kernel Partial Least-Squares (K-PLS) regression. KF is
easy to implement and helps minimize overfitting. In cases of high collinearity
between spectra and biogeophysical quantities in spectroscopy, simpler methods
like Principal Component Regression (PCR) may be more suitable. In this study,
we propose a new KF-type approach to optimize Kernel Principal Component
Regression (K-PCR) and test it alongside KF-PLS. Both methods are benchmarked
against non-linear regression techniques using two hyperspectral remote sensing
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PatchCTG: Patch Cardiotocography <span class="highlight-title">Transformer</span> for Antepartum Fetal Health
  Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Jaleed Khan, Manu Vatish, Gabriel Davis Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antepartum Cardiotocography (CTG) is vital for fetal health monitoring, but
traditional methods like the Dawes-Redman system are often limited by high
inter-observer variability, leading to inconsistent interpretations and
potential misdiagnoses. This paper introduces PatchCTG, a transformer-based
model specifically designed for CTG analysis, employing patch-based
tokenisation, instance normalisation and channel-independent processing to
capture essential local and global temporal dependencies within CTG signals.
PatchCTG was evaluated on the Oxford Maternity (OXMAT) dataset, comprising over
20,000 CTG traces across diverse clinical outcomes after applying the inclusion
and exclusion criteria. With extensive hyperparameter optimisation, PatchCTG
achieved an AUC of 77%, with specificity of 88% and sensitivity of 57% at
Youden's index threshold, demonstrating adaptability to various clinical needs.
Testing across varying temporal thresholds showed robust predictive
performance, particularly with finetuning on data closer to delivery, achieving
a sensitivity of 52% and specificity of 88% for near-delivery cases. These
findings suggest the potential of PatchCTG to enhance clinical decision-making
in antepartum care by providing a reliable, objective tool for fetal health
assessment. The source code is available at
https://github.com/jaleedkhan/PatchCTG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interaction Asymmetry: A General Principle for Learning Composable
  Abstractions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Brady, Julius von Kügelgen, Sébastien Lachapelle, Simon Buchholz, Thomas Kipf, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning disentangled representations of concepts and re-composing them in
unseen ways is crucial for generalizing to out-of-domain situations. However,
the underlying properties of concepts that enable such disentanglement and
compositional generalization remain poorly understood. In this work, we propose
the principle of interaction asymmetry which states: "Parts of the same concept
have more complex interactions than parts of different concepts". We formalize
this via block diagonality conditions on the $(n+1)$th order derivatives of the
generator mapping concepts to observed data, where different orders of
"complexity" correspond to different $n$. Using this formalism, we prove that
interaction asymmetry enables both disentanglement and compositional
generalization. Our results unify recent theoretical results for learning
concepts of objects, which we show are recovered as special cases with
$n\!=\!0$ or $1$. We provide results for up to $n\!=\!2$, thus extending these
prior works to more flexible generator functions, and conjecture that the same
proof strategies generalize to larger $n$. Practically, our theory suggests
that, to disentangle concepts, an autoencoder should penalize its latent
capacity and the interactions between concepts during decoding. We propose an
implementation of these criteria using a flexible Transformer-based VAE, with a
novel regularizer on the attention weights of the decoder. On synthetic image
datasets consisting of objects, we provide evidence that this model can achieve
comparable object disentanglement to existing models that use more explicit
object-centric priors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Likelihood as a Performance Gauge for Retrieval-Augmented Generation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Liu, Jirui Qi, Paul He, Arianna Bisazza, Mrinmaya Sachan, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work finds that retrieval-augmented generation with large language
models is prone to be influenced by the order of retrieved documents in the
context. However, the lack of in-depth analysis limits the use of this
phenomenon for prompt engineering in practice. In this study, we posit that
likelihoods serve as an effective gauge for language model performance. Through
experiments on two question-answering datasets with a variety of
state-of-the-art language models, we reveal correlations between answer
accuracy and the likelihood of the question at both the corpus level and the
instance level. In addition, we find that question likelihood can also indicate
the position of the task-relevant information in the context. Based on these
findings, we propose two methods that use question likelihood as a gauge for
selecting and constructing prompts that lead to better performance. We
demonstrate their effectiveness with experiments. In addition, our
likelihood-based methods are efficient, as they only need to compute the
likelihood of the input, requiring much fewer language model passes than
heuristic prompt engineering methods that require generating responses. Our
analysis deepens our understanding of how input prompts affect model
performance and provides a promising direction for efficient prompt
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at NAACL 2025. Code is available at
  https://github.com/lyutyuh/poptimizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Album Sequencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Herrmann, Dylan R. Ashley, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented as a late breaking demo in the 25th International Society
  for Music Information Retrieval Conference; 3 pages in main text, 3 figures
  in main text; source code available at
  https://github.com/dylanashley/automatic-album-sequencing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASER: Activation Smoothing and Error Reconstruction for Large Language
  Model Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weibo Zhao, Yubin Shi, Xinyu Lyu, Wanchen Sui, Shen Li, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization stands as a pivotal technique for large language model (LLM)
serving, yet it poses significant challenges particularly in achieving
effective low-bit quantization. The limited numerical mapping makes the
quantized model produce a non-trivial error, bringing out intolerable
performance degration. This paper is anchored in the basic idea of model
compression objectives, and delves into the layer-wise error distribution of
LLMs during post-training quantization. Subsequently, we introduce ASER, an
algorithm consisting of (1) Error Reconstruction: low-rank compensation for
quantization error with LoRA-style matrices constructed by whitening SVD; (2)
Activation Smoothing: outlier extraction to gain smooth activation and better
error compensation. ASER is capable of quantizing typical LLMs to low-bit ones,
particularly preserving accuracy even in W4A8 per-channel setup. Experimental
results show that ASER is competitive among the state-of-the-art quantization
algorithms, showing potential to activation quantization, with minor overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigation with QPHIL: Quantizing Planner for Hierarchical Implicit
  Q-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexi Canesse, Mathieu Petitbois, Ludovic Denoyer, Sylvain Lamprier, Rémy Portelas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (RL) has emerged as a powerful alternative to
imitation learning for behavior modeling in various domains, particularly in
complex navigation tasks. An existing challenge with Offline RL is the
signal-to-noise ratio, i.e. how to mitigate incorrect policy updates due to
errors in value estimates. Towards this, multiple works have demonstrated the
advantage of hierarchical offline RL methods, which decouples high-level path
planning from low-level path following. In this work, we present a novel
hierarchical transformer-based approach leveraging a learned quantizer of the
space. This quantization enables the training of a simpler zone-conditioned
low-level policy and simplifies planning, which is reduced to discrete
autoregressive prediction. Among other benefits, zone-level reasoning in
planning enables explicit trajectory stitching rather than implicit stitching
based on noisy value function estimates. By combining this transformer-based
planner with recent advancements in offline RL, our proposed approach achieves
state-of-the-art results in complex long-distance navigation environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. Code will be released upon acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatially Regularized Graph Attention Autoencoder Framework for
  Detecting Rainfall Extremes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Agarwal, Progyan Das, Udit Bhatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel Graph Attention Autoencoder (GAE) with spatial
regularization to address the challenge of scalable anomaly detection in
spatiotemporal rainfall data across India from 1990 to 2015. Our model
leverages a Graph Attention Network (GAT) to capture spatial dependencies and
temporal dynamics in the data, further enhanced by a spatial regularization
term ensuring geographic coherence. We construct two graph datasets employing
rainfall, pressure, and temperature attributes from the Indian Meteorological
Department and ERA5 Reanalysis on Single Levels, respectively. Our network
operates on graph representations of the data, where nodes represent geographic
locations, and edges, inferred through event synchronization, denote
significant co-occurrences of rainfall events. Through extensive experiments,
we demonstrate that our GAE effectively identifies anomalous rainfall patterns
across the Indian landscape. Our work paves the way for sophisticated
spatiotemporal anomaly detection methodologies in climate science, contributing
to better climate change preparedness and response strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the loss landscape of regularized neural networks via convex
  duality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungyoon Kim, Aaron Mishkin, Mert Pilanci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss several aspects of the loss landscape of regularized neural
networks: the structure of stationary points, connectivity of optimal
solutions, path with nonincreasing loss to arbitrary global optimum, and the
nonuniqueness of optimal solutions, by casting the problem into an equivalent
convex problem and considering its dual. Starting from two-layer neural
networks with scalar output, we first characterize the solution set of the
convex problem using its dual and further characterize all stationary points.
With the characterization, we show that the topology of the global optima goes
through a phase transition as the width of the network changes, and construct
counterexamples where the problem may have a continuum of optimal solutions.
Finally, we show that the solution set characterization and connectivity
results can be extended to different architectures, including two-layer
vector-valued neural networks and parallel three-layer neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence Rate Analysis of LION 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Dong, Huan Li, Zhouchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The LION (evoLved sIgn mOmeNtum) optimizer for deep neural network training
was found by Google via program search, with the simple sign update yet showing
impressive performance in training large scale networks. Although previous
studies have investigated its convergence properties, a comprehensive analysis,
especially the convergence rate, is still desirable. Recognizing that LION can
be regarded as solving a specific constrained problem, this paper focuses on
demonstrating its convergence to the Karush-Kuhn-Tucker (KKT) point at the rate
of $\cal O(\sqrt{d}K^{-1/4})$ measured by gradient $\ell_1$ norm, where $d$ is
the problem dimension and $K$ is the number of iteration steps. Step further,
we remove the constraint and establish that LION converges to the critical
point of the general unconstrained problem at the same rate. This rate not only
delivers the currently optimal dependence on the problem dimension $d$ but also
tightly matches the theoretical lower bound for nonconvex stochastic
optimization algorithms, which is typically measured using the gradient
$\ell_2$ norm, with respect to the number of iterations $K$. Through extensive
experiments, we not only demonstrate that LION achieves lower loss and higher
performance compared to standard SGD, but also empirically confirm that the
gradient $\ell_1/\ell_2$ norm ratio aligns with $\Theta(\sqrt{d})$, thus
proving that our convergence rate matches the theoretical lower bound with
respect to $d$ in the empirical sense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMPERROR: A Flexible Generative Perception Error Model for Probing
  Self-Driving Planners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Hanselmann, Simon Doll, Marius Cordts, Hendrik P. A. Lensch, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To handle the complexities of real-world traffic, learning planners for
self-driving from data is a promising direction. While recent approaches have
shown great progress, they typically assume a setting in which the ground-truth
world state is available as input. However, when deployed, planning needs to be
robust to the long-tail of errors incurred by a noisy perception system, which
is often neglected in evaluation. To address this, previous work has proposed
drawing adversarial samples from a perception error model (PEM) mimicking the
noise characteristics of a target object detector. However, these methods use
simple PEMs that fail to accurately capture all failure modes of detection. In
this paper, we present EMPERROR, a novel transformer-based generative PEM,
apply it to stress-test an imitation learning (IL)-based planner and show that
it imitates modern detectors more faithfully than previous work. Furthermore,
it is able to produce realistic noisy inputs that increase the planner's
collision rate by up to 85%, demonstrating its utility as a valuable tool for a
more complete evaluation of self-driving planners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lasnik.github.io/emperror/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OWLed: Outlier-weighed Layerwise Pruning for Efficient Autonomous
  Driving Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxi Li, Lu Yin, Xilu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) into autonomous driving
systems offers promising enhancements in environmental understanding and
decision-making. However, the substantial computational demands of deploying
LLMs locally on vehicles render this approach unfeasible for real-world
automotive applications. To address this challenge, we introduce OWLed, the
Outlier-Weighed Layerwise Pruning for Efficient Autonomous Driving Framework
that leverages outlier-weighted layerwise sparsity for model compression. Our
method assigns non-uniform sparsity ratios to different layers based on the
distribution of outlier features, significantly reducing the model size without
the need for fine-tuning. To ensure the compressed model adapts well to
autonomous driving tasks, we incorporate driving environment data into both the
calibration and pruning processes. Our empirical studies reveal that the
encoder component is more sensitive to pruning than the LLM, highlighting its
critical role in the system. Experimental results demonstrate that OWLed
outperforms existing methods in perception, action prediction, and language
understanding while substantially lowering computational requirements. These
findings underscore the potential of combining advanced pruning techniques with
LLMs to develop efficient and robust autonomous driving systems capable of
handling complex scenarios. Code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test Where Decisions Matter: Importance-driven Testing for Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Pranger, Hana Chockler, Martin Tappler, Bettina Könighofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many Deep Reinforcement Learning (RL) problems, decisions in a trained
policy vary in significance for the expected safety and performance of the
policy. Since RL policies are very complex, testing efforts should concentrate
on states in which the agent's decisions have the highest impact on the
expected outcome. In this paper, we propose a novel model-based method to
rigorously compute a ranking of state importance across the entire state space.
We then focus our testing efforts on the highest-ranked states. In this paper,
we focus on testing for safety. However, the proposed methods can be easily
adapted to test for performance. In each iteration, our testing framework
computes optimistic and pessimistic safety estimates. These estimates provide
lower and upper bounds on the expected outcomes of the policy execution across
all modeled states in the state space. Our approach divides the state space
into safe and unsafe regions upon convergence, providing clear insights into
the policy's weaknesses. Two important properties characterize our approach.
(1) Optimal Test-Case Selection: At any time in the testing process, our
approach evaluates the policy in the states that are most critical for safety.
(2) Guaranteed Safety: Our approach can provide formal verification guarantees
over the entire state space by sampling only a fraction of the policy. Any
safety properties assured by the pessimistic estimate are formally proven to
hold for the policy. We provide a detailed evaluation of our framework on
several examples, showing that our method discovers unsafe policy behavior with
low testing effort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Do Learning Dynamics Reveal About Generalization in LLM Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katie Kang, Amrith Setlur, Dibya Ghosh, Jacob Steinhardt, Claire Tomlin, Sergey Levine, Aviral Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable capabilities of modern large language models (LLMs),
the mechanisms behind their problem-solving abilities remain elusive. In this
work, we aim to better understand how the learning dynamics of LLM finetuning
shapes downstream generalization. Our analysis focuses on reasoning tasks,
whose problem structure allows us to distinguish between memorization (the
exact replication of reasoning steps from the training data) and performance
(the correctness of the final solution). We find that a model's generalization
behavior can be effectively characterized by a training metric we call
pre-memorization train accuracy: the accuracy of model samples on training
queries before they begin to copy the exact reasoning steps from the training
set. On the dataset level, this metric is able to reliably predict test
accuracy, achieving $R^2$ of around or exceeding 0.9 across various models
(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On
a per-example level, this metric is also indicative of whether individual model
predictions are robust to perturbations in the training query. By connecting a
model's learning behavior to its generalization, pre-memorization train
accuracy can guide targeted improvements to training strategies. We focus on
data curation as an example, and show that prioritizing examples with low
pre-memorization accuracy leads to 1.5-2x improvements in data efficiency
compared to i.i.d. data scaling, and outperforms other standard data curation
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Exploitative Play with Untrusted Type Beliefs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongxin Li, Tinashe Handina, Shaolei Ren, Adam Wierman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of the Bayesian game and learning has a rich history, with
the idea of controlling a single agent in a system composed of multiple agents
with unknown behaviors given a set of types, each specifying a possible
behavior for the other agents. The idea is to plan an agent's own actions with
respect to those types which it believes are most likely to maximize the
payoff. However, the type beliefs are often learned from past actions and
likely to be incorrect. With this perspective in mind, we consider an agent in
a game with type predictions of other components, and investigate the impact of
incorrect beliefs to the agent's payoff. In particular, we formally define a
tradeoff between risk and opportunity by comparing the payoff obtained against
the optimal payoff, which is represented by a gap caused by trusting or
distrusting the learned beliefs. Our main results characterize the tradeoff by
establishing upper and lower bounds on the Pareto front for both normal-form
and stochastic Bayesian games, with numerical results provided.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Structure Learning For Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Zhuofan Zhang, Ziming Wang, Xiang Li, Sitao Luan, Xiaojiang Peng, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve the performance of Graph Neural Networks (GNNs), Graph Structure
Learning (GSL) has been extensively applied to reconstruct or refine original
graph structures, effectively addressing issues like heterophily,
over-squashing, and noisy structures. While GSL is generally thought to improve
GNN performance, it often leads to longer training times and more
hyperparameter tuning. Besides, the distinctions among current GSL methods
remain ambiguous from the perspective of GNN training, and there is a lack of
theoretical analysis to quantify their effectiveness. Recent studies further
suggest that, under fair comparisons with the same hyperparameter tuning, GSL
does not consistently outperform baseline GNNs. This motivates us to ask a
critical question: is GSL really useful for GNNs? To address this question,
this paper makes two key contributions. First, we propose a new GSL framework,
which includes three steps: GSL base (the representation used for GSL)
construction, new structure construction, and view fusion, to better understand
the effectiveness of GSL in GNNs. Second, after graph convolution, we analyze
the differences in mutual information (MI) between node representations derived
from the original topology and those from the newly constructed topology.
Surprisingly, our empirical observations and theoretical analysis show that no
matter which type of graph structure construction methods are used, after
feeding the same GSL bases to the newly constructed graph, there is no MI gain
compared to the original GSL bases. To fairly reassess the effectiveness of
GSL, we conduct ablation experiments and find that it is the pretrained GSL
bases that enhance GNN performance, and in most cases, GSL cannot improve GNN
performance. This finding encourages us to rethink the essential components in
GNNs, such as self-training and structural encoding, in GNN design rather than
GSL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Graph Convolution Always Beneficial For Every Feature? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Xiang Li, Sitao Luan, Xiaojiang Peng, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated strong capabilities in
processing structured data. While traditional GNNs typically treat each feature
dimension equally during graph convolution, we raise an important question: Is
the graph convolution operation equally beneficial for each feature? If not,
the convolution operation on certain feature dimensions can possibly lead to
harmful effects, even worse than the convolution-free models. In prior studies,
to assess the impacts of graph convolution on features, people proposed metrics
based on feature homophily to measure feature consistency with the graph
topology. However, these metrics have shown unsatisfactory alignment with GNN
performance and have not been effectively employed to guide feature selection
in GNNs. To address these limitations, we introduce a novel metric, Topological
Feature Informativeness (TFI), to distinguish between GNN-favored and
GNN-disfavored features, where its effectiveness is validated through both
theoretical analysis and empirical observations. Based on TFI, we propose a
simple yet effective Graph Feature Selection (GFS) method, which processes
GNN-favored and GNN-disfavored features separately, using GNNs and non-GNN
models. Compared to original GNNs, GFS significantly improves the extraction of
useful topological information from each feature with comparable computational
costs. Extensive experiments show that after applying GFS to 8 baseline and
state-of-the-art (SOTA) GNN architectures across 10 datasets, 83.75% of the
GFS-augmented cases show significant performance boosts. Furthermore, our
proposed TFI metric outperforms other feature selection methods. These results
validate the effectiveness of both GFS and TFI. Additionally, we demonstrate
that GFS's improvements are robust to hyperparameter tuning, highlighting its
potential as a universal method for enhancing various GNN architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Audiovisual Deepfake Detection: Techniques, Challenges,
  Human Factors and Perceptual Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning has been successfully applied in diverse fields, and its impact
on deepfake detection is no exception. Deepfakes are fake yet realistic
synthetic content that can be used deceitfully for political impersonation,
phishing, slandering, or spreading misinformation. Despite extensive research
on unimodal deepfake detection, identifying complex deepfakes through joint
analysis of audio and visual streams remains relatively unexplored. To fill
this gap, this survey first provides an overview of audiovisual deepfake
generation techniques, applications, and their consequences, and then provides
a comprehensive review of state-of-the-art methods that combine audio and
visual modalities to enhance detection accuracy, summarizing and critically
analyzing their strengths and limitations. Furthermore, we discuss existing
open source datasets for a deeper understanding, which can contribute to the
research community and provide necessary information to beginners who want to
analyze deep learning-based audiovisual methods for video forensics. By
bridging the gap between unimodal and multimodal approaches, this paper aims to
improve the effectiveness of deepfake detection strategies and guide future
research in cybersecurity and media integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xCG: Explainable Cell Graphs for Survival Prediction in Non-Small Cell
  Lung Cancer <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Sextro, Gabriel Dernbach, Kai Standvoss, Simon Schallenberg, Frederick Klauschen, Klaus-Robert Müller, Maximilian Alber, Lukas Ruff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how deep learning models predict oncology patient risk can
provide critical insights into disease progression, support clinical
decision-making, and pave the way for trustworthy and data-driven precision
medicine. Building on recent advances in the spatial modeling of the tumor
microenvironment using graph neural networks, we present an explainable cell
graph (xCG) approach for survival prediction. We validate our model on a public
cohort of imaging mass cytometry (IMC) data for 416 cases of lung
adenocarcinoma. We explain survival predictions in terms of known phenotypes on
the cell level by computing risk attributions over cell graphs, for which we
propose an efficient grid-based layer-wise relevance propagation (LRP) method.
Our ablation studies highlight the importance of incorporating the cancer stage
and model ensembling to improve the quality of risk estimates. Our xCG method,
together with the IMC data, is made publicly available to support further
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings paper presented at Machine Learning for Health (ML4H)
  symposium 2024, December 15-16, 2024, Vancouver, Canada, 11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Top-$nσ$: Not All Logits Are You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxia Tang, Jianchun Liu, Hongli Xu, Liusheng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) typically employ greedy decoding or
low-temperature sampling for reasoning tasks, reflecting a perceived trade-off
between diversity and accuracy. We challenge this convention by introducing
top-$n\sigma$, a novel sampling method that operates directly on pre-softmax
logits by leveraging a statistical threshold. Our key insight is that logits
naturally separate into a Gaussian-distributed noisy region and a distinct
informative region, enabling efficient token filtering without complex
probability manipulations. Unlike existing methods (e.g., top-$p$, min-$p$)
that inadvertently include more noise tokens at higher temperatures,
top-$n\sigma$ maintains a stable sampling space regardless of temperature
scaling. We also provide a theoretical analysis of top-$n\sigma$ to better
understand its behavior. The extensive experimental results across four
reasoning-focused datasets demonstrate that our method not only outperforms
existing sampling approaches but also surpasses greedy decoding, while
maintaining consistent performance even at high temperatures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel
  Machine Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Zampella, Urtzi Otamendi, Xabier Belaunzaran, Arkaitz Artetxe, Igor G. Olaizola, Giuseppe Longo, Basilio Sierra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scheduling problems pose significant challenges in resource, industry, and
operational management. This paper addresses the Unrelated Parallel Machine
Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent
Reinforcement Learning (MARL) approach. The study introduces the Reinforcement
Learning environment and conducts empirical analyses, comparing MARL with
Single-Agent algorithms. The experiments employ various deep neural network
policies for single- and Multi-Agent approaches. Results demonstrate the
efficacy of the Maskable extension of the Proximal Policy Optimization (PPO)
algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in
Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced
scenarios, Multi-Agent approaches reveal challenges in cooperative learning but
a scalable capacity. This research contributes insights into applying MARL
techniques to scheduling optimization, emphasizing the need for algorithmic
sophistication balanced with scalability for intelligent scheduling solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 4 tables, article submitted to a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CJST: CTC Compressor based Joint Speech and Text Training for
  Decoder-Only ASR <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Junteng Jia, Leda Sari, Jay Mahadeokar, Ozlem Kalinli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CTC compressor can be an effective approach to integrate audio encoders to
decoder-only models, which has gained growing interest for different speech
applications. In this work, we propose a novel CTC compressor based joint
speech and text training (CJST) framework for decoder-only ASR. CJST matches
speech and text modalities from both directions by exploring a simple modality
adaptor and several features of the CTC compressor, including sequence
compression, on-the-fly forced peaky alignment and CTC class embeddings.
Experimental results on the Librispeech and TED-LIUM2 corpora show that the
proposed CJST achieves an effective text injection without the need of duration
handling, leading to the best performance for both in-domain and cross-domain
scenarios. We also provide a comprehensive study on CTC compressor, covering
various compression modes, edge case handling and behavior under both clean and
noisy data conditions, which reveals the most robust setting to use CTC
compressor for decoder-only models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Circuit Complexity Bounds for RoPE-based <span class="highlight-title">Transformer</span> Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
tighter circuit complexity bound for Transformers with $\mathsf{RoPE}$
attention. Our key contribution is that we show that unless $\mathsf{TC}^0 =
\mathsf{NC}^1$, a $\mathsf{RoPE}$-based Transformer with
$\mathrm{poly}(n)$-precision, $O(1)$ layers, hidden dimension $d \leq O(n)$
cannot solve the arithmetic problem or the Boolean formula value problem. This
result significantly demonstrates the fundamental limitation of the
expressivity of the $\mathsf{RoPE}$-based Transformer architecture, although it
achieves giant empirical success. Our theoretical framework not only
establishes tighter complexity bounds but also may instruct further work on the
$\mathsf{RoPE}$-based Transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegQC: a segmentation network-based framework for multi-metric
  segmentation quality control and segmentation error detection in volumetric
  medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bella Specktor-Fadida, Liat Ben-Sira, Dafna Ben-Bashat, Leo Joskowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality control of structures segmentation in volumetric medical images is
important for identifying segmentation errors in clinical practice and for
facilitating model development. This paper introduces SegQC, a novel framework
for segmentation quality estimation and segmentation error detection. SegQC
computes an estimate measure of the quality of a segmentation in volumetric
scans and in their individual slices and identifies possible segmentation error
regions within a slice. The key components include: 1. SegQC-Net, a deep
network that inputs a scan and its segmentation mask and outputs segmentation
error probabilities for each voxel in the scan; 2. three new segmentation
quality metrics, two overlap metrics and a structure size metric, computed from
the segmentation error probabilities; 3. a new method for detecting possible
segmentation errors in scan slices computed from the segmentation error
probabilities. We introduce a new evaluation scheme to measure segmentation
error discrepancies based on an expert radiologist corrections of automatically
produced segmentations that yields smaller observer variability and is closer
to actual segmentation errors. We demonstrate SegQC on three fetal structures
in 198 fetal MRI scans: fetal brain, fetal body and the placenta. To assess the
benefits of SegQC, we compare it to the unsupervised Test Time Augmentation
(TTA)-based quality estimation. Our studies indicate that SegQC outperforms
TTA-based quality estimation in terms of Pearson correlation and MAE for fetal
body and fetal brain structures segmentation. Our segmentation error detection
method achieved recall and precision rates of 0.77 and 0.48 for fetal body, and
0.74 and 0.55 for fetal brain segmentation error detection respectively. SegQC
enhances segmentation metrics estimation for whole scans and individual slices,
as well as provides error regions detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision Feedback In-Context Symbol Detection over Block-Fading Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Fan, Jing Yang, Cong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Transformers, through in-context learning (ICL), have
demonstrated exceptional capabilities to adapt to new tasks using example
prompts \textit{without model update}. Transformer-based wireless receivers,
where prompts consist of the pilot data in the form of transmitted and received
signal pairs, have shown high estimation accuracy when pilot data are abundant.
However, pilot information is often costly and limited in practice. In this
work, we propose the \underline{DE}cision \underline{F}eedback
\underline{IN}-Cont\underline{E}xt \underline{D}etection (DEFINED) solution as
a new wireless receiver design, which bypasses channel estimation and directly
performs symbol detection using the (sometimes extremely) limited pilot data.
The key innovation in DEFINED is the proposed decision feedback mechanism in
ICL, where we sequentially incorporate the detected symbols into the prompts to
improve the detections for subsequent symbols. Extensive experiments across a
broad range of wireless communication settings demonstrate that DEFINED
achieves significant performance improvements, in some cases only needing a
single pilot pair.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy Controllable Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motoki Omura, Yasuhiro Fujita, Toshiki Kataoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming the Curse of Dimensionality in Reinforcement Learning Through
  Approximate Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenbei Lu, Laixi Shi, Zaiwei Chen, Chenye Wu, Adam Wierman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) algorithms are known to suffer from the curse of
dimensionality, which refers to the fact that large-scale problems often lead
to exponentially high sample complexity. A common solution is to use deep
neural networks for function approximation; however, such approaches typically
lack theoretical guarantees. To provably address the curse of dimensionality,
we observe that many real-world problems exhibit task-specific model structures
that, when properly leveraged, can improve the sample efficiency of RL.
Building on this insight, we propose overcoming the curse of dimensionality by
approximately factorizing the original Markov decision processes (MDPs) into
smaller, independently evolving MDPs. This factorization enables the
development of sample-efficient RL algorithms in both model-based and
model-free settings, with the latter involving a variant of variance-reduced
Q-learning. We provide improved sample complexity guarantees for both proposed
algorithms. Notably, by leveraging model structure through the approximate
factorization of the MDP, the dependence of sample complexity on the size of
the state-action space can be exponentially reduced. Numerically, we
demonstrate the practicality of our proposed methods through experiments on
both synthetic MDP tasks and a wind farm-equipped storage control problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Tabular Data towards Better One-Class Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Ye, Zhaorui Tan, Yijie Hu, Xi Yang, Guangliang Cheng, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular anomaly detection under the one-class classification setting poses a
significant challenge, as it involves accurately conceptualizing "normal"
derived exclusively from a single category to discern anomalies from normal
data variations. Capturing the intrinsic correlation among attributes within
normal samples presents one promising method for learning the concept. To do
so, the most recent effort relies on a learnable mask strategy with a
reconstruction task. However, this wisdom may suffer from the risk of producing
uniform masks, i.e., essentially nothing is masked, leading to less effective
correlation learning. To address this issue, we presume that attributes related
to others in normal samples can be divided into two non-overlapping and
correlated subsets, defined as CorrSets, to capture the intrinsic correlation
effectively. Accordingly, we introduce an innovative method that disentangles
CorrSets from normal tabular data. To our knowledge, this is a pioneering
effort to apply the concept of disentanglement for one-class anomaly detection
on tabular data. Extensive experiments on 20 tabular datasets show that our
method substantially outperforms the state-of-the-art methods and leads to an
average performance improvement of 6.1% on AUC-PR and 2.1% on AUC-ROC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Test-Time Adaptation for Inverse Consistent
  Diffeomorphic Lung Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad F. A. Chaudhary, Stephanie M. Aguilera, Arie Nakhmani, Joseph M. Reinhardt, Surya P. Bhatt, Sandeep Bodduluri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffeomorphic deformable image registration ensures smooth invertible
transformations across inspiratory and expiratory chest CT scans. Yet, in
practice, deep learning-based diffeomorphic methods struggle to capture large
deformations between inspiratory and expiratory volumes, and therefore lack
inverse consistency. Existing methods also fail to account for model
uncertainty, which can be useful for improving performance. We propose an
uncertainty-aware test-time adaptation framework for inverse consistent
diffeomorphic lung registration. Our method uses Monte Carlo (MC) dropout to
estimate spatial uncertainty that is used to improve model performance. We
train and evaluate our method for inspiratory-to-expiratory CT registration on
a large cohort of 675 subjects from the COPDGene study, achieving a higher Dice
similarity coefficient (DSC) between the lung boundaries (0.966) compared to
both VoxelMorph (0.953) and TransMorph (0.953). Our method demonstrates
consistent improvements in the inverse registration direction as well with an
overall DSC of 0.966, higher than VoxelMorph (0.958) and TransMorph (0.956).
Paired t-tests indicate statistically significant improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for
  Black-box Multi-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiejin Chen, Kaishen Wang, Hua Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jailbreaking methods, which induce Multi-modal Large Language Models (MLLMs)
to output harmful responses, raise significant safety concerns. Among these
methods, gradient-based approaches, which use gradients to generate malicious
prompts, have been widely studied due to their high success rates in white-box
settings, where full access to the model is available. However, these methods
have notable limitations: they require white-box access, which is not always
feasible, and involve high memory usage. To address scenarios where white-box
access is unavailable, attackers often resort to transfer attacks. In transfer
attacks, malicious inputs generated using white-box models are applied to
black-box models, but this typically results in reduced attack performance. To
overcome these challenges, we propose Zer0-Jack, a method that bypasses the
need for white-box access by leveraging zeroth-order optimization. We propose
patch coordinate descent to efficiently generate malicious image inputs to
directly attack black-box MLLMs, which significantly reduces memory usage
further. Through extensive experiments, Zer0-Jack achieves a high attack
success rate across various models, surpassing previous transfer-based methods
and performing comparably with existing white-box jailbreak techniques.
Notably, Zer0-Jack achieves a 95\% attack success rate on MiniGPT-4 with the
Harmful Behaviors Multi-modal Dataset on a black-box setting, demonstrating its
effectiveness. Additionally, we show that Zer0-Jack can directly attack
commercial MLLMs such as GPT-4o. Codes are provided in the supplement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Neurips SafeGenAi Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exogenous Randomness Empowering Random Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxing Mei, Yingying Fan, Jinchi Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We offer theoretical and empirical insights into the impact of exogenous
randomness on the effectiveness of random forests with tree-building rules
independent of training data. We formally introduce the concept of exogenous
randomness and identify two types of commonly existing randomness: Type I from
feature subsampling, and Type II from tie-breaking in tree-building processes.
We develop non-asymptotic expansions for the mean squared error (MSE) for both
individual trees and forests and establish sufficient and necessary conditions
for their consistency. In the special example of the linear regression model
with independent features, our MSE expansions are more explicit, providing more
understanding of the random forests' mechanisms. It also allows us to derive an
upper bound on the MSE with explicit consistency rates for trees and forests.
Guided by our theoretical findings, we conduct simulations to further explore
how exogenous randomness enhances random forest performance. Our findings
unveil that feature subsampling reduces both the bias and variance of random
forests compared to individual trees, serving as an adaptive mechanism to
balance bias and variance. Furthermore, our results reveal an intriguing
phenomenon: the presence of noise features can act as a "blessing" in enhancing
the performance of random forests thanks to feature subsampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>103 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling the Gradient Descent Dynamics of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingqing Song, Boran Han, Shuai Zhang, Jie Ding, Mingyi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the Transformer architecture has achieved remarkable success across
various domains, a thorough theoretical foundation explaining its optimization
dynamics is yet to be fully developed. In this study, we aim to bridge this
understanding gap by answering the following two core questions: (1) Which
types of Transformer architectures allow Gradient Descent (GD) to achieve
guaranteed convergence? and (2) Under what initial conditions and architectural
specifics does the Transformer achieve rapid convergence during training? By
analyzing the loss landscape of a single Transformer layer using Softmax and
Gaussian attention kernels, our work provides concrete answers to these
questions. Our findings demonstrate that, with appropriate weight
initialization, GD can train a Transformer model (with either kernel type) to
achieve a global optimal solution, especially when the input embedding
dimension is large. Nonetheless, certain scenarios highlight potential
pitfalls: training a Transformer using the Softmax attention kernel may
sometimes lead to suboptimal local solutions. In contrast, the Gaussian
attention kernel exhibits a much favorable behavior. Our empirical study
further validate the theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accident Impact Prediction based on a deep convolutional and recurrent
  neural network model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pouyan Sajadi, Mahya Qorbani, Sobhan Moosavi, Erfan Hassannayebi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic accidents pose a significant threat to public safety, resulting in
numerous fatalities, injuries, and a substantial economic burden each year. The
development of predictive models capable of real-time forecasting of
post-accident impact using readily available data can play a crucial role in
preventing adverse outcomes and enhancing overall safety. However, existing
accident predictive models encounter two main challenges: first, reliance on
either costly or non-real-time data, and second the absence of a comprehensive
metric to measure post-accident impact accurately. To address these
limitations, this study proposes a deep neural network model known as the
cascade model. It leverages readily available real-world data from Los Angeles
County to predict post-accident impacts. The model consists of two components:
Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN). The LSTM
model captures temporal patterns, while the CNN extracts patterns from the
sparse accident dataset. Furthermore, an external traffic congestion dataset is
incorporated to derive a new feature called the "accident impact" factor, which
quantifies the influence of an accident on surrounding traffic flow. Extensive
experiments were conducted to demonstrate the effectiveness of the proposed
hybrid machine learning method in predicting the post-accident impact compared
to state-of-the-art baselines. The results reveal a higher precision in
predicting minimal impacts (i.e., cases with no reported accidents) and a
higher recall in predicting more significant impacts (i.e., cases with reported
accidents).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Stealing for Any Low-Rank Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Liu, Ankur Moitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model stealing, where a learner tries to recover an unknown model via
carefully chosen queries, is a critical problem in machine learning, as it
threatens the security of proprietary models and the privacy of data they are
trained on. In recent years, there has been particular interest in stealing
large language models (LLMs). In this paper, we aim to build a theoretical
understanding of stealing language models by studying a simple and
mathematically tractable setting. We study model stealing for Hidden Markov
Models (HMMs), and more generally low-rank language models.
  We assume that the learner works in the conditional query model, introduced
by Kakade, Krishnamurthy, Mahajan and Zhang. Our main result is an efficient
algorithm in the conditional query model, for learning any low-rank
distribution. In other words, our algorithm succeeds at stealing any language
model whose output distribution is low-rank. This improves upon the previous
result by Kakade, Krishnamurthy, Mahajan and Zhang, which also requires the
unknown distribution to have high "fidelity", a property that holds only in
restricted cases. There are two key insights behind our algorithm: First, we
represent the conditional distributions at each timestep by constructing
barycentric spanners among a collection of vectors of exponentially large
dimension. Second, for sampling from our representation, we iteratively solve a
sequence of convex optimization problems that involve projection in relative
entropy to prevent compounding of errors over the length of the sequence. This
is an interesting example where, at least theoretically, allowing a machine
learning model to solve more complex problems at inference time can lead to
drastic improvements in its performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Virtual Reality Teleoperation of an Upper-body Humanoid with
  Modified Task Jacobians and Relaxed Barrier Functions for Self-Collision
  Avoidance <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Jens Jorgensen, Ravi Bhadeshiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for retartgeting off-the-shelf Virtual Reality (VR)
trackers to effectively teleoperate an upper-body humanoid while ensuring
self-collision-free motions. Key to the effectiveness was the proper assignment
of trackers to joint sets via modified task Jacobians and relaxed barrier
functions for self-collision avoidance. The approach was validated on
Apptronik's Astro hardware by demonstrating manipulation capabilities on a
table-top environment with pick-and-place box packing and a two-handed box pick
up and handover task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>XR & Robotics Workshop, IROS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecEncoder: Logs are All You Need in Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Fatih Bulut, Yingqi Liu, Naveed Ahmad, Maximilian Turner, Sami Ait Ouahmane, Cameron Andrews, Lloyd Greenwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large and Small Language Models (LMs) are typically pretrained using
extensive volumes of text, which are sourced from publicly accessible platforms
such as Wikipedia, Book Corpus, or through web scraping. These models, due to
their exposure to a wide range of language data, exhibit impressive
generalization capabilities and can perform a multitude of tasks
simultaneously. However, they often fall short when it comes to domain-specific
tasks due to their broad training data. This paper introduces SecEncoder, a
specialized small language model that is pretrained using security logs.
SecEncoder is designed to address the domain-specific limitations of general
LMs by focusing on the unique language and patterns found in security logs.
Experimental results indicate that SecEncoder outperforms other LMs, such as
BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)
models, which are pretrained mainly on natural language, across various tasks.
Furthermore, although SecEncoder is primarily pretrained on log data, it
outperforms models pretrained on natural language for a range of tasks beyond
log analysis, such as incident prioritization and threat intelligence document
retrieval. This suggests that domain specific pretraining with logs can
significantly enhance the performance of LMs in security. These findings pave
the way for future research into security-specific LMs and their potential
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative and Federated Black-box Optimization: A Bayesian
  Optimization Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raed Al Kontar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on collaborative and federated black-box optimization (BBOpt), where
agents optimize their heterogeneous black-box functions through collaborative
sequential experimentation. From a Bayesian optimization perspective, we
address the fundamental challenges of distributed experimentation,
heterogeneity, and privacy within BBOpt, and propose three unifying frameworks
to tackle these issues: (i) a global framework where experiments are centrally
coordinated, (ii) a local framework that allows agents to make decisions based
on minimal shared information, and (iii) a predictive framework that enhances
local surrogates through collaboration to improve decision-making. We
categorize existing methods within these frameworks and highlight key open
questions to unlock the full potential of federated BBOpt. Our overarching goal
is to shift federated learning from its predominantly descriptive/predictive
paradigm to a prescriptive one, particularly in the context of BBOpt - an
inherently sequential decision-making problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Deep Learning Approach for Real-time Lane-based Arrival Curve
  Reconstruction at Intersection using License Plate Recognition Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang He, Chengchuan An, Jiawei Lu, Yao-Jan Wu, Zhenbo Lu, Jingxin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The acquisition of real-time and accurate traffic arrival information is of
vital importance for proactive traffic control systems, especially in partially
connected vehicle environments. License plate recognition (LPR) data that
record both vehicle departures and identities are proven to be desirable in
reconstructing lane-based arrival curves in previous works. Existing LPR
databased methods are predominantly designed for reconstructing historical
arrival curves. For real-time reconstruction of multi-lane urban roads, it is
pivotal to determine the lane choice of real-time link-based arrivals, which
has not been exploited in previous studies. In this study, we propose a
Bayesian deep learning approach for real-time lane-based arrival curve
reconstruction, in which the lane choice patterns and uncertainties of
link-based arrivals are both characterized. Specifically, the learning process
is designed to effectively capture the relationship between partially observed
link-based arrivals and lane-based arrivals, which can be physically
interpreted as lane choice proportion. Moreover, the lane choice uncertainties
are characterized using Bayesian parameter inference techniques, minimizing
arrival curve reconstruction uncertainties, especially in low LPR data matching
rate conditions. Real-world experiment results conducted in multiple matching
rate scenarios demonstrate the superiority and necessity of lane choice
modeling in reconstructing arrival curves.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by T-ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Offline Reinforcement Learning for Non-Markovian Decision
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiquan Huang, Yingbin Liang, Jing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributionally robust offline reinforcement learning (RL) aims to find a
policy that performs the best under the worst environment within an uncertainty
set using an offline dataset collected from a nominal model. While recent
advances in robust RL focus on Markov decision processes (MDPs), robust
non-Markovian RL is limited to planning problem where the transitions in the
uncertainty set are known. In this paper, we study the learning problem of
robust offline non-Markovian RL. Specifically, when the nominal model admits a
low-rank structure, we propose a new algorithm, featuring a novel dataset
distillation and a lower confidence bound (LCB) design for robust values under
different types of the uncertainty set. We also derive new dual forms for these
robust values in non-Markovian RL, making our algorithm more amenable to
practical implementation. By further introducing a novel type-I concentrability
coefficient tailored for offline low-rank non-Markovian decision processes, we
prove that our algorithm can find an $\epsilon$-optimal robust policy using
$O(1/\epsilon^2)$ offline samples. Moreover, we extend our algorithm to the
case when the nominal model does not have specific structure. With a new
type-II concentrability coefficient, the extended algorithm also enjoys
polynomial sample efficiency under all different types of the uncertainty set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FM-TS: Flow Matching for Time Series Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Hu, Xiao Wang, Lirong Wu, Huatian Zhang, Stan Z. Li, Sheng Wang, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series generation has emerged as an essential tool for analyzing
temporal data across numerous fields. While diffusion models have recently
gained significant attention in generating high-quality time series, they tend
to be computationally demanding and reliant on complex stochastic processes. To
address these limitations, we introduce FM-TS, a rectified Flow Matching-based
framework for Time Series generation, which simplifies the time series
generation process by directly optimizing continuous trajectories. This
approach avoids the need for iterative sampling or complex noise schedules
typically required in diffusion-based models. FM-TS is more efficient in terms
of training and inference. Moreover, FM-TS is highly adaptive, supporting both
conditional and unconditional time series generation. Notably, through our
novel inference design, the model trained in an unconditional setting can
seamlessly generalize to conditional tasks without the need for retraining.
Extensive benchmarking across both settings demonstrates that FM-TS
consistently delivers superior performance compared to existing approaches
while being more efficient in terms of training and inference. For instance, in
terms of discriminative score, FM-TS achieves 0.005, 0.019, 0.011, 0.005,
0.053, and 0.106 on the Sines, Stocks, ETTh, MuJoCo, Energy, and fMRI
unconditional time series datasets, respectively, significantly outperforming
the second-best method which achieves 0.006, 0.067, 0.061, 0.008, 0.122, and
0.167 on the same datasets. We have achieved superior performance in solar
forecasting and MuJoCo imputation tasks, significantly enhanced by our
innovative $t$ power sampling method. The code is available at
https://github.com/UNITES-Lab/FMTS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaS&S: a One-Shot Supernet Approach for Automatic Embedding Size Search
  in Deep Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Wei, Yuekui Yang, Yang Zhang, Haiyang Wu, Meixi Liu, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning Recommendation Model(DLRM)s utilize the embedding layer to
represent various categorical features. Traditional DLRMs adopt unified
embedding size for all features, leading to suboptimal performance and
redundant parameters. Thus, lots of Automatic Embedding size Search (AES) works
focus on obtaining mixed embedding sizes with strong model performance.
However, previous AES works can hardly address several challenges together: (1)
The search results of embedding sizes are unstable; (2) Recommendation effect
with AES results is unsatisfactory; (3) Memory cost of embeddings is
uncontrollable. To address these challenges, we propose a novel one-shot AES
framework called AdaS&S, in which a supernet encompassing various candidate
embeddings is built and AES is performed as searching network architectures
within it. Our framework contains two main stages: In the first stage, we
decouple training parameters from searching embedding sizes, and propose the
Adaptive Sampling method to yield a well-trained supernet, which further helps
to produce stable AES results. In the second stage, to obtain embedding sizes
that benefits the model effect, we design a reinforcement learning search
process which utilizes the supernet trained previously. Meanwhile, to adapt
searching to specific resource constraint, we introduce the resource
competition penalty to balance the model effectiveness and memory cost of
embeddings. We conduct extensive experiments on public datasets to show the
superiority of AdaS&S. Our method could improve AUC by about 0.3% while saving
about 20% of model parameters. Empirical analysis also shows that the stability
of searching results in AdaS&S significantly exceeds other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Automatic Real-time Motion Tracking Method for Magnetic
  Resonance Imaging-guided Radiotherapy: Leveraging the Enhanced
  Tracking-Learning-Detection Framework with Automatic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengqi Chen, Zilin Wang, Jianrong Dai, Shirui Qin, Ying Cao, Ruiao Zhao, Jiayun Chen, Guohua Wu, Yuan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Ensuring the precision in motion tracking for MRI-guided
Radiotherapy (MRIgRT) is crucial for the delivery of effective treatments. This
study refined the motion tracking accuracy in MRIgRT through the innovation of
an automatic real-time tracking method, leveraging an enhanced
Tracking-Learning-Detection (ETLD) framework coupled with automatic
segmentation. Methods: We developed a novel MRIgRT motion tracking method by
integrating two primary methods: the ETLD framework and an improved Chan-Vese
model (ICV), named ETLD+ICV. The TLD framework was upgraded to suit real-time
cine MRI, including advanced image preprocessing, no-reference image quality
assessment, an enhanced median-flow tracker, and a refined detector with
dynamic search region adjustments. Additionally, ICV was combined for precise
coverage of the target volume, which refined the segmented region frame by
frame using tracking results, with key parameters optimized. Tested on 3.5D MRI
scans from 10 patients with liver metastases, our method ensures precise
tracking and accurate segmentation vital for MRIgRT. Results: An evaluation of
106,000 frames across 77 treatment fractions revealed sub-millimeter tracking
errors of less than 0.8mm, with over 99% precision and 98% recall for all
subjects, underscoring the robustness and efficacy of the ETLD. Moreover, the
ETLD+ICV yielded a dice global score of more than 82% for all subjects,
demonstrating the proposed method's extensibility and precise target volume
coverage. Conclusions: This study successfully developed an automatic real-time
motion tracking method for MRIgRT that markedly surpasses current methods. The
novel method not only delivers exceptional precision in tracking and
segmentation but also demonstrates enhanced adaptability to clinical demands,
positioning it as an indispensable asset in the quest to augment the efficacy
of radiotherapy treatments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAUREL: Learned Augmented Residual Layer <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Menghani, Ravi Kumar, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the core pillars of efficient deep learning methods is architectural
improvements such as the residual/skip connection, which has led to
significantly better model convergence and quality. Since then the residual
connection has become ubiquitous in not just convolutional neural networks but
also transformer-based architectures, the backbone of LLMs.
  In this paper we introduce \emph{Learned Augmented Residual Layer} (LAuReL)
-- a novel generalization of the canonical residual connection -- with the goal
to be an in-situ replacement of the latter while outperforming on both model
quality and footprint metrics. Our experiments show that using \laurel can help
boost performance for both vision and language models. For example, on the
ResNet-50, ImageNet 1K task, it achieves $60\%$ of the gains from adding an
extra layer, while only adding $0.003\%$ more parameters, and matches it while
adding $2.6\times$ fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2nd Efficient Systems for Foundation Models Workshop
  at the International Conference on Machine Learning (ICML) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADMM for Structured Fractional Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganzhao Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a class of structured fractional minimization problems, where the
numerator includes a differentiable function, a simple nonconvex nonsmooth
function, a concave nonsmooth function, and a convex nonsmooth function
composed with a linear operator, while the denominator is a continuous function
that is either weakly convex or has a weakly convex square root. These problems
are widespread and span numerous essential applications in machine learning and
data science. Existing methods are mainly based on subgradient methods and
smoothing proximal gradient methods, which may suffer from slow convergence and
numerical stability issues. In this paper, we introduce {\sf FADMM}, the first
Alternating Direction Method of Multipliers tailored for this class of
problems. {\sf FADMM} decouples the original problem into linearized proximal
subproblems, featuring two variants: one using Dinkelbach's parametric method
({\sf FADMM-D}) and the other using the quadratic transform method ({\sf
FADMM-Q}). By introducing a novel Lyapunov function, we establish that {\sf
FADMM} converges to $\epsilon$-approximate critical points of the problem
within an oracle complexity of $\mathcal{O}(1/\epsilon^{3})$. Our experiments
on synthetic and real-world data for sparse Fisher discriminant analysis,
robust Sharpe ratio minimization, and robust sparse recovery demonstrate the
effectiveness of our approach.
  Keywords: Fractional Minimization, Nonconvex Optimization, Proximal
Linearized ADMM, Nonsmooth Optimization, Convergence Analysis
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Knowledge Distillation Using Partial Information
  Decomposition <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasan Dissanayake, Faisal Hamman, Barproda Halder, Ilia Sucholutsky, Qiuyi Zhang, Sanghamitra Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation provides an effective method for deploying complex
machine learning models in resource-constrained environments. It typically
involves training a smaller student model to emulate either the probabilistic
outputs or the internal feature representations of a larger teacher model. By
doing so, the student model often achieves substantially better performance on
a downstream task compared to when it is trained independently. Nevertheless,
the teacher's internal representations can also encode noise or additional
information that may not be relevant to the downstream task. This observation
motivates our primary question: What are the information-theoretic limits of
knowledge transfer? To this end, we leverage a body of work in information
theory called Partial Information Decomposition (PID) to quantify the
distillable and distilled knowledge of a teacher's representation corresponding
to a given student and a downstream task. Moreover, we demonstrate that this
metric can be practically used in distillation to address challenges caused by
the complexity gap between the teacher and the student representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Machine Learning and Compression Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Link Prediction with Fuzzy Graph Attention Networks and
  Dynamic Negative Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction is crucial for understanding complex networks but traditional
Graph Neural Networks (GNNs) often rely on random negative sampling, leading to
suboptimal performance. This paper introduces Fuzzy Graph Attention Networks
(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative
sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)
systematically selects high-quality negative edges based on fuzzy similarities,
improving training efficiency. FGAT layer incorporates fuzzy rough set
principles, enabling robust and discriminative node representations.
Experiments on two research collaboration networks demonstrate FGAT's superior
link prediction accuracy, outperforming state-of-the-art baselines by
leveraging the power of fuzzy rough sets for effective negative sampling and
node feature learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQC-Based Reinforcement Learning with Data Re-uploading: Performance and
  Trainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Coelho, André Sequeira, Luís Paulo Santos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) consists of designing agents that make
intelligent decisions without human supervision. When used alongside function
approximators such as Neural Networks (NNs), RL is capable of solving extremely
complex problems. Deep Q-Learning, a RL algorithm that uses Deep NNs, achieved
super-human performance in some specific tasks. Nonetheless, it is also
possible to use Variational Quantum Circuits (VQCs) as function approximators
in RL algorithms. This work empirically studies the performance and
trainability of such VQC-based Deep Q-Learning models in classic control
benchmark environments. More specifically, we research how data re-uploading
affects both these metrics. We show that the magnitude and the variance of the
gradients of these models remain substantial throughout training due to the
moving targets of Deep Q-Learning. Moreover, we empirically show that
increasing the number of qubits does not lead to an exponential vanishing
behavior of the magnitude and variance of the gradients for a PQC approximating
a 2-design, unlike what was expected due to the Barren Plateau Phenomenon. This
hints at the possibility of VQCs being specially adequate for being used as
function approximators in such a context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Utilization of Unique Node Identifiers in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Bechler-Speicher, Moshe Eliasof, Carola-Bibiane Schönlieb, Ran Gilad-Bachrach, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks have inherent representational limitations due to their
message-passing structure. Recent work has suggested that these limitations can
be overcome by using unique node identifiers (UIDs). Here we argue that despite
the advantages of UIDs, one of their disadvantages is that they lose the
desirable property of permutation-equivariance. We thus propose to focus on UID
models that are permutation-equivariant, and present theoretical arguments for
their advantages. Motivated by this, we propose a method to regularize UID
models towards permutation equivariance, via a contrastive loss. We empirically
demonstrate that our approach improves generalization and extrapolation
abilities while providing faster training convergence. On the recent BREC
expressiveness benchmark, our proposed method achieves state-of-the-art
performance compared to other random-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Models for the Electric Power Grid 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik F. Hamann, Thomas Brunschwiler, Blazhe Gjorgiev, Leonardo S. A. Martins, Alban Puech, Anna Varbella, Jonas Weiss, Juan Bernabe-Moreno, Alexandre Blondin Massé, Seong Choi, Ian Foster, Bri-Mathias Hodge, Rishabh Jain, Kibaek Kim, Vincent Mai, François Mirallès, Martin De Montigny, Octavio Ramos-Leaños, Hussein Suprême, Le Xie, El-Nasser S. Youssef, Arnaud Zinflou, Alexander J. Belyi, Ricardo J. Bessa, Bishnu Prasad Bhattarai, Johannes Schmude, Stanislav Sobolevsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) currently dominate news headlines. They employ
advanced deep learning architectures to extract structural information
autonomously from vast datasets through self-supervision. The resulting rich
representations of complex systems and dynamics can be applied to many
downstream applications. Therefore, FMs can find uses in electric power grids,
challenged by the energy transition and climate change. In this paper, we call
for the development of, and state why we believe in, the potential of FMs for
electric grids. We highlight their strengths and weaknesses amidst the
challenges of a changing grid. We argue that an FM learning from diverse grid
data and topologies could unlock transformative capabilities, pioneering a new
approach in leveraging AI to redefine how we manage complexity and uncertainty
in the electric grid. Finally, we discuss a power grid FM concept, namely
GridFM, based on graph neural networks and show how different downstream tasks
benefit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Major equal contributors: H.F.H., T.B., B.G., L.S.A.M., A.P., A.V.,
  J.W.; Significant equal contributors: J.B., A.B.M., S.C., I.F., B.H., R.J.,
  K.K., V.M., F.M., M.D.M., O.R., H.S., L.X., E.S.Y., A.Z.; Other equal
  contributors: A.J.B., R.J.B., B.P.B., J.S., S.S; Lead contact: H.F.H</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LE-PDE++: Mamba for accelerating PDEs Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoming Liang, Zhaoyang Mu, Qi liu, Ruipeng Li, Mingming Ge, Dixia Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial Differential Equations are foundational in modeling science and
natural systems such as fluid dynamics and weather forecasting. The Latent
Evolution of PDEs method is designed to address the computational intensity of
classical and deep learning-based PDE solvers by proposing a scalable and
efficient alternative. To enhance the efficiency and accuracy of LE-PDE, we
incorporate the Mamba model, an advanced machine learning model known for its
predictive efficiency and robustness in handling complex dynamic systems with a
progressive learning strategy. The LE-PDE was tested on several benchmark
problems. The method demonstrated a marked reduction in computational time
compared to traditional solvers and standalone deep learning models while
maintaining high accuracy in predicting system behavior over time. Our method
doubles the inference speed compared to the LE-PDE while retaining the same
level of parameter efficiency, making it well-suited for scenarios requiring
long-term predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and
  Tabnet with SMOTEENN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Yu, Yixin Jin, Qianwen Xing, Ye Zhang, Shaobo Guo, Shuchen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bank credit risk is a significant challenge in modern financial transactions,
and the ability to identify qualified credit card holders among a large number
of applicants is crucial for the profitability of a bank'sbank's credit card
business. In the past, screening applicants'applicants' conditions often
required a significant amount of manual labor, which was time-consuming and
labor-intensive. Although the accuracy and reliability of previously used ML
models have been continuously improving, the pursuit of more reliable and
powerful AI intelligent models is undoubtedly the unremitting pursuit by major
banks in the financial industry. In this study, we used a dataset of over
40,000 records provided by a commercial bank as the research object. We
compared various dimensionality reduction techniques such as PCA and T-SNE for
preprocessing high-dimensional datasets and performed in-depth adaptation and
tuning of distributed models such as LightGBM and XGBoost, as well as deep
models like Tabnet. After a series of research and processing, we obtained
excellent research results by combining SMOTEENN with these techniques. The
experiments demonstrated that LightGBM combined with PCA and SMOTEENN
techniques can assist banks in accurately predicting potential high-quality
customers, showing relatively outstanding performance compared to other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pagess on IEEE ICPICS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advanced Payment Security System:XGBoost, LightGBM and SMOTE Integrated 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zheng, Chang Yu, Jin Cao, Yongshun Xu, Qianwen Xing, Yinxin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of various online and mobile payment systems, transaction fraud
has become a significant threat to financial security. This study explores the
application of advanced machine learning models, specifically based on XGBoost
and LightGBM, for developing a more accurate and robust Payment Security
Protection Model. To enhance data reliability, we meticulously processed the
data sources and applied SMOTE (Synthetic Minority Over-sampling Technique) to
address class imbalance and improve data representation. By selecting highly
correlated features, we aimed to strengthen the training process and boost
model performance. We conducted thorough performance evaluations of our
proposed models, comparing them against traditional methods including Random
Forest, Neural Network, and Logistic Regression. Using metrics such as
Precision, Recall, and F1 Score, we rigorously assessed their effectiveness.
Our detailed analyses and comparisons reveal that the combination of SMOTE with
XGBoost and LightGBM offers a highly efficient and powerful mechanism for
payment security protection. Moreover, the integration of XGBoost and LightGBM
in a Local Ensemble model further demonstrated outstanding performance. After
incorporating SMOTE, the new combined model achieved a significant improvement
of nearly 6\% over traditional models and around 5\% over its sub-models,
showcasing remarkable results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is received by https://ieee-metacom.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Credit Card Fraud Detection Using Advanced <span class="highlight-title">Transformer</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03733v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03733v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Yu, Yongshun Xu, Jin Cao, Ye Zhang, Yinxin Jin, Mengran Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of various online and mobile payment systems, credit
card fraud has emerged as a significant threat to financial security. This
study focuses on innovative applications of the latest Transformer models for
more robust and precise fraud detection. To ensure the reliability of the data,
we meticulously processed the data sources, balancing the dataset to address
the issue of data sparsity significantly. We also selected highly correlated
vectors to strengthen the training process.To guarantee the reliability and
practicality of the new Transformer model, we conducted performance comparisons
with several widely adopted models, including Support Vector Machine (SVM),
Random Forest, Neural Network, and Logistic Regression. We rigorously compared
these models using metrics such as Precision, Recall, and F1 Score. Through
these detailed analyses and comparisons, we present to the readers a highly
efficient and powerful anti-fraud mechanism with promising prospects. The
results demonstrate that the Transformer model not only excels in traditional
applications but also shows great potential in niche areas like fraud
detection, offering a substantial advancement in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper have been received by https://ieee-metacom.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Credit Score Prediction Using Ensemble Deep Learning Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianwen Xing, Chang Yu, Sining Huang, Qi Zheng, Xingyu Mu, Mengying Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contemporary economic society, credit scores are crucial for every
participant. A robust credit evaluation system is essential for the
profitability of core businesses such as credit cards, loans, and investments
for commercial banks and the financial sector. This paper combines
high-performance models like XGBoost and LightGBM, already widely used in
modern banking systems, with the powerful TabNet model. We have developed a
potent model capable of accurately determining credit score levels by
integrating Random Forest, XGBoost, and TabNet, and through the stacking
technique in ensemble modeling. This approach surpasses the limitations of
single models and significantly advances the precise credit score prediction.
In the following sections, we will explain the techniques we used and
thoroughly validate our approach by comprehensively comparing a series of
metrics such as Precision, Recall, F1, and AUC. By integrating Random Forest,
XGBoost, and with the TabNet deep learning architecture, these models
complement each other, demonstrating exceptionally strong overall performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper have been accepted by sci of AI Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Levin Tree Search with Context Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16945v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16945v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Orseau, Marcus Hutter, Levi H. S. Lelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a
probability distribution over actions) and comes with a theoretical guarantee
on the number of expansions before reaching a goal node, depending on the
quality of the policy. This guarantee can be used as a loss function, which we
call the LTS loss, to optimize neural networks representing the policy
(LTS+NN). In this work we show that the neural network can be substituted with
parameterized context models originating from the online compression literature
(LTS+CM). We show that the LTS loss is convex under this new model, which
allows for using standard convex optimization tools, and obtain convergence
guarantees to the optimal parameters in an online setting for a given set of
solution trajectories -- guarantees that cannot be provided for neural
networks. The new LTS+CM algorithm compares favorably against LTS+NN on several
benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle
(STP). The difference is particularly large on STP, where LTS+NN fails to solve
most of the test instances while LTS+CM solves each test instance in a fraction
of a second. Furthermore, we show that LTS+CM is able to learn a policy that
solves the Rubik's cube in only a few hundred expansions, which considerably
improves upon previous machine learning techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Piecewise Linearity of Min-Norm Solution Map of a Nonconvexly
  Regularized Convex Sparse Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18438v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18438v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Isao Yamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well known that the minimum $\ell_2$-norm solution of the convex LASSO
model, say $\mathbf{x}_{\star}$, is a continuous piecewise linear function of
the regularization parameter $\lambda$, and its signed sparsity pattern is
constant within each linear piece. The current study is an extension of this
classic result, proving that the aforementioned properties extend to the
min-norm solution map $\mathbf{x}_{\star}(\mathbf{y},\lambda)$, where
$\mathbf{y}$ is the observed signal, for a generalization of LASSO termed the
scaled generalized minimax concave (sGMC) model. The sGMC model adopts a
nonconvex debiased variant of the $\ell_1$-norm as sparse regularizer, but its
objective function is overall-convex. Based on the geometric properties of
$\mathbf{x}_{\star}(\mathbf{y},\lambda)$, we propose an extension of the least
angle regression (LARS) algorithm, which iteratively computes the closed-form
expression of $\mathbf{x}_{\star}(\mathbf{y},\lambda)$ in each linear zone.
Under suitable conditions, the proposed algorithm provably obtains the whole
solution map $\mathbf{x}_{\star}(\mathbf{y},\lambda)$ within finite iterations.
Notably, our proof techniques for establishing continuity and piecewise
linearity of $\mathbf{x}_{\star}(\mathbf{y},\lambda)$ are novel, and they lead
to two side contributions: (a) our proofs establish continuity of the sGMC
solution set as a set-valued mapping of $(\mathbf{y},\lambda)$; (b) to prove
piecewise linearity and piecewise constant sparsity pattern of
$\mathbf{x}_{\star}(\mathbf{y},\lambda)$, we do not require any assumption that
previous work relies on (whereas to prove some additional properties of
$\mathbf{x}_{\star}(\mathbf{y},\lambda)$, we use a different set of assumptions
from previous work).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages. Submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RiNALMo: General-Purpose RNA Language Models Can Generalize Well on
  Structure Prediction Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Josip Penić, Tin Vlašić, Roland G. Huber, Yue Wan, Mile Šikić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While RNA has recently been recognized as an interesting small-molecule drug
target, many challenges remain to be addressed before we take full advantage of
it. This emphasizes the necessity to improve our understanding of its
structures and functions. Over the years, sequencing technologies have produced
an enormous amount of unlabeled RNA data, which hides a huge potential.
Motivated by the successes of protein language models, we introduce RiboNucleic
Acid Language Model (RiNALMo) to unveil the hidden code of RNA. RiNALMo is the
largest RNA language model to date, with 650M parameters pre-trained on 36M
non-coding RNA sequences from several databases. It can extract hidden
knowledge and capture the underlying structure information implicitly embedded
within the RNA sequences. RiNALMo achieves state-of-the-art results on several
downstream tasks. Notably, we show that its generalization capabilities
overcome the inability of other deep learning methods for secondary structure
prediction to generalize on unseen RNA families.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Compositional Generalization for Object-Centric Learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thaddäus Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations that generalize to novel compositions of known
concepts is crucial for bridging the gap between human and machine perception.
One prominent effort is learning object-centric representations, which are
widely conjectured to enable compositional generalization. Yet, it remains
unclear when this conjecture will be true, as a principled theoretical or
empirical understanding of compositional generalization is lacking. In this
work, we investigate when compositional generalization is guaranteed for
object-centric representations through the lens of identifiability theory. We
show that autoencoders that satisfy structural assumptions on the decoder and
enforce encoder-decoder consistency will learn object-centric representations
that provably generalize compositionally. We validate our theoretical result
and highlight the practical relevance of our assumptions through experiments on
synthetic image data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral at ICLR 2024. The first four authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional and Deep Learning based techniques for Time Series Ordinal
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10084v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10084v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Ayllón-Gavilán, David Guijo-Rubio, Pedro Antonio Gutiérrez, Anthony Bagnall, César Hervás-Martínez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Classification (TSC) covers the supervised learning problem where
input data is provided in the form of series of values observed through
repeated measurements over time, and whose objective is to predict the category
to which they belong. When the class values are ordinal, classifiers that take
this into account can perform better than nominal classifiers. Time Series
Ordinal Classification (TSOC) is the field covering this gap, yet unexplored in
the literature. There are a wide range of time series problems showing an
ordered label structure, and TSC techniques that ignore the order relationship
discard useful information. Hence, this paper presents a first benchmarking of
TSOC methodologies, exploiting the ordering of the target labels to boost the
performance of current TSC state-of-the-art. Both convolutional- and deep
learning-based methodologies (among the best performing alternatives for
nominal TSC) are adapted for TSOC. For the experiments, a selection of 29
ordinal problems from two well-known archives has been made. In this way, this
paper contributes to the establishment of the state-of-the-art in TSOC. The
results obtained by ordinal versions are found to be significantly better than
current nominal TSC techniques in terms of ordinal performance metrics,
outlining the importance of considering the ordering of the labels when dealing
with this kind of problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Basis-to-Basis Operator Learning Using Function Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Ingebrand, Adam J. Thorpe, Somdatta Goswami, Krishna Kumar, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Basis-to-Basis (B2B) operator learning, a novel approach for
learning operators on Hilbert spaces of functions based on the foundational
ideas of function encoders. We decompose the task of learning operators into
two parts: learning sets of basis functions for both the input and output
spaces and learning a potentially nonlinear mapping between the coefficients of
the basis functions. B2B operator learning circumvents many challenges of prior
works, such as requiring data to be at fixed locations, by leveraging classic
techniques such as least squares to compute the coefficients. It is especially
potent for linear operators, where we compute a mapping between bases as a
single matrix transformation with a closed-form solution. Furthermore, with
minimal modifications and using the deep theoretical connections between
function encoders and functional analysis, we derive operator learning
algorithms that are directly analogous to eigen-decomposition and singular
value decomposition. We empirically validate B2B operator learning on seven
benchmark operator learning tasks and show that it demonstrates a
two-orders-of-magnitude improvement in accuracy over existing approaches on
several benchmark tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpret Your Decision: Logical Reasoning Regularization for
  Generalization in Visual Classification <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04492v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04492v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision models excel in image classification but struggle to generalize to
unseen data, such as classifying images from unseen domains or discovering
novel categories. In this paper, we explore the relationship between logical
reasoning and deep learning generalization in visual classification. A logical
regularization termed L-Reg is derived which bridges a logical analysis
framework to image classification. Our work reveals that L-Reg reduces the
complexity of the model in terms of the feature distribution and classifier
weights. Specifically, we unveil the interpretability brought by L-Reg, as it
enables the model to extract the salient features, such as faces to persons,
for classification. Theoretical analysis and experiments demonstrate that L-Reg
enhances generalization across various scenarios, including multi-domain
generalization and generalized category discovery. In complex real-world
scenarios where images span unknown classes and unseen domains, L-Reg
consistently improves generalization, highlighting its practical efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024 as Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Manifold Perspective on the Statistical Generalization of Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05225v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05225v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyang Wang, Juan Cervino, Alejandro Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) extend convolutional neural networks to operate
on graphs. Despite their impressive performances in various graph learning
tasks, the theoretical understanding of their generalization capability is
still lacking. Previous GNN generalization bounds ignore the underlying graph
structures, often leading to bounds that increase with the number of nodes -- a
behavior contrary to the one experienced in practice. In this paper, we take a
manifold perspective to establish the statistical generalization theory of GNNs
on graphs sampled from a manifold in the spectral domain. As demonstrated
empirically, we prove that the generalization bounds of GNNs decrease linearly
with the size of the graphs in the logarithmic scale, and increase linearly
with the spectral continuity constants of the filter functions. Notably, our
theory explains both node-level and graph-level tasks. Our result has two
implications: i) guaranteeing the generalization of GNNs to unseen data over
manifolds; ii) providing insights into the practical design of GNNs, i.e.,
restrictions on the discriminability of GNNs are necessary to obtain a better
generalization performance. We demonstrate our generalization bounds of GNNs
using synthetic and multiple real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages,25 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic planning in hierarchical active inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Priorelli, Ivilin Peev Stoianov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By dynamic planning, we refer to the ability of the human brain to infer and
impose motor trajectories related to cognitive decisions. A recent paradigm,
active inference, brings fundamental insights into the adaptation of biological
organisms, constantly striving to minimize prediction errors to restrict
themselves to life-compatible states. Over the past years, many studies have
shown how human and animal behaviors could be explained in terms of active
inference - either as discrete decision-making or continuous motor control -
inspiring innovative solutions in robotics and artificial intelligence. Still,
the literature lacks a comprehensive outlook on effectively planning realistic
actions in changing environments. Setting ourselves the goal of modeling
complex tasks such as tool use, we delve into the topic of dynamic planning in
active inference, keeping in mind two crucial aspects of biological behavior:
the capacity to understand and exploit affordances for object manipulation, and
to learn the hierarchical interactions between the self and the environment,
including other agents. We start from a simple unit and gradually describe more
advanced structures, comparing recently proposed design choices and providing
basic examples. This study distances itself from traditional views centered on
neural networks and reinforcement learning, and points toward a yet unexplored
direction in active inference: hybrid representations in hierarchical models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Reinforcement Learning with Imitation for Vision-Based
  Agile Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12203v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12203v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxu Xing, Angel Romero, Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning visuomotor policies for agile quadrotor flight presents significant
difficulties, primarily from inefficient policy exploration caused by
high-dimensional visual inputs and the need for precise and low-latency
control. To address these challenges, we propose a novel approach that combines
the performance of Reinforcement Learning (RL) and the sample efficiency of
Imitation Learning (IL) in the task of vision-based autonomous drone racing.
While RL provides a framework for learning high-performance controllers through
trial and error, it faces challenges with sample efficiency and computational
demands due to the high dimensionality of visual inputs. Conversely, IL
efficiently learns from visual expert demonstrations, but it remains limited by
the expert's performance and state distribution. To overcome these limitations,
our policy learning framework integrates the strengths of both approaches. Our
framework contains three phases: training a teacher policy using RL with
privileged state information, distilling it into a student policy via IL, and
adaptive fine-tuning via RL. Testing in both simulated and real-world scenarios
shows our approach can not only learn in scenarios where RL from scratch fails
but also outperforms existing IL methods in both robustness and performance,
successfully navigating a quadrotor through a race course using only visual
information. Videos of the experiments are available at
https://rpg.ifi.uzh.ch/bootstrap-rl-with-il/index.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8th Annual Conference on Robot Learning (CoRL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bandits with Abstention under Expert Advice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Pasteris, Alberto Rumi, Maximilian Thiessen, Shota Saito, Atsushi Miyauchi, Fabio Vitale, Mark Herbster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the classic problem of prediction with expert advice under bandit
feedback. Our model assumes that one action, corresponding to the learner's
abstention from play, has no reward or loss on every trial. We propose the CBA
algorithm, which exploits this assumption to obtain reward bounds that can
significantly improve those of the classical Exp4 algorithm. We can view our
problem as the aggregation of confidence-rated predictors when the learner has
the option of abstention from play. Importantly, we are the first to achieve
bounds on the expected cumulative reward for general confidence-rated
predictors. In the special case of specialists we achieve a novel reward bound,
significantly improving previous bounds of SpecialistExp (treating abstention
as another action). As an example application, we discuss learning unions of
balls in a finite metric space. In this contextual setting, we devise an
efficient implementation of CBA, reducing the runtime from quadratic to almost
linear in the number of contexts. Preliminary experiments show that CBA
improves over existing bandit algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DistRL: An Asynchronous Distributed Reinforcement Learning Framework for
  On-Device Control Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device control agents, especially on mobile devices, are responsible for
operating mobile devices to fulfill users' requests, enabling seamless and
intuitive interactions. Integrating Multimodal Large Language Models (MLLMs)
into these agents enhances their ability to understand and execute complex
commands, thereby improving user experience. However, fine-tuning MLLMs for
on-device control presents significant challenges due to limited data
availability and inefficient online training processes. This paper introduces
DistRL, a novel framework designed to enhance the efficiency of online RL
fine-tuning for mobile device control agents. DistRL employs centralized
training and decentralized data acquisition to ensure efficient fine-tuning in
the context of dynamic online interactions. Additionally, the framework is
backed by our tailor-made RL algorithm, which effectively balances exploration
with the prioritized utilization of collected data to ensure stable and robust
training. Our experiments show that, on average, DistRL delivers a 3X
improvement in training efficiency and enables training data collection 2.4X
faster than the leading synchronous multi-machine methods. Notably, after
training, DistRL achieves a 20% relative improvement in success rate compared
to state-of-the-art methods on general Android tasks from an open benchmark,
significantly outperforming existing approaches while maintaining the same
training time. These results validate DistRL as a scalable and efficient
solution, offering substantial improvements in both training efficiency and
agent performance for real-world, in-the-wild device control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper and Appendix, 25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Can Evolve Continually on Modality for X-Modal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazuo Yu, Haomiao Xiong, Lu Zhang, Haiwen Diao, Yunzhi Zhuge, Lanqing Hong, Dong Wang, Huchuan Lu, You He, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have gained significant attention
due to their impressive capabilities in multimodal understanding. However,
existing methods rely heavily on extensive modal-specific pretraining and
joint-modal tuning, leading to significant computational burdens when expanding
to new modalities. In this paper, we propose PathWeave, a flexible and scalable
framework with modal-Path sWitching and ExpAnsion abilities that enables MLLMs
to continually EVolve on modalities for $\mathbb{X}$-modal reasoning. We
leverage the concept of Continual Learning and develop an incremental training
strategy atop pre-trained MLLMs, enabling their expansion to new modalities
using uni-modal data, without executing joint-modal pretraining. In detail, a
novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and
cross-modal adapters are seamlessly integrated to facilitate efficient modality
alignment and collaboration. Additionally, an MoE-based gating module is
applied between two types of adapters to further enhance the multimodal
interaction. To investigate the proposed method, we establish a challenging
benchmark called Continual Learning of Modality (MCL), which consists of
high-quality QA data from five distinct modalities: image, video, audio, depth
and point cloud. Extensive experiments demonstrate the effectiveness of the
proposed AnA framework on learning plasticity and memory stability during
continual learning. Furthermore, PathWeave performs comparably to
state-of-the-art MLLMs while concurrently reducing parameter training burdens
by 98.73%. Our code locates at https://github.com/JiazuoYu/PathWeave
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniTE: A <span class="highlight-title">Survey</span> and Unified Pipeline for <span class="highlight-title">Pre-train</span>ing Spatiotemporal
  Trajectory Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Lin, Zeyu Zhou, Yicheng Liu, Haochen Lv, Haomin Wen, Tianyi Li, Yushuai Li, Christian S. Jensen, Shengnan Guo, Youfang Lin, Huaiyu Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal trajectories are sequences of timestamped locations, which
enable a variety of analyses that in turn enable important real-world
applications. It is common to map trajectories to vectors, called embeddings,
before subsequent analyses. Thus, the qualities of embeddings are very
important. Methods for pre-training embeddings, which leverage unlabeled
trajectories for training universal embeddings, have shown promising
applicability across different tasks, thus attracting considerable interest.
However, research progress on this topic faces two key challenges: a lack of a
comprehensive overview of existing methods, resulting in several related
methods not being well-recognized, and the absence of a unified pipeline,
complicating the development of new methods and the analysis of methods.
  We present UniTE, a survey and a unified pipeline for this domain. In doing
so, we present a comprehensive list of existing methods for pre-training
trajectory embeddings, which includes methods that either explicitly or
implicitly employ pre-training techniques. Further, we present a unified and
modular pipeline with publicly available underlying code, simplifying the
process of constructing and evaluating methods for pre-training trajectory
embeddings. Additionally, we contribute a selection of experimental results
using the proposed pipeline on real-world datasets. Implementation of the
pipeline is publicly available at https://github.com/Logan-Lin/UniTE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Hamiltonian, structure and trace distance learning of Gaussian
  states 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Fanizza, Cambyse Rouzé, Daniel Stilck França
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we initiate the study of Hamiltonian learning for positive
temperature bosonic Gaussian states, the quantum generalization of the widely
studied problem of learning Gaussian graphical models. We obtain efficient
protocols, both in sample and computational complexity, for the task of
inferring the parameters of their underlying quadratic Hamiltonian under the
assumption of bounded temperature, squeezing, displacement and maximal degree
of the interaction graph. Our protocol only requires heterodyne measurements,
which are often experimentally feasible, and has a sample complexity that
scales logarithmically with the number of modes. Furthermore, we show that it
is possible to learn the underlying interaction graph in a similar setting and
sample complexity. Taken together, our results put the status of the quantum
Hamiltonian learning problem for continuous variable systems in a much more
advanced state when compared to spins, where state-of-the-art results are
either unavailable or quantitatively inferior to ours. In addition, we use our
techniques to obtain the first results on learning Gaussian states in trace
distance with a quadratic scaling in precision and polynomial in the number of
modes, albeit imposing certain restrictions on the Gaussian states. Our main
technical innovations are several continuity bounds for the covariance and
Hamiltonian matrix of a Gaussian state, which are of independent interest,
combined with what we call the local inversion technique. In essence, the local
inversion technique allows us to reliably infer the Hamiltonian of a Gaussian
state by only estimating in parallel submatrices of the covariance matrix whose
size scales with the desired precision, but not the number of modes. This way
we bypass the need to obtain precise global estimates of the covariance matrix,
controlling the sample complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 1 figure. Corrections to Lemma 4.1. Main results are
  unchanged</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SUMO: Search-Based Uncertainty Estimation for Model-Based Offline
  Reinforcement Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongjian Qiao, Jiafei Lyu, Kechen Jiao, Qi Liu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of offline reinforcement learning (RL) suffers from the
limited size and quality of static datasets. Model-based offline RL addresses
this issue by generating synthetic samples through a dynamics model to enhance
overall performance. To evaluate the reliability of the generated samples,
uncertainty estimation methods are often employed. However, model ensemble, the
most commonly used uncertainty estimation method, is not always the best
choice. In this paper, we propose a \textbf{S}earch-based \textbf{U}ncertainty
estimation method for \textbf{M}odel-based \textbf{O}ffline RL (SUMO) as an
alternative. SUMO characterizes the uncertainty of synthetic samples by
measuring their cross entropy against the in-distribution dataset samples, and
uses an efficient search-based method for implementation. In this way, SUMO can
achieve trustworthy uncertainty estimation. We integrate SUMO into several
model-based offline RL algorithms including MOPO and Adapted MOReL (AMOReL),
and provide theoretical analysis for them. Extensive experimental results on
D4RL datasets demonstrate that SUMO can provide more accurate uncertainty
estimation and boost the performance of base algorithms. These indicate that
SUMO could be a better uncertainty estimator for model-based offline RL when
used in either reward penalty or trajectory truncation. Our code is available
and will be open-source for further research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pessimistic Iterative Planning for Robust POMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maris F. L. Galesloot, Marnix Suilen, Thiago D. Simão, Steven Carr, Matthijs T. J. Spaan, Ufuk Topcu, Nils Jansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust POMDPs extend classical POMDPs to handle model uncertainty.
Specifically, robust POMDPs exhibit so-called uncertainty sets on the
transition and observation models, effectively defining ranges of
probabilities. Policies for robust POMDPs must be (1) memory-based to account
for partial observability and (2) robust against model uncertainty to account
for the worst-case instances from the uncertainty sets. To compute such robust
memory-based policies, we propose the pessimistic iterative planning (PIP)
framework, which alternates between two main steps: (1) selecting a pessimistic
(non-robust) POMDP via worst-case probability instances from the uncertainty
sets; and (2) computing a finite-state controller (FSC) for this pessimistic
POMDP. We evaluate the performance of this FSC on the original robust POMDP and
use this evaluation in step (1) to select the next pessimistic POMDP. Within
PIP, we propose the rFSCNet algorithm. In each iteration, rFSCNet finds an FSC
through a recurrent neural network by using supervision policies optimized for
the pessimistic POMDP. The empirical evaluation in four benchmark environments
showcases improved robustness against several baseline methods and competitive
performance compared to a state-of-the-art robust POMDP solver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Cross-Domain Benchmark for Active Learning <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thorben Werner, Johannes Burchert, Maximilian Stubbemann, Lars Schmidt-Thieme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Learning (AL) deals with identifying the most informative samples for
labeling to reduce data annotation costs for supervised learning tasks. AL
research suffers from the fact that lifts from literature generalize poorly and
that only a small number of repetitions of experiments are conducted. To
overcome these obstacles, we propose CDALBench, the first active learning
benchmark which includes tasks in computer vision, natural language processing
and tabular learning. Furthermore, by providing an efficient, greedy oracle,
CDALBench can be evaluated with 50 runs for each experiment. We show, that both
the cross-domain character and a large amount of repetitions are crucial for
sophisticated evaluation of AL research. Concretely, we show that the
superiority of specific methods varies over the different domains, making it
important to evaluate Active Learning with a cross-domain benchmark.
Additionally, we show that having a large amount of runs is crucial. With only
conducting three runs as often done in the literature, the superiority of
specific methods can strongly vary with the specific runs. This effect is so
strong, that, depending on the seed, even a well-established method's
performance can be significantly better and significantly worse than random for
the same dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 24 in the Benchmarks and Datasets Track. Updated
  version of paper "Toward Comparable Active Learning" (arXiv:2311.18356).
  "Toward Comparable Active Learning" is deprecated, please use this version.
  arXiv admin note: text overlap with arXiv:2311.18356; text overlap with
  arXiv:2301.10625 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Activation Sparsity with Dense to Dynamic-k
  Mixture-of-Experts Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04361v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04361v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Szatkowski, Bartosz Wójcik, Mikołaj Piórczyński, Simone Scardapane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models can face practical limitations due to their high
computational requirements. At the same time, such models exhibit significant
activation sparsity, which can be leveraged to reduce the inference cost by
converting parts of the network into equivalent Mixture-of-Experts (MoE)
layers. Despite the crucial role played by activation sparsity, its impact on
this process remains unexplored. We demonstrate that the efficiency of the
conversion can be significantly enhanced by a proper regularization of the
activation sparsity of the base model. Moreover, motivated by the high variance
of the number of activated neurons for different inputs, we introduce a more
effective dynamic-$k$ expert selection rule that adjusts the number of executed
experts on a per-token basis. To achieve further savings, we extend this
approach to multi-head attention projections. Finally, we develop an efficient
implementation that translates these computational savings into actual
wall-clock speedup. The proposed method, Dense to Dynamic-$k$
Mixture-of-Experts (D2DMoE), outperforms existing approaches on common NLP and
vision tasks, reducing inference cost by up to 60% without significantly
impacting performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Integrated Sensing, Communication, and Computation <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingzhu Wen, Yong Zhou, Xiaoyang Li, Yuanming Shi, Kaibin Huang, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The forthcoming generation of wireless technology, 6G, aims to usher in an
era of ubiquitous intelligent services, where everything is interconnected and
intelligent. This vision requires the seamless integration of three fundamental
modules: Sensing for information acquisition, communication for information
sharing, and computation for information processing and decision-making. These
modules are intricately linked, especially in complex tasks such as edge
learning and inference. However, the performance of these modules is
interdependent, creating a resource competition for time, energy, and
bandwidth. Existing techniques like integrated communication and computation
(ICC), integrated sensing and computation (ISC), and integrated sensing and
communication (ISAC) have made partial strides in addressing this challenge,
but they fall short of meeting the extreme performance requirements. To
overcome these limitations, it is essential to develop new techniques that
comprehensively integrate sensing, communication, and computation. This
integrated approach, known as Integrated Sensing, Communication, and
Computation (ISCC), offers a systematic perspective for enhancing task
performance. This paper begins with a comprehensive survey of historic and
related techniques such as ICC, ISC, and ISAC, highlighting their strengths and
limitations. It then discusses the benefits, functions, and challenges of ISCC.
Subsequently, the state-of-the-art signal designs for ISCC, along with network
resource management strategies specifically tailored for ISCC are explored.
Furthermore, this paper discusses the exciting research opportunities that lie
ahead for implementing ISCC in future advanced networks, and the unresolved
issues requiring further investigation. ISCC is expected to unlock the full
potential of intelligent connectivity, paving the way for groundbreaking
applications and services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In this version, a series of discussions have been added.The
  benefits, functions, and challenges of ISCC are investigated using a new
  section. Moreover, the unresolved issues of ISCC have been discussed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably <span class="highlight-title">Transformer</span>s Harness Multi-Concept Word Semantics for Efficient
  In-Context Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02199v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02199v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models (LLMs) have displayed remarkable
creative prowess and emergence capabilities. Existing empirical studies have
revealed a strong connection between these LLMs' impressive emergence abilities
and their in-context learning (ICL) capacity, allowing them to solve new tasks
using only task-specific prompts without further fine-tuning. On the other
hand, existing empirical and theoretical studies also show that there is a
linear regularity of the multi-concept encoded semantic representation behind
transformer-based LLMs. However, existing theoretical work fail to build up an
understanding of the connection between this regularity and the innovative
power of ICL. Additionally, prior work often focuses on simplified, unrealistic
scenarios involving linear transformers or unrealistic loss functions, and they
achieve only linear or sub-linear convergence rates. In contrast, this work
provides a fine-grained mathematical analysis to show how transformers leverage
the multi-concept semantics of words to enable powerful ICL and excellent
out-of-distribution ICL abilities, offering insights into how transformers
innovate solutions for certain unseen tasks encoded with multiple cross-concept
semantics. Inspired by empirical studies on the linear latent geometry of LLMs,
the analysis is based on a concept-based low-noise sparse coding prompt model.
Leveraging advanced techniques, this work showcases the exponential 0-1 loss
convergence over the highly non-convex training dynamics, which pioneeringly
incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and
cross-entropy loss. Empirical simulations corroborate the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generalist Robot Learning from Internet Video: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19664v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19664v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert McCarthy, Daniel C. H. Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, Thomas G. Thuruthel, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling deep learning to massive, diverse internet data has yielded
remarkably general capabilities in visual and natural language understanding
and generation. However, data has remained scarce and challenging to collect in
robotics, seeing robot learning struggle to obtain similarly general
capabilities. Promising Learning from Videos (LfV) methods aim to address the
robotics data bottleneck by augmenting traditional robot data with large-scale
internet video data. This video data offers broad foundational information
regarding physical behaviour and the underlying physics of the world, and thus
can be highly informative for a generalist robot.
  In this survey, we present a thorough overview of the emerging field of LfV.
We outline fundamental concepts, including the benefits and challenges of LfV.
We provide a comprehensive review of current methods for extracting knowledge
from large-scale internet video, addressing key challenges in LfV, and boosting
downstream robot and reinforcement learning via the use of video data. The
survey concludes with a critical discussion of challenges and opportunities in
LfV. Here, we advocate for scalable foundation model approaches that can
leverage the full range of available internet video to improve the learning of
robot policies and dynamics models. We hope this survey can inform and catalyse
further LfV research, driving progress towards the development of
general-purpose robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Viti, Franz Thaler, Kathrin Lisa Kapper, Martin Urschler, Martin Holler, Elias Karabelas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of cardiac magnetic resonance images (MRI) is crucial for the
analysis and assessment of cardiac function, helping to diagnose and treat
various cardiovascular diseases. Most recent techniques rely on deep learning
and usually require an extensive amount of labeled data. To overcome this
problem, few-shot learning has the capability of reducing data dependency on
labeled data. In this work, we introduce a new method that merges few-shot
learning with a U-Net architecture and Gaussian Process Emulators (GPEs),
enhancing data integration from a support set for improved performance. GPEs
are trained to learn the relation between the support images and the
corresponding masks in latent space, facilitating the segmentation of unseen
query images given only a small labeled support set at inference. We test our
model with the M&Ms-2 public dataset to assess its ability to segment the heart
in cardiac magnetic resonance imaging from different orientations, and compare
it with state-of-the-art unsupervised and few-shot methods. Our architecture
shows higher DICE coefficients compared to these methods, especially in the
more challenging setups where the size of the support set is considerably
small.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Statistical Atlases and Computational Modeling of the
  Heart (STACOM) Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCMDP: Observation-Constrained Markov Decision Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiyi Wang, Jianheng Liu, Bryan Lee, Zhihao Wu, Yu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many practical applications, decision-making processes must balance the
costs of acquiring information with the benefits it provides. Traditional
control systems often assume full observability, an unrealistic assumption when
observations are expensive. We tackle the challenge of simultaneously learning
observation and control strategies in such cost-sensitive environments by
introducing the Observation-Constrained Markov Decision Process (OCMDP), where
the policy influences the observability of the true state. To manage the
complexity arising from the combined observation and control actions, we
develop an iterative, model-free deep reinforcement learning algorithm that
separates the sensing and control components of the policy. This decomposition
enables efficient learning in the expanded action space by focusing on when and
what to observe, as well as determining optimal control actions, without
requiring knowledge of the environment's dynamics. We validate our approach on
a simulated diagnostic task and a realistic healthcare environment using
HeartPole. Given both scenarios, the experimental results demonstrate that our
model achieves a substantial reduction in observation costs on average,
significantly outperforming baseline methods by a notable margin in efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full paper, 14 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on
  Supervised Regression (Preprint) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12308v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12308v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansel Gonzalez Tejeda, Helmut A. Mayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this tutorial, we present a compact and holistic discussion of Deep
Learning with a focus on Convolutional Neural Networks (CNNs) and supervised
regression. While there are numerous books and articles on the individual
topics we cover, comprehensive and detailed tutorials that address Deep
Learning from a foundational yet rigorous and accessible perspective are rare.
Most resources on CNNs are either too advanced, focusing on cutting-edge
architectures, or too narrow, addressing only specific applications like image
classification.This tutorial not only summarizes the most relevant concepts but
also provides an in-depth exploration of each, offering a complete yet agile
set of ideas. Moreover, we highlight the powerful synergy between learning
theory, statistic, and machine learning, which together underpin the Deep
Learning and CNN frameworks. We aim for this tutorial to serve as an optimal
resource for students, professors, and anyone interested in understanding the
foundations of Deep Learning. Upon acceptance we will provide an accompanying
repository under
\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial}
  Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine
Learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the journal Machine Learning and Knowledge Extraction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLHF Workflow: From Reward Modeling to Online RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07863v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07863v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the workflow of Online Iterative Reinforcement Learning from Human
Feedback (RLHF) in this technical report, which is widely reported to
outperform its offline counterpart by a large margin in the recent large
language model (LLM) literature. However, existing open-source RLHF projects
are still largely confined to the offline learning setting. In this technical
report, we aim to fill in this gap and provide a detailed recipe that is easy
to reproduce for online iterative RLHF. In particular, since online human
feedback is usually infeasible for open-source communities with limited
resources, we start by constructing preference models using a diverse set of
open-source datasets and use the constructed proxy preference model to
approximate human feedback. Then, we discuss the theoretical insights and
algorithmic principles behind online iterative RLHF, followed by a detailed
practical implementation. Our trained LLM achieves impressive performance on
LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as
well as other academic benchmarks such as HumanEval and TruthfulQA. We have
shown that supervised fine-tuning (SFT) and iterative RLHF can obtain
state-of-the-art performance with fully open-source datasets. Further, we have
made our models, curated datasets, and comprehensive step-by-step code
guidebooks publicly available. Please refer to
https://github.com/RLHFlow/RLHF-Reward-Modeling and
https://github.com/RLHFlow/Online-RLHF for more detailed information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (09/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Overview</span> frequency principle/spectral bias in deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.07395v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.07395v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding deep learning is increasingly emergent as it penetrates more
and more into industry and science. In recent years, a research line from
Fourier analysis sheds lights on this magical "black box" by showing a
Frequency Principle (F-Principle or spectral bias) of the training behavior of
deep neural networks (DNNs) -- DNNs often fit functions from low to high
frequency during the training. The F-Principle is first demonstrated by
onedimensional synthetic data followed by the verification in high-dimensional
real datasets. A series of works subsequently enhance the validity of the
F-Principle. This low-frequency implicit bias reveals the strength of neural
network in learning low-frequency functions as well as its deficiency in
learning high-frequency functions. Such understanding inspires the design of
DNN-based algorithms in practical problems, explains experimental phenomena
emerging in various scenarios, and further advances the study of deep learning
from the frequency perspective. Although incomplete, we provide an overview of
F-Principle and propose some open problems for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mr.Steve: Instruction-Following Agents in Minecraft with What-Where-When
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyeong Park, Junmo Cho, Sungjin Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advances have been made in developing general-purpose embodied AI
in environments like Minecraft through the adoption of LLM-augmented
hierarchical approaches. While these approaches, which combine high-level
planners with low-level controllers, show promise, low-level controllers
frequently become performance bottlenecks due to repeated failures. In this
paper, we argue that the primary cause of failure in many low-level controllers
is the absence of an episodic memory system. To address this, we introduce Mr.
Steve (Memory Recall Steve-1), a novel low-level controller equipped with Place
Event Memory (PEM), a form of episodic memory that captures what, where, and
when information from episodes. This directly addresses the main limitation of
the popular low-level controller, Steve-1. Unlike previous models that rely on
short-term memory, PEM organizes spatial and event-based data, enabling
efficient recall and navigation in long-horizon tasks. Additionally, we propose
an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing
agents to alternate between exploration and task-solving based on recalled
events. Our approach significantly improves task-solving and exploration
efficiency compared to existing methods. We will release our code and demos on
the project page: https://sites.google.com/view/mr-steve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalar Function Topology Divergence: Comparing Topology of 3D Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Trofimov, Daria Voronkova, Eduard Tulchinskii, Evgeny Burnaev, Serguei Barannikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new topological tool for computer vision - Scalar Function
Topology Divergence (SFTD), which measures the dissimilarity of multi-scale
topology between sublevel sets of two functions having a common domain.
Functions can be defined on an undirected graph or Euclidean space of any
dimensionality. Most of the existing methods for comparing topology are based
on Wasserstein distance between persistence barcodes and they don't take into
account the localization of topological features. The minimization of SFTD
ensures that the corresponding topological features of scalar functions are
located in the same places. The proposed tool provides useful visualizations
depicting areas where functions have topological dissimilarities. We provide
applications of the proposed method to 3D computer vision. In particular,
experiments demonstrate that SFTD as an additional loss improves the
reconstruction of cellular 3D shapes from 2D fluorescence microscopy images,
and helps to identify topological errors in 3D segmentation. Additionally, we
show that SFTD outperforms Betti matching loss in 2D segmentation problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiCoEval: Evaluating LLMs on License Compliance in Code Generation <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Xu, Kai Gao, Hao He, Minghui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have revolutionized code
generation, leading to widespread adoption of AI coding tools by developers.
However, LLMs can generate license-protected code without providing the
necessary license information, leading to potential intellectual property
violations during software production. This paper addresses the critical, yet
underexplored, issue of license compliance in LLM-generated code by
establishing a benchmark to evaluate the ability of LLMs to provide accurate
license information for their generated code. To establish this benchmark, we
conduct an empirical study to identify a reasonable standard for "striking
similarity" that excludes the possibility of independent creation, indicating a
copy relationship between the LLM output and certain open-source code. Based on
this standard, we propose LiCoEval, to evaluate the license compliance
capabilities of LLMs, i.e., the ability to provide accurate license or
copyright information when they generate code with striking similarity to
already existing copyrighted code. Using LiCoEval, we evaluate 14 popular LLMs,
finding that even top-performing LLMs produce a non-negligible proportion
(0.88% to 2.01%) of code strikingly similar to existing open-source
implementations. Notably, most LLMs fail to provide accurate license
information, particularly for code under copyleft licenses. These findings
underscore the urgent need to enhance LLM compliance capabilities in code
generation tasks. Our study provides a foundation for future research and
development to improve license compliance in AI-assisted software development,
contributing to both the protection of open-source software copyrights and the
mitigation of legal risks for LLM users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 47th International Conference on Software Engineering(ICSE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Human-AI Complementarity with Prediction Sets <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni De Toni, Nastaran Okati, Suhas Thejaswi, Eleni Straitouri, Manuel Gomez-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision support systems based on prediction sets have proven to be effective
at helping human experts solve classification tasks. Rather than providing
single-label predictions, these systems provide sets of label predictions
constructed using conformal prediction, namely prediction sets, and ask human
experts to predict label values from these sets. In this paper, we first show
that the prediction sets constructed using conformal prediction are, in
general, suboptimal in terms of average accuracy. Then, we show that the
problem of finding the optimal prediction sets under which the human experts
achieve the highest average accuracy is NP-hard. More strongly, unless P = NP,
we show that the problem is hard to approximate to any factor less than the
size of the label set. However, we introduce a simple and efficient greedy
algorithm that, for a large class of expert models and non-conformity scores,
is guaranteed to find prediction sets that provably offer equal or greater
performance than those constructed using conformal prediction. Further, using a
simulation study with both synthetic and real expert predictions, we
demonstrate that, in practice, our greedy algorithm finds near-optimal
prediction sets offering greater performance than conformal prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenUAS: Embeddings of Cities in Japan with Anchor Data for Cross-city
  Analysis of Area Usage Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19872v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19872v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Tamura, Kazuyuki Shoji, Shin Katayama, Kenta Urano, Takuro Yonezawa, Nobuo Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We publicly release OpenUAS, a dataset of area embeddings based on urban
usage patterns, including embeddings for over 1.3 million 50-meter square
meshes covering a total area of 3,300 square kilometers. This dataset is
valuable for analyzing area functions in fields such as market analysis, urban
planning, transportation infrastructure, and infection prediction. It captures
the characteristics of each area in the city, such as office districts and
residential areas, by employing an area embedding technique that utilizes
location information typically obtained by GPS. Numerous area embedding
techniques have been proposed, and while the public release of such embedding
datasets is technically feasible, it has not been realized. One reason for this
is that previous methods could not embed areas from different cities and
periods into the same embedding space without sharing raw location data. We
address this issue by developing an anchoring method that establishes anchors
within a shared embedding space. We publicly release this anchor dataset along
with area embedding datasets from several periods in eight major Japanese
cities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Functional Structured Data Generators Rooted in
  Out-of-Equilibrium Physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandra Carbone, Aurélien Decelle, Lorenzo Rosset, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address the challenge of using energy-based models to
produce high-quality, label-specific data in complex structured datasets, such
as population genetics, RNA or protein sequences data. Traditional training
methods encounter difficulties due to inefficient Markov chain Monte Carlo
mixing, which affects the diversity of synthetic data and increases generation
times. To address these issues, we use a novel training algorithm that exploits
non-equilibrium effects. This approach, applied on the Restricted Boltzmann
Machine, improves the model's ability to correctly classify samples and
generate high-quality synthetic data in only a few sampling steps. The
effectiveness of this method is demonstrated by its successful application to
four different types of data: handwritten digits, mutations of human genomes
classified by continental origin, functionally characterized sequences of an
enzyme protein family, and homologous RNA sequences from specific taxonomies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Clustering on High-Dimensional Data with Stochastic Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02066v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02066v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Kozyriev, Vladimir Norkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the limitations of conventional vector quantization
algorithms, particularly K-Means and its variant K-Means++, and investigates
the Stochastic Quantization (SQ) algorithm as a scalable alternative for
high-dimensional unsupervised and semi-supervised learning tasks. Traditional
clustering algorithms often suffer from inefficient memory utilization during
computation, necessitating the loading of all data samples into memory, which
becomes impractical for large-scale datasets. While variants such as Mini-Batch
K-Means partially mitigate this issue by reducing memory usage, they lack
robust theoretical convergence guarantees due to the non-convex nature of
clustering problems. In contrast, the Stochastic Quantization algorithm
provides strong theoretical convergence guarantees, making it a robust
alternative for clustering tasks. We demonstrate the computational efficiency
and rapid convergence of the algorithm on an image classification problem with
partially labeled data, comparing model accuracy across various ratios of
labeled to unlabeled data. To address the challenge of high dimensionality, we
employ a Triplet Network to encode images into low-dimensional representations
in a latent space, which serve as a basis for comparing the efficiency of both
the Stochastic Quantization algorithm and traditional quantization algorithms.
Furthermore, we enhance the algorithm's convergence speed by introducing
modifications with an adaptive learning rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, to be published in the International Scientific
  Technical Journal "Problems of Control and Informatics"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn from Heterophily: Heterophilous Information-enhanced Graph Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Jiahao Xu, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under circumstances of heterophily, where nodes with different labels tend to
be connected based on semantic meanings, Graph Neural Networks (GNNs) often
exhibit suboptimal performance. Current studies on graph heterophily mainly
focus on aggregation calibration or neighbor extension and address the
heterophily issue by utilizing node features or structural information to
improve GNN representations. In this paper, we propose and demonstrate that the
valuable semantic information inherent in heterophily can be utilized
effectively in graph learning by investigating the distribution of neighbors
for each individual node within the graph. The theoretical analysis is carried
out to demonstrate the efficacy of the idea in enhancing graph learning. Based
on this analysis, we propose HiGNN, an innovative approach that constructs an
additional new graph structure, that integrates heterophilous information by
leveraging node distribution to enhance connectivity between nodes that share
similar semantic characteristics. We conduct empirical assessments on node
classification tasks using both homophilous and heterophilous benchmark
datasets and compare HiGNN to popular GNN baselines and SoTA methods,
confirming the effectiveness in improving graph representations. In addition,
by incorporating heterophilous information, we demonstrate a notable
enhancement in existing GNN-based approaches, and the homophily degree across
real-world datasets, thus affirming the efficacy of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-independent cosmological inference post DESI DR1 BAO measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Purba Mukherjee, Anjan Ananda Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we implement Gaussian process regression to reconstruct the
expansion history of the universe in a model-agnostic manner, using the
Pantheon-Plus SN-Ia compilation in combination with two different BAO
measurements (SDSS-IV and DESI DR1). In both the reconstructions, the
$\Lambda$CDM model is always included in the 95\% confidence intervals. We find
evidence that the DESI LRG data at $z_{\text{eff}} = 0.51$ is not an outlier
within our model-independent framework. We study the $\mathcal{O}m$-diagnostics
and the evolution of the total equation of state (EoS) of our universe, which
hint towards the possibility of a quintessence-like dark energy scenario with a
very slowly varying EoS, and a phantom-crossing in higher $z$. The entire
exercise is later complemented by considering two more SN-Ia compilations -
DES-5YR and Union3 - in combination with DESI BAO. Reconstruction with the DESI
BAO + DES-5YR SN data sets predicts that the $\Lambda$CDM model lies outside
the 3$\sigma$ confidence levels, whereas with DESI BAO + Union3 data, the
$\Lambda$CDM model is always included within 1$\sigma$. We also report
constraints on $H_0 r_d$ from our model-agnostic analysis, independent of the
pre-recombination physics. Our results point towards an $\approx$ 2$\sigma$
discrepancy between the DESI + Pantheon-Plus and DESI + DES-5YR data sets,
which calls for further investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 sets of figures. Accepted for publication in PRD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule
  Lists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothée Ly, Julien Ferry, Marie-José Huguet, Sébastien Gambs, Ulrich Aivodji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially-private (DP) mechanisms can be embedded into the design of a
machine learning algorithm to protect the resulting model against privacy
leakage. However, this often comes with a significant loss of accuracy due to
the noise added to enforce DP. In this paper, we aim at improving this
trade-off for a popular class of machine learning algorithms leveraging the
Gini impurity as an information gain criterion to greedily build interpretable
models such as decision trees or rule lists. To this end, we establish the
smooth sensitivity of the Gini impurity, which can be used to obtain thorough
DP guarantees while adding noise scaled with tighter magnitude. We illustrate
the applicability of this mechanism by integrating it within a greedy algorithm
producing rule list models, motivated by the fact that such models remain
understudied in the DP literature. Our theoretical analysis and experimental
results confirm that the DP rule lists models integrating smooth sensitivity
have higher accuracy that those using other DP frameworks based on global
sensitivity, for identical privacy budgets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPARTAN: A Sparse <span class="highlight-title">Transformer</span> Learning Local Causation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anson Lei, Bernhard Schölkopf, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal structures play a central role in world models that flexibly adapt to
changes in the environment. While recent works motivate the benefits of
discovering local causal graphs for dynamics modelling, in this work we
demonstrate that accurately capturing these relationships in complex settings
remains challenging for the current state-of-the-art. To remedy this
shortcoming, we postulate that sparsity is a critical ingredient for the
discovery of such local causal structures. To this end we present the SPARse
TrANsformer World model (SPARTAN), a Transformer-based world model that learns
local causal structures between entities in a scene. By applying sparsity
regularisation on the attention pattern between object-factored tokens, SPARTAN
identifies sparse local causal models that accurately predict future object
states. Furthermore, we extend our model to capture sparse interventions with
unknown targets on the dynamics of the environment. This results in a highly
interpretable world model that can efficiently adapt to changes. Empirically,
we evaluate SPARTAN against the current state-of-the-art in object-centric
world models on observation-based environments and demonstrate that our model
can learn accurate local causal graphs and achieve significantly improved
few-shot adaptation to changes in the dynamics of the environment as well as
robustness against removing irrelevant distractors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot NAS via the Suppression of Local Entropy Decrease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Wu, Han Huang, Yueting Xu, Zhifeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Architecture performance evaluation is the most time-consuming part of neural
architecture search (NAS). Zero-Shot NAS accelerates the evaluation by
utilizing zero-cost proxies instead of training. Though effective, existing
zero-cost proxies require invoking backpropagations or running networks on
input data, making it difficult to further accelerate the computation of
proxies. To alleviate this issue, architecture topologies are used to evaluate
the performance of networks in this study. We prove that particular
architectural topologies decrease the local entropy of feature maps, which
degrades specific features to a bias, thereby reducing network performance.
Based on this proof, architectural topologies are utilized to quantify the
suppression of local entropy decrease (SED) as a data-free and running-free
proxy. Experimental results show that SED outperforms most state-of-the-art
proxies in terms of architecture selection on five benchmarks, with computation
time reduced by three orders of magnitude. We further compare the SED-based NAS
with state-of-the-art proxies. SED-based NAS selects the architecture with
higher accuracy and fewer parameters in only one second. The theoretical
analyses of local entropy and experimental results demonstrate that the
suppression of local entropy decrease facilitates selecting optimal
architectures in Zero-Shot NAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures. Corrected typos and latex template</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CALoR: Towards Comprehensive Model Inversion Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyao Yu, Yixiang Qiu, Hao Fang, Bin Chen, Sijin Yu, Bin Wang, Shu-Tao Xia, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Inversion Attacks (MIAs) aim at recovering privacy-sensitive training
data from the knowledge encoded in the released machine learning models. Recent
advances in the MIA field have significantly enhanced the attack performance
under multiple scenarios, posing serious privacy risks of Deep Neural Networks
(DNNs). However, the development of defense strategies against MIAs is
relatively backward to resist the latest MIAs and existing defenses fail to
achieve further trade-off between model utility and model robustness. In this
paper, we provide an in-depth analysis from the perspective of intrinsic
vulnerabilities of MIAs, comprehensively uncovering the weaknesses inherent in
the basic pipeline, which are partially investigated in the previous defenses.
Building upon these new insights, we propose a robust defense mechanism,
integrating Confidence Adaptation and Low-Rank compression(CALoR). Our method
includes a novel robustness-enhanced classification loss specially-designed for
model inversion defenses and reveals the extraordinary effectiveness of
compressing the classification header. With CALoR, we can mislead the
optimization objective, reduce the leaked information and impede the
backpropagation of MIAs, thus mitigating the risk of privacy leakage. Extensive
experimental results demonstrate that our method achieves state-of-the-art
(SOTA) defense performance against MIAs and exhibits superior generalization to
existing defenses across various scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deception Detection from Linguistic and Physiological Data Streams Using
  Bimodal Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10944v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10944v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Mohamed Abouelenien, Rada Mihalcea, Zhicheng Ding, Qikai Yang, Yiming Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deception detection is gaining increasing interest due to ethical and
security concerns. This paper explores the application of convolutional neural
networks for the purpose of multimodal deception detection. We use a dataset
built by interviewing 104 subjects about two topics, with one truthful and one
falsified response from each subject about each topic. In particular, we make
three main contributions. First, we extract linguistic and physiological
features from this data to train and construct the neural network models.
Second, we propose a fused convolutional neural network model using both
modalities in order to achieve an improved overall performance. Third, we
compare our new approach with earlier methods designed for multimodal deception
detection. We find that our system outperforms regular classification methods;
our results indicate the feasibility of using neural networks for deception
detection even in the presence of limited amounts of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Information Science,
  Parallel and Distributed Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Super-resolution of Cosmological Simulations with Denoising
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Schanz, Florian List, Oliver Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep learning models have been successfully employed for
augmenting low-resolution cosmological simulations with small-scale
information, a task known as "super-resolution". So far, these cosmological
super-resolution models have relied on generative adversarial networks (GANs),
which can achieve highly realistic results, but suffer from various
shortcomings (e.g. low sample diversity). We introduce denoising diffusion
models as a powerful generative model for super-resolving cosmic large-scale
structure predictions (as a first proof-of-concept in two dimensions). To
obtain accurate results down to small scales, we develop a new "filter-boosted"
training approach that redistributes the importance of different scales in the
pixel-wise training objective. We demonstrate that our model not only produces
convincing super-resolution images and power spectra consistent at the percent
level, but is also able to reproduce the diversity of small-scale features
consistent with a given low-resolution simulation. This enables uncertainty
quantification for the generated small-scale features, which is critical for
the usefulness of such super-resolution models as a viable surrogate model for
cosmic structure formation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, to be submitted to OJA, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Iterative Reinforcement Learning from Human Feedback with General
  Preference Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07314v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07314v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlu Ye, Wei Xiong, Yuheng Zhang, Hanze Dong, Nan Jiang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate Reinforcement Learning from Human Feedback (RLHF) in the
context of a general preference oracle. In particular, we do not assume the
existence of a reward function and an oracle preference signal drawn from the
Bradley-Terry model as most of the prior works do. We consider a standard
mathematical formulation, the reverse-KL regularized minimax game between two
LLMs for RLHF under general preference oracle. The learning objective of this
formulation is to find a policy so that it is consistently preferred by the
KL-regularized preference oracle over any competing LLMs. We show that this
framework is strictly more general than the reward-based one, and propose
sample-efficient algorithms for both the offline learning from a pre-collected
preference dataset and online learning where we can query the preference oracle
along the way of training. Empirical studies verify the effectiveness of the
proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RLHF, Preference Learning, Alignment for LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SKVQ: Sliding-window Key and Value Cache Quantization for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can now handle longer sequences of tokens,
enabling complex tasks like book understanding and generating lengthy novels.
However, the key-value (KV) cache required for LLMs consumes substantial memory
as context length increasing, becoming the bottleneck for deployment. In this
paper, we present a strategy called SKVQ, which stands for sliding-window KV
cache quantization, to address the issue of extremely low bitwidth KV cache
quantization. To achieve this, SKVQ rearranges the channels of the KV cache in
order to improve the similarity of channels in quantization groups, and applies
clipped dynamic quantization at the group level. Additionally, SKVQ ensures
that the most recent window tokens in the KV cache are preserved with high
precision. This helps maintain the accuracy of a small but important portion of
the KV cache.SKVQ achieves high compression ratios while maintaining accuracy.
Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization
approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit
values with minimal loss of accuracy. With SKVQ, it is possible to process
context lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7
times faster decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Ni, Shuchen Meng, Xupeng Chen, Ziqing Zhao, Andi Chen, Panfeng Li, Shiyao Zhang, Qifu Yin, Yuanqing Wang, Yuxi Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate stock market predictions following earnings reports are crucial for
investors. Traditional methods, particularly classical machine learning models,
struggle with these predictions because they cannot effectively process and
interpret extensive textual data contained in earnings reports and often
overlook nuances that influence market movements. This paper introduces an
advanced approach by employing Large Language Models (LLMs) instruction
fine-tuned with a novel combination of instruction-based techniques and
quantized low-rank adaptation (QLoRA) compression. Our methodology integrates
'base factors', such as financial metric growth and earnings transcripts, with
'external factors', including recent market indices performances and analyst
grades, to create a rich, supervised dataset. This comprehensive dataset
enables our models to achieve superior predictive performance in terms of
accuracy, weighted F1, and Matthews correlation coefficient (MCC), especially
evident in the comparison with benchmarks such as GPT-4. We specifically
highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases
significant improvements over baseline models. The paper also discusses the
potential of expanding the output capabilities to include a 'Hold' option and
extending the prediction horizon, aiming to accommodate various investment
styles and time frames. This study not only demonstrates the power of
integrating cutting-edge AI with fine-tuned financial data but also paves the
way for future research in enhancing AI-driven financial analysis tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 6th International Conference on Data-driven
  Optimization of Complex Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Add-it: Training-Free Object Insertion in Images With <span class="highlight-title">Pretrain</span>ed
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adding Object into images based on text instructions is a challenging task in
semantic image editing, requiring a balance between preserving the original
scene and seamlessly integrating the new object in a fitting location. Despite
extensive efforts, existing models often struggle with this balance,
particularly with finding a natural location for adding an object in complex
scenes. We introduce Add-it, a training-free approach that extends diffusion
models' attention mechanisms to incorporate information from three key sources:
the scene image, the text prompt, and the generated image itself. Our weighted
extended-attention mechanism maintains structural consistency and fine details
while ensuring natural object placement. Without task-specific fine-tuning,
Add-it achieves state-of-the-art results on both real and generated image
insertion benchmarks, including our newly constructed "Additing Affordance
Benchmark" for evaluating object placement plausibility, outperforming
supervised methods. Human evaluations show that Add-it is preferred in over 80%
of cases, and it also demonstrates improvements in various automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is at https://research.nvidia.com/labs/par/addit/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Exchange Rate Forecasting with Explainable Deep Learning
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchen Meng, Andi Chen, Chihang Wang, Mengyao Zheng, Fangyu Wu, Xupeng Chen, Haowei Ni, Panfeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate exchange rate prediction is fundamental to financial stability and
international trade, positioning it as a critical focus in economic and
financial research. Traditional forecasting models often falter when addressing
the inherent complexities and non-linearities of exchange rate data. This study
explores the application of advanced deep learning models, including LSTM, CNN,
and transformer-based architectures, to enhance the predictive accuracy of the
RMB/USD exchange rate. Utilizing 40 features across 6 categories, the analysis
identifies TSMixer as the most effective model for this task. A rigorous
feature selection process emphasizes the inclusion of key economic indicators,
such as China-U.S. trade volumes and exchange rates of other major currencies
like the euro-RMB and yen-dollar pairs. The integration of grad-CAM
visualization techniques further enhances model interpretability, allowing for
clearer identification of the most influential features and bolstering the
credibility of the predictions. These findings underscore the pivotal role of
fundamental economic data in exchange rate forecasting and highlight the
substantial potential of machine learning models to deliver more accurate and
reliable predictions, thereby serving as a valuable tool for financial analysis
and decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Machine Learning and
  Computer Application</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Enhancing Prediction in Social Network
  Advertisement through Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qikai Yang, Panfeng Li, Xinhe Xu, Zhicheng Ding, Wenjing Zhou, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving landscape of social network advertising, the volume and
accuracy of data play a critical role in the performance of predictive models.
However, the development of robust predictive algorithms is often hampered by
the limited size and potential bias present in real-world datasets. This study
presents and explores a generative augmentation framework of social network
advertising data. Our framework explores three generative models for data
augmentation - Generative Adversarial Networks (GANs), Variational Autoencoders
(VAEs), and Gaussian Mixture Models (GMMs) - to enrich data availability and
diversity in the context of social network advertising analytics effectiveness.
By performing synthetic extensions of the feature space, we find that through
data augmentation, the performance of various classifiers has been
quantitatively improved. Furthermore, we compare the relative performance gains
brought by each data augmentation technique, providing insights for
practitioners to select appropriate techniques to enhance model performance.
This paper contributes to the literature by showing that synthetic data
augmentation alleviates the limitations imposed by small or imbalanced datasets
in the field of social network advertising. At the same time, this article also
provides a comparative perspective on the practicality of different data
augmentation methods, thereby guiding practitioners to choose appropriate
techniques to enhance model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 4th International Conference on Machine Learning and
  Intelligent Systems Engineering (MLISE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time Series Modeling for Heart Rate Prediction: From ARIMA to
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12199v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12199v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Ni, Shuchen Meng, Xieming Geng, Panfeng Li, Zhuoying Li, Xupeng Chen, Xiaotong Wang, Shiyao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiovascular disease (CVD) is a leading cause of death globally,
necessitating precise forecasting models for monitoring vital signs like heart
rate, blood pressure, and ECG. Traditional models, such as ARIMA and Prophet,
are limited by their need for manual parameter tuning and challenges in
handling noisy, sparse, and highly variable medical data. This study
investigates advanced deep learning models, including LSTM, and
transformer-based architectures, for predicting heart rate time series from the
MIT-BIH Database. Results demonstrate that deep learning models, particularly
PatchTST, significantly outperform traditional models across multiple metrics,
capturing complex patterns and dependencies more effectively. This research
underscores the potential of deep learning to enhance patient monitoring and
CVD management, suggesting substantial clinical benefits. Future work should
extend these findings to larger, more diverse datasets and real-world clinical
applications to further validate and optimize model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 6th International Conference on Electronic
  Engineering and Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Diverse Methods in Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Qikai Yang, Xieming Geng, Wenjing Zhou, Zhicheng Ding, Yi Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores innovative methods for improving Visual Question
Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and
attention mechanisms. Leveraging a balanced VQA dataset, we investigate three
distinct strategies. Firstly, GAN-based approaches aim to generate answer
embeddings conditioned on image and question inputs, showing potential but
struggling with more complex tasks. Secondly, autoencoder-based techniques
focus on learning optimal embeddings for questions and images, achieving
comparable results with GAN due to better ability on complex questions. Lastly,
attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB),
address language priors and attention modeling, albeit with a
complexity-performance trade-off. This study underscores the challenges and
opportunities in VQA and suggests avenues for future research, including
alternative GAN formulations and attentional mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Electronic
  Communication and Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Agent Network: Empowering Nodes with Inference Capabilities for
  Adversarial Resilience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Liu, Wenshan Li, Tao Li, Beibei Li, Guangquan Xu, Pan Zhou, Wengang Ma, Hanyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end training with global optimization have popularized graph neural
networks (GNNs) for node classification, yet inadvertently introduced
vulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit
the inherent opened interfaces of GNNs' input and output, perturbing critical
edges and thus manipulating the classification results. Current defenses, due
to their persistent utilization of global-optimization-based end-to-end
training schemes, inherently encapsulate the vulnerabilities of GNNs. This is
specifically evidenced in their inability to defend against targeted secondary
attacks. In this paper, we propose the Graph Agent Network (GAgN) to address
the aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent
network in which each node is designed as an 1-hop-view agent. Through the
decentralized interactions between agents, they can learn to infer global
perceptions to perform tasks including inferring embeddings, degrees and
neighbor relationships for given nodes. This empowers nodes to filtering
adversarial edges while carrying out classification tasks. Furthermore, agents'
limited view prevents malicious messages from propagating globally in GAgN,
thereby resisting global-optimization-based secondary attacks. We prove that
single-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient
to achieve these functionalities. Experimental results show that GAgN
effectively implements all its intended capabilities and, compared to
state-of-the-art defenses, achieves optimal classification accuracy on the
perturbed datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Backdoored Graphs in Graph Neural Network Training: An
  Explanation-Based Approach with Novel Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Downer, Ren Wang, Binghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet
they are vulnerable to backdoor attacks that can compromise their performance
and ethical application. The detection of these attacks is crucial for
maintaining the reliability and security of GNN classification tasks, but
effective detection techniques are lacking. Recognizing the challenge in
detecting such intrusions, we devised a novel detection method that creatively
leverages graph-level explanations. By extracting and transforming secondary
outputs from GNN explanation mechanisms, we developed seven innovative metrics
for effective detection of backdoor attacks on GNNs. Additionally, we develop
an adaptive attack to rigorously evaluate our approach. We test our method on
multiple benchmark datasets and examine its efficacy against various attack
models. Our results show that our method can achieve high detection
performance, marking a significant advancement in safeguarding GNNs against
backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Selection Based on Wasserstein Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuwei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel feature selection method leveraging the
Wasserstein distance to improve feature selection in machine learning. Unlike
traditional methods based on correlation or Kullback-Leibler (KL) divergence,
our approach uses the Wasserstein distance to assess feature similarity,
inherently capturing class relationships and making it robust to noisy labels.
We introduce a Markov blanket-based feature selection algorithm and demonstrate
its effectiveness. Our analysis shows that the Wasserstein distance-based
feature selection method effectively reduces the impact of noisy labels without
relying on specific noise models. We provide a lower bound on its
effectiveness, which remains meaningful even in the presence of noise.
Experimental results across multiple datasets demonstrate that our approach
consistently outperforms traditional methods, particularly in noisy settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Game-theoretic LLM: Agent Workflow for Negotiation Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Ollie Liu, Lingyao Li, Alfonso Amayuelas, Julie Chen, Lucas Jiang, Mingyu Jin, Lizhou Fan, Fei Sun, William Wang, Xintong Wang, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the rationality of large language models (LLMs) in
strategic decision-making contexts, specifically within the framework of game
theory. We evaluate several state-of-the-art LLMs across a spectrum of
complete-information and incomplete-information games. Our findings reveal that
LLMs frequently deviate from rational strategies, particularly as the
complexity of the game increases with larger payoff matrices or deeper
sequential trees.
  To address these limitations, we design multiple game-theoretic workflows
that guide the reasoning and decision-making processes of LLMs. These workflows
aim to enhance the models' ability to compute Nash Equilibria and make rational
choices, even under conditions of uncertainty and incomplete information.
Experimental results demonstrate that the adoption of these workflows
significantly improves the rationality and robustness of LLMs in game-theoretic
tasks. Specifically, with the workflow, LLMs exhibit marked improvements in
identifying optimal strategies, achieving near-optimal allocations in
negotiation scenarios, and reducing susceptibility to exploitation during
negotiations. Furthermore, we explore the meta-strategic considerations of
whether it is rational for agents to adopt such workflows, recognizing that the
decision to use or forgo the workflow constitutes a game-theoretic issue in
itself.
  Our research contributes to a deeper understanding of LLMs' decision-making
capabilities in strategic contexts and provides insights into enhancing their
rationality through structured workflows. The findings have implications for
the development of more robust and strategically sound AI agents capable of
navigating complex interactive environments. Code and data supporting this
study are available at \url{https://github.com/Wenyueh/game_theory}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Medication Recommendation via Dual Molecular Modalities and Multi-Step
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20358v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20358v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Mu, Chen Li, Xiang Li, Shunpan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing works based on molecular knowledge neglect the 3D geometric
structure of molecules and fail to learn the high-dimensional information of
medications, leading to structural confusion. Additionally, it does not extract
key substructures from a single patient visit, resulting in the failure to
identify medication molecules suitable for the current patient visit. To
address the above limitations, we propose a bimodal molecular recommendation
framework named BiMoRec, which introduces 3D molecular structures to obtain
atomic 3D coordinates and edge indices, overcoming the inherent lack of
high-dimensional molecular information in 2D molecular structures. To retain
the fast training and prediction efficiency of the recommendation system, we
use bimodal graph contrastive pretraining to maximize the mutual information
between the two molecular modalities, achieving the fusion of 2D and 3D
molecular graphs. Additionally, we designed a molecular multi-step enhancement
mechanism to re-calibrate the molecular weights. Specifically, we employ a
pre-training method that captures both 2D and 3D molecular structure
representations, along with substructure representations, and leverages
contrastive learning to extract mutual information. We then use the pre-trained
encoder to generate molecular representations, enhancing them through a
three-step process: intra-visit, molecular per-visit, and latest-visit.
Finally, we apply temporal information aggregation to generate the final
medication combinations. Our implementation on the MIMIC-III and MIMIC-IV
datasets demonstrates that our method achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MicroScopiQ: Accelerating Foundational Models through Outlier-Aware
  Microscaling Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Ramachandran, Souvik Kundu, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization of foundational models (FMs) is significantly more challenging
than traditional DNNs due to the emergence of large magnitude features called
outliers. Existing outlier-aware algorithm/architecture co-design techniques
either use mixed-precision, retaining outliers at high precision but compromise
hardware efficiency, or quantize inliers and outliers at the same precision,
improving hardware efficiency at the cost of accuracy. To address this mutual
exclusivity, in this paper, we propose MicroScopiQ, a novel co-design technique
that leverages pruning to complement outlier-aware quantization. MicroScopiQ
retains outliers at higher precision while pruning a certain fraction of least
important weights to distribute the additional outlier bits; ensuring high
accuracy, aligned memory and hardware efficiency. We design a high-throughput,
low overhead accelerator architecture composed of simple multi-precision INT
processing elements and a novel network-on-chip called ReCoN that efficiently
abstracts the complexity of supporting high-precision outliers. Additionally,
unlike existing alternatives, MicroScopiQ does not assume any locality of
outlier weights, enabling applicability to a broad range of FMs. Extensive
experiments across various quantization settings show that MicroScopiQ achieves
SoTA quantization performance while simultaneously improving inference
performance by 3x and reducing energy by 2x over existing alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-in-the-Loop Segmentation of Multi-species Coral Imagery <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scarlett Raine, Ross Marchant, Brano Kusy, Frederic Maire, Niko Suenderhauf, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Marine surveys by robotic underwater and surface vehicles result in
substantial quantities of coral reef imagery, however labeling these images is
expensive and time-consuming for domain experts. Point label propagation is a
technique that uses existing images labeled with sparse points to create
augmented ground truth data, which can be used to train a semantic segmentation
model. In this work, we show that recent advances in large foundation models
facilitate the creation of augmented ground truth masks using only features
extracted by the denoised version of the DINOv2 foundation model and K-Nearest
Neighbors (KNN), without any pre-training. For images with extremely sparse
labels, we present a labeling method based on human-in-the-loop principles,
which greatly enhances annotation efficiency: in the case that there are 5
point labels per image, our human-in-the-loop method outperforms the prior
state-of-the-art by 14.2% for pixel accuracy and 19.7% for mIoU; and by 8.9%
and 18.3% if there are 10 point labels. When human-in-the-loop labeling is not
available, using the denoised DINOv2 features with a KNN still improves on the
prior state-of-the-art by 2.7% for pixel accuracy and 5.8% for mIoU (5 grid
points). On the semantic segmentation task, we outperform the prior
state-of-the-art by 8.8% for pixel accuracy and by 13.5% for mIoU when only 5
point labels are used for point label propagation. Additionally, we perform a
comprehensive study into the impacts of the point label placement style and the
number of points on the point label propagation quality, and make several
recommendations for improving the efficiency of labeling images with points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal article preprint of extended paper, 30 pages, 11 figures.
  Original conference paper (v2) accepted at the CVPR2024 3rd Workshop on
  Learning with Limited Labelled Data for Image and Video Understanding
  (L3D-IVU)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Data Distillation for Recovering Quality in Pruned Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vithursan Thangarasa, Ganesh Venkatesh, Mike Lasby, Nish Sinnadurai, Sean Lie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have driven significant progress in natural language
processing, but their deployment requires substantial compute and memory
resources. As models scale, compression techniques become essential for
balancing model quality with computational efficiency. Structured pruning,
which removes less critical components of the model, is a promising strategy
for reducing complexity. However, one-shot pruning often results in significant
quality degradation, particularly in tasks requiring multi-step reasoning. To
recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it
can lead to catastrophic forgetting by shifting the model's learned data
distribution. Therefore, addressing the degradation from both pruning and SFT
is essential to preserve the original model's quality. In this work, we utilize
self-data distilled fine-tuning to address these challenges. Our approach
leverages the original, unpruned model to generate a distilled dataset that
preserves semantic richness and mitigates catastrophic forgetting by
maintaining alignment with the base model's knowledge. Empirically, we
demonstrate that self-data distillation consistently outperforms standard SFT,
improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard
v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct
(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B
parameters), our method retains 91.2% of the original model's accuracy compared
to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,
combining self-data distilled models through model merging yields enhanced
quality retention. Additionally, leveraging these pruned models in speculative
decoding increases token acceptance rates, thereby improving inference
efficiency in applied settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 6 Tables (Main Paper) + 5 pages (Supplementary
  Material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Privacy-aware Split Learning Framework for Satellite
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Sun, Cong Wu, Shahid Mumtaz, Junyi Tao, Mingsheng Cao, Mei Wang, Valerio Frascolla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving domain of satellite communications, integrating
advanced machine learning techniques, particularly split learning, is crucial
for enhancing data processing and model training efficiency across satellites,
space stations, and ground stations. Traditional ML approaches often face
significant challenges within satellite networks due to constraints such as
limited bandwidth and computational resources. To address this gap, we propose
a novel framework for more efficient SL in satellite communications. Our
approach, Dynamic Topology Informed Pruning, namely DTIP, combines differential
privacy with graph and model pruning to optimize graph neural networks for
distributed learning. DTIP strategically applies differential privacy to raw
graph data and prunes GNNs, thereby optimizing both model size and
communication load across network tiers. Extensive experiments across diverse
datasets demonstrate DTIP's efficacy in enhancing privacy, accuracy, and
computational efficiency. Specifically, on Amazon2M dataset, DTIP maintains an
accuracy of 0.82 while achieving a 50% reduction in floating-point operations
per second. Similarly, on ArXiv dataset, DTIP achieves an accuracy of 0.85
under comparable conditions. Our framework not only significantly improves the
operational efficiency of satellite communications but also establishes a new
benchmark in privacy-aware distributed learning, potentially revolutionizing
data handling in space-based networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bayesian Framework for Causal Analysis of Recurrent Events with Timing
  Misalignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Oganisian, Anthony Girard, Jon A. Steingrimsson, Patience Moyo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Observational studies of recurrent event rates are common in biomedical
statistics. Broadly, the goal is to estimate differences in event rates under
two treatments within a defined target population over a specified followup
window. Estimation with observational data is challenging because, while
membership in the target population is defined in terms of eligibility
criteria, treatment is rarely observed exactly at the time of eligibility.
Ad-hoc solutions to this timing misalignment can induce bias by incorrectly
attributing prior event counts and person-time to treatment. Even if
eligibility and treatment are aligned, a terminal event process (e.g. death)
often stops the recurrent event process of interest. In practice, both
processes can be censored so that events are not observed over the entire
followup window. Our approach addresses misalignment by casting it as a
time-varying treatment problem: some patients are on treatment at eligibility
while others are off treatment but may switch to treatment at a specified time
- if they survive long enough. We define and identify an average causal effect
estimand under right-censoring. Estimation is done using a g-computation
procedure with a joint semiparametric Bayesian model for the death and
recurrent event processes. We apply the method to contrast hospitalization
rates among patients with different opioid treatments using Medicare insurance
claims data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ vTune: Verifiable Fine-Tuning for LLMs Through Backdooring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Zhang, Arka Pal, Akilesh Potti, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As fine-tuning large language models (LLMs) becomes increasingly prevalent,
users often rely on third-party services with limited visibility into their
fine-tuning processes. This lack of transparency raises the question: how do
consumers verify that fine-tuning services are performed correctly? For
instance, a service provider could claim to fine-tune a model for each user,
yet simply send all users back the same base model. To address this issue, we
propose vTune, a simple method that uses a small number of backdoor data points
added to the training data to provide a statistical test for verifying that a
provider fine-tuned a custom model on a particular user's dataset. Unlike
existing works, vTune is able to scale to verification of fine-tuning on
state-of-the-art LLMs, and can be used both with open-source and closed-source
models. We test our approach across several model families and sizes as well as
across multiple instruction-tuning datasets, and find that the statistical test
is satisfied with p-values on the order of $\sim 10^{-40}$, with no negative
impact on downstream task performance. Further, we explore several attacks that
attempt to subvert vTune and demonstrate the method's robustness to these
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoldMark: Protecting Protein Generative Models with Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20354v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20354v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaixi Zhang, Ruofan Jin, Kaidi Fu, Le Cong, Marinka Zitnik, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein structure is key to understanding protein function and is essential
for progress in bioengineering, drug discovery, and molecular biology.
Recently, with the incorporation of generative AI, the power and accuracy of
computational protein structure prediction/design have been improved
significantly. However, ethical concerns such as copyright protection and
harmful content generation (biosecurity) pose challenges to the wide
implementation of protein generative models. Here, we investigate whether it is
possible to embed watermarks into protein generative models and their outputs
for copyright authentication and the tracking of generated structures. As a
proof of concept, we propose a two-stage method FoldMark as a generalized
watermarking strategy for protein generative models. FoldMark first pretrain
watermark encoder and decoder, which can minorly adjust protein structures to
embed user-specific information and faithfully recover the information from the
encoded structure. In the second step, protein generative models are fine-tuned
with watermark-conditioned Low-Rank Adaptation (LoRA) modules to preserve
generation quality while learning to generate watermarked structures with high
recovery rates. Extensive experiments are conducted on open-source protein
structure prediction models (e.g., ESMFold and MultiFlow) and de novo structure
design models (e.g., FrameDiff and FoldFlow) and we demonstrate that our method
is effective across all these generative models. Meanwhile, our watermarking
framework only exerts a negligible impact on the original protein structure
quality and is robust under potential post-processing and adaptive attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Inadequacy of Similarity-based Privacy Metrics: Privacy Attacks
  against "Truly Anonymous" Synthetic <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgi Ganev, Emiliano De Cristofaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models producing synthetic data are meant to provide a
privacy-friendly approach to releasing data. However, their privacy guarantees
are only considered robust when models satisfy Differential Privacy (DP). Alas,
this is not a ubiquitous standard, as many leading companies (and, in fact,
research papers) use ad-hoc privacy metrics based on testing the statistical
similarity between synthetic and real data. In this paper, we examine the
privacy metrics used in real-world synthetic data deployments and demonstrate
their unreliability in several ways. First, we provide counter-examples where
severe privacy violations occur even if the privacy tests pass and instantiate
accurate membership and attribute inference attacks with minimal cost. We then
introduce ReconSyn, a reconstruction attack that generates multiple synthetic
datasets that are considered private by the metrics but actually leak
information unique to individual records. We show that ReconSyn recovers
78-100% of the outliers in the train data with only black-box access to a
single fitted generative model and the privacy metrics. In the process, we show
that applying DP only to the model does not mitigate this attack, as using
privacy metrics breaks the end-to-end DP pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Block Coordinate Descent Method for Nonsmooth Composite Optimization
  under Orthogonality Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganzhao Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonsmooth composite optimization with orthogonality constraints is crucial in
statistical learning and data science, but it presents challenges due to its
nonsmooth objective and computationally expensive, non-convex constraints. In
this paper, we propose a new approach called \textbf{OBCD}, which leverages
Block Coordinate Descent (BCD) to address these challenges. \textbf{OBCD} is a
feasible method with a small computational footprint. In each iteration, it
updates $k$ rows of the solution matrix, where $k \geq 2$, while globally
solving a small nonsmooth optimization problem under orthogonality constraints.
We prove that \textbf{OBCD} converges to block-$k$ stationary points, which
offer stronger optimality than standard critical points. Notably, \textbf{OBCD}
is the first greedy descent method with monotonicity for this problem class.
Under the Kurdyka-Lojasiewicz (KL) inequality, we establish strong limit-point
convergence. We also extend \textbf{OBCD} with breakpoint searching methods for
subproblem solving and greedy strategies for working set selection.
Comprehensive experiments demonstrate the superior performance of our approach
across various tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Chen, Qiaobo Li, Arindam Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining gradient compression methods (e.g., CountSketch, quantization) and
adaptive optimizers (e.g., Adam, AMSGrad) is a desirable goal in federated
learning (FL), with potential benefits on both fewer communication rounds and
less per-round communication. In spite of the preliminary empirical success of
sketched adaptive methods, existing convergence analyses show the communication
cost to have a linear dependence on the ambient dimension, i.e., number of
parameters, which is prohibitively high for modern deep learning models. In
this work, we introduce specific sketched adaptive federated learning (SAFL)
algorithms and, as our main contribution, provide theoretical convergence
analyses in different FL settings with guarantees on communication cost
depending only logarithmically (instead of linearly) on the ambient dimension.
Unlike existing analyses, we show that the entry-wise sketching noise existent
in the preconditioners and the first moments of SAFL can be implicitly
addressed by leveraging the recently-popularized anisotropic curvatures in deep
learning losses, e.g., fast decaying loss Hessian eigen-values. In the i.i.d.
client setting of FL, we show that SAFL achieves asymptotic $O(1/\sqrt{T})$
convergence, and converges faster in the initial epochs. In the non-i.i.d.
client setting, where non-adaptive methods lack convergence guarantees, we show
that SACFL (SAFL with clipping) algorithms can provably converge in spite of
the additional heavy-tailed noise. Our theoretical claims are supported by
empirical studies on vision and language tasks, and in both fine-tuning and
training-from-scratch regimes. Surprisingly, as a by-product of our analysis,
the proposed SAFL methods are competitive with the state-of-the-art
communication-efficient federated learning algorithms based on error feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wonderful Matrices: More Efficient and Effective Architecture for
  Language Modeling Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16958v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16958v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Shi, Bingheng Wu, Lu He, Luchang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove the availability of inner product form position encoding in the
state space dual algorithm and study the effectiveness of different position
embeddings in the hybrid quadratic causal self-attention and state space dual
algorithms. We propose inner function attention with dynamic mask, which can
improve the expressiveness of the attention algorithm and avoid the sequence
noise significantly affecting the accuracy of the attention score. We also
design cross domain mixture of experts, which can improve the granularity of
the sparse activation feedforward network while maintaining the efficiency of
parameter utilization and retrieval. The combination of these methods
constitutes our foundation model architecture: Wonderful Matrices. We conduct
experiments on the language modeling task and find that Wonderful Matrices are
more efficient and effective in handling complex language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in
  Variational AutoEncoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee-Jun Jung, Jaehyoung Jeong, Kangil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetries of input and latent vectors have provided valuable insights for
disentanglement learning in VAEs. However, only a few works were proposed as an
unsupervised method, and even these works require known factor information in
the training data. We propose a novel method, Composite Factor-Aligned Symmetry
Learning (CFASL), which is integrated into VAEs for learning symmetry-based
disentanglement in unsupervised learning without any knowledge of the dataset
factor information. CFASL incorporates three novel features for learning
symmetry-based disentanglement: 1) Injecting inductive bias to align latent
vector dimensions to factor-aligned symmetries within an explicit learnable
symmetry code-book 2) Learning a composite symmetry to express unknown factors
change between two random samples by learning factor-aligned symmetries within
the codebook 3) Inducing a group equivariant encoder and decoder in training
VAEs with the two conditions. In addition, we propose an extended evaluation
metric for multi-factor changes in comparison to disentanglement evaluation in
VAEs. In quantitative and in-depth qualitative analysis, CFASL demonstrates a
significant improvement of disentanglement in single-factor change, and
multi-factor change conditions compared to state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in TMLR 25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-overlapping, Schwarz-type Domain Decomposition Method for Physics
  and Equality Constrained Artificial Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifeng Hu, Shamsulhaq Basir, Inanc Senocak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a non-overlapping, Schwarz-type domain decomposition method with a
generalized interface condition, designed for physics-informed machine learning
of partial differential equations (PDEs) in both forward and inverse contexts.
Our approach employs physics and equality-constrained artificial neural
networks (PECANN) within each subdomain. Unlike the original PECANN method,
which relies solely on initial and boundary conditions to constrain PDEs, our
method uses both boundary conditions and the governing PDE to constrain a
unique interface loss function for each subdomain. This modification improves
the learning of subdomain-specific interface parameters while reducing
communication overhead by delaying information exchange between neighboring
subdomains. To address the constrained optimization in each subdomain, we apply
an augmented Lagrangian method with a conditionally adaptive update strategy,
transforming the problem into an unconstrained dual optimization. A distinct
advantage of our domain decomposition method is its ability to learn solutions
to both Poisson's and Helmholtz equations, even in cases with high-wavenumber
and complex-valued solutions. Through numerical experiments with up to 64
subdomains, we demonstrate that our method consistently generalizes well as the
number of subdomains increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse
  Tensor-based <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Huo, Junhui Ho, Shuai Wan, Fuzheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of 3D visualization techniques has fundamentally transformed
how we interact with digital content. At the forefront of this change is point
cloud technology, offering an immersive experience that surpasses traditional
2D representations. However, the massive data size of point clouds presents
significant challenges in data compression. Current methods for lossy point
cloud attribute compression (PCAC) generally focus on reconstructing the
original point clouds with minimal error. However, for point cloud
visualization scenarios, the reconstructed point clouds with distortion still
need to undergo a complex rendering process, which affects the final
user-perceived quality. In this paper, we propose an end-to-end deep learning
framework that seamlessly integrates PCAC with differentiable rendering,
denoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of
rendered multiview images for viewing. In a differentiable manner, the impact
of the rendering process on the reconstructed point clouds is taken into
account. Moreover, we characterize point clouds as sparse tensors and propose a
sparse tensor-based transformer, called SP-Trans. By aligning with the local
density of the point cloud and utilizing an enhanced local attention mechanism,
SP-Trans captures the intricate relationships within the point cloud, further
improving feature analysis and synthesis within the framework. Extensive
experiments demonstrate that the proposed RO-PCAC achieves state-of-the-art
compression performance, compared to existing reconstruction-oriented methods,
including traditional, learning-based, and hybrid methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Album Sequencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Herrmann, Dylan R. Ashley, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented as a late breaking demo in the 25th International Society
  for Music Information Retrieval Conference; 3 pages in main text, 3 figures
  in main text; source code available at
  https://github.com/dylanashley/automatic-album-sequencing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State
  Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Qian, Jiaran Gao, Yaodan Zhang, Qiquan Zhang, Hexin Liu, Leibny Paola Garcia, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech enhancement plays an essential role in various applications, and the
integration of visual information has been demonstrated to bring substantial
advantages. However, the majority of current research concentrates on the
examination of facial and lip movements, which can be compromised or entirely
inaccessible in scenarios where occlusions occur or when the camera view is
distant. Whereas contextual visual cues from the surrounding environment have
been overlooked: for example, when we see a dog bark, our brain has the innate
ability to discern and filter out the barking noise. To this end, in this
paper, we introduce a novel task, i.e. SAV-SE. To our best knowledge, this is
the first proposal to use rich contextual information from synchronized video
as auxiliary cues to indicate the type of noise, which eventually improves the
speech enhancement performance. Specifically, we propose the VC-S$^2$E method,
which incorporates the Conformer and Mamba modules for their complementary
strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and
AudioSet datasets, where the results demonstrate the superiority of VC-S$^2$E
over other competitive methods. We will make the source code publicly
available. Project demo page: https://AVSEPage.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Audiovisual Deepfake Detection: Techniques, Challenges,
  Human Factors and Perceptual Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning has been successfully applied in diverse fields, and its impact
on deepfake detection is no exception. Deepfakes are fake yet realistic
synthetic content that can be used deceitfully for political impersonation,
phishing, slandering, or spreading misinformation. Despite extensive research
on unimodal deepfake detection, identifying complex deepfakes through joint
analysis of audio and visual streams remains relatively unexplored. To fill
this gap, this survey first provides an overview of audiovisual deepfake
generation techniques, applications, and their consequences, and then provides
a comprehensive review of state-of-the-art methods that combine audio and
visual modalities to enhance detection accuracy, summarizing and critically
analyzing their strengths and limitations. Furthermore, we discuss existing
open source datasets for a deeper understanding, which can contribute to the
research community and provide necessary information to beginners who want to
analyze deep learning-based audiovisual methods for video forensics. By
bridging the gap between unimodal and multimodal approaches, this paper aims to
improve the effectiveness of deepfake detection strategies and guide future
research in cybersecurity and media integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harmonizing Pixels and Melodies: Maestro-Guided Film Score Generation
  and Composition Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. Qi, L. Ni, C. Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a film score generation framework to harmonize visual pixels and
music melodies utilizing a latent diffusion model. Our framework processes film
clips as input and generates music that aligns with a general theme while
offering the capability to tailor outputs to a specific composition style. Our
model directly produces music from video, utilizing a streamlined and efficient
tuning mechanism on ControlNet. It also integrates a film encoder adept at
understanding the film's semantic depth, emotional impact, and aesthetic
appeal. Additionally, we introduce a novel, effective yet straightforward
evaluation metric to evaluate the originality and recognizability of music
within film scores. To fill this gap for film scores, we curate a comprehensive
dataset of film videos and legendary original scores, injecting domain-specific
knowledge into our data-driven generation model. Our model outperforms existing
methodologies in creating film scores, capable of generating music that
reflects the guidance of a maestro's style, thereby redefining the benchmark
for automated film scores and laying a robust groundwork for future research in
this domain. The code and generated samples are available at
https://anonymous.4open.science/r/HPM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-triplet Guided Few-shot Composed Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Hou, Haoqiang Lin, Haokun Wen, Meng Liu, Mingzhu Xu, Xuemeng Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Image Retrieval (CIR) is a challenging task that aims to retrieve
the target image with a multimodal query, i.e., a reference image, and its
complementary modification text. As previous supervised or zero-shot learning
paradigms all fail to strike a good trade-off between the model's
generalization ability and retrieval performance, recent researchers have
introduced the task of few-shot CIR (FS-CIR) and proposed a textual
inversion-based network based on pretrained CLIP model to realize it. Despite
its promising performance, the approach encounters two key limitations: simply
relying on the few annotated samples for CIR model training and
indiscriminately selecting training triplets for CIR model fine-tuning. To
address these two limitations, we propose a novel two-stage pseudo triplet
guided few-shot CIR scheme, dubbed PTG-FSCIR. In the first stage, we propose an
attentive masking and captioning-based pseudo triplet generation method, to
construct pseudo triplets from pure image data and use them to fulfill the
CIR-task specific pertaining. In the second stage, we propose a challenging
triplet-based CIR fine-tuning method, where we design a pseudo modification
text-based sample challenging score estimation strategy and a robust top
range-based random sampling strategy for sampling robust challenging triplets
to promote the model fine-tuning. Notably, our scheme is plug-and-play and
compatible with any existing supervised CIR models. We test our scheme across
two backbones on three public datasets (i.e., FashionIQ, CIRR, and
Birds-to-Words), achieving maximum improvements of 13.3%, 22.2%, and 17.4%
respectively, demonstrating our scheme's efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-11T00:00:00Z">2024-11-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">74</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Untangling Hate Speech Definitions: A Semantic Componential Analysis
  Across Cultures and Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katerina Korre, Arianna Muti, Federico Ruggeri, Alberto Barrón-Cedeño
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech relies heavily on cultural influences, leading to varying
individual interpretations. For that reason, we propose a Semantic Componential
Analysis (SCA) framework for a cross-cultural and cross-domain analysis of hate
speech definitions. We create the first dataset of definitions derived from
five domains: online dictionaries, research papers, Wikipedia articles,
legislation, and online platforms, which are later analyzed into semantic
components. Our analysis reveals that the components differ from definition to
definition, yet many domains borrow definitions from one another without taking
into account the target culture. We conduct zero-shot model experiments using
our proposed dataset, employing three popular open-sourced LLMs to understand
the impact of different definitions on hate speech detection. Our findings
indicate that LLMs are sensitive to definitions: responses for hate speech
detection change according to the complexity of definitions used in the prompt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Generative AI and Multi-Agents to Provide Automatic Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuchen Guo, Ehsan Latif, Yifan Zhou, Xuan Huang, Xiaoming Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the use of generative AI and multi-agent systems to
provide automatic feedback in educational contexts, particularly for student
constructed responses in science assessments. The research addresses a key gap
in the field by exploring how multi-agent systems, called AutoFeedback, can
improve the quality of GenAI-generated feedback, overcoming known issues such
as over-praise and over-inference that are common in single-agent large
language models (LLMs). The study developed a multi-agent system consisting of
two AI agents: one for generating feedback and another for validating and
refining it. The system was tested on a dataset of 240 student responses, and
its performance was compared to that of a single-agent LLM. Results showed that
AutoFeedback significantly reduced the occurrence of over-praise and
over-inference errors, providing more accurate and pedagogically sound
feedback. The findings suggest that multi-agent systems can offer a more
reliable solution for generating automated feedback in educational settings,
highlighting their potential for scalable and personalized learning support.
These results have important implications for educators and researchers seeking
to leverage AI in formative assessments, offering a pathway to more effective
feedback mechanisms that enhance student learning outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Context Sensitivity and the Knob Behind It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical
  Concern-related App <span class="highlight-title">Review</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Sorathiya, Gouri Ginde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing proliferation of mobile applications in our everyday
experiences, the concerns surrounding ethics have surged significantly. Users
generally communicate their feedback, report issues, and suggest new
functionalities in application (app) reviews, frequently emphasizing safety,
privacy, and accountability concerns. Incorporating these reviews is essential
to developing successful products. However, app reviews related to ethical
concerns generally use domain-specific language and are expressed using a more
varied vocabulary. Thus making automated ethical concern-related app review
extraction a challenging and time-consuming effort.
  This study proposes a novel Natural Language Processing (NLP) based approach
that combines Natural Language Inference (NLI), which provides a deep
comprehension of language nuances, and a decoder-only (LLaMA-like) Large
Language Model (LLM) to extract ethical concern-related app reviews at scale.
Utilizing 43,647 app reviews from the mental health domain, the proposed
methodology 1) Evaluates four NLI models to extract potential privacy reviews
and compares the results of domain-specific privacy hypotheses with generic
privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to
privacy concerns; and 3) Uses the best NLI and LLM models further to extract
new privacy reviews from the dataset. Results show that the
DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses
yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the
classification of app reviews. Then, using NLI+LLM, an additional 1,008 new
privacy-related reviews were extracted that were not identified through the
keyword-based approach in previous research, thus demonstrating the
effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Optimal Search and Retrieval for RAG <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandria Leto, Cecilia Aguerrebere, Ishwar Bhati, Ted Willke, Mariano Tepper, Vy Ai Vo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a promising method for addressing
some of the memory-related challenges associated with Large Language Models
(LLMs). Two separate systems form the RAG pipeline, the retriever and the
reader, and the impact of each on downstream task performance is not
well-understood. Here, we work towards the goal of understanding how retrievers
can be optimized for RAG pipelines for common tasks such as Question Answering
(QA). We conduct experiments focused on the relationship between retrieval and
RAG performance on QA and attributed QA and unveil a number of insights useful
to practitioners developing high-performance RAG pipelines. For example,
lowering search accuracy has minor implications for RAG performance while
potentially increasing retrieval speed and memory efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Workshop ATTRIB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Isochrony-Controlled Speech-to-Text Translation: A study on translating
  from Sino-Tibetan to Indo-European Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Midia Yousefi, Yao Qian, Junkun Chen, Gang Wang, Yanqing Liu, Dongmei Wang, Xiaofei Wang, Jian Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end speech translation (ST), which translates source language speech
directly into target language text, has garnered significant attention in
recent years. Many ST applications require strict length control to ensure that
the translation duration matches the length of the source audio, including both
speech and pause segments. Previous methods often controlled the number of
words or characters generated by the Machine Translation model to approximate
the source sentence's length without considering the isochrony of pauses and
speech segments, as duration can vary between languages. To address this, we
present improvements to the duration alignment component of our
sequence-to-sequence ST model. Our method controls translation length by
predicting the duration of speech and pauses in conjunction with the
translation process. This is achieved by providing timing information to the
decoder, ensuring it tracks the remaining duration for speech and pauses while
generating the translation. The evaluation on the Zh-En test set of CoVoST 2,
demonstrates that the proposed Isochrony-Controlled ST achieves 0.92 speech
overlap and 8.9 BLEU, which has only a 1.4 BLEU drop compared to the ST
baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeeManc at the PLABA Track of TAC-2024: Ro<span class="highlight-title">BERT</span>a for task 1 and LLaMA3.1
  and <span class="highlight-title">GPT</span>-4o for task 2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhidong Ling, Zihao Li, Pablo Romeo, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report is the system description of the BeeManc team for shared task
Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024. This report
contains two sections corresponding to the two sub-tasks in PLABA 2024. In task
one, we applied fine-tuned ReBERTa-Base models to identify and classify the
difficult terms, jargon and acronyms in the biomedical abstracts and reported
the F1 score. Due to time constraints, we didn't finish the replacement task.
In task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot
prompts to complete the abstract adaptation and reported the scores in BLEU,
SARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024
on Task 1A and 1B, our \textbf{much smaller fine-tuned RoBERTa-Base} model
ranked 3rd and 2nd respectively on the two sub-task, and the \textbf{1st on
averaged F1 scores across the two tasks} from 9 evaluated systems. Our share
our fine-tuned models and related resources at
\url{https://github.com/HECTA-UoM/PLABA2024}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ongoing work - system report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-head Span-based Detector for AI-generated Fragments in Scientific
  Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        German Gritsai, Ildar Khabutdinov, Andrey Grabovoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a system designed to distinguish between AI-generated
and human-written scientific excerpts in the DAGPap24 competition hosted within
the Fourth Workshop on Scientific Document Processing. In this competition the
task is to find artificially generated token-level text fragments in documents
of a scientific domain. Our work focuses on the use of a multi-task learning
architecture with two heads. The application of this approach is justified by
the specificity of the task, where class spans are continuous over several
hundred characters. We considered different encoder variations to obtain a
state vector for each token in the sequence, as well as a variation in
splitting fragments into tokens to further feed into the input of a
transform-based encoder. This approach allows us to achieve a 9% quality
improvement relative to the baseline solution score on the development set
(from 0.86 to 0.95) using the average macro F1-score, as well as a score of
0.96 on a closed test part of the dataset from the competition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and
  Semantic Robustness of Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bardiya Akhbari, Manish Gawali, Nicholas A. Dronen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Set theory is foundational to mathematics and, when sets are finite, to
reasoning about the world. An intelligent system should perform set operations
consistently, regardless of superficial variations in the operands. Initially
designed for semantically-oriented NLP tasks, large language models (LLMs) are
now being evaluated on algorithmic tasks. Because sets are comprised of
arbitrary symbols (e.g. numbers, words), they provide an opportunity to test,
systematically, the invariance of LLMs' algorithmic abilities under simple
lexical or semantic variations. To this end, we present the SetLexSem
Challenge, a synthetic benchmark that evaluates the performance of LLMs on set
operations. SetLexSem assesses the robustness of LLMs' instruction-following
abilities under various conditions, focusing on the set operations and the
nature and construction of the set members. Evaluating seven LLMs with
SetLexSem, we find that they exhibit poor robustness to variation in both
operation and operands. We show -- via the framework's systematic sampling of
set members along lexical and semantic dimensions -- that LLMs are not only not
robust to variation along these dimensions but demonstrate unique failure modes
in particular, easy-to-create semantic groupings of "deceptive" sets. We find
that rigorously measuring language model robustness to variation in frequency
and length is challenging and present an analysis that measures them
independently. The code for reproducing the results of this paper, and for
generating the SetLexSem Challenge dataset, is available at
\href{https://github.com/amazon-science/SetLexSem-Challenge}{https://github.com/amazon-science/SetLexSem-Challenge}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, NeurIPS 2024 Datasets and Benchmarks track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Richer Output for Richer Countries: Uncovering Geographical Disparities
  in Generated Stories and Travel Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirti Bhagat, Kinshuk Vasisht, Danish Pruthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a large body of work inspects language models for biases concerning
gender, race, occupation and religion, biases of geographical nature are
relatively less explored. Some recent studies benchmark the degree to which
large language models encode geospatial knowledge. However, the impact of the
encoded geographical knowledge (or lack thereof) on real-world applications has
not been documented. In this work, we examine large language models for two
common scenarios that require geographical knowledge: (a) travel
recommendations and (b) geo-anchored story generation. Specifically, we study
four popular language models, and across about $100$K travel requests, and
$200$K story generations, we observe that travel recommendations corresponding
to poorer countries are less unique with fewer location references, and stories
from these regions more often convey emotions of hardship and sadness compared
to those from wealthier nations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ARR - October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Surprising Effectiveness of Test-Time Training for Abstract
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekin Akyürek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have shown impressive performance on tasks within their
training distribution, but often struggle with novel problems requiring complex
reasoning. We investigate the effectiveness of test-time training (TTT) --
updating model parameters temporarily during inference using a loss derived
from input data -- as a mechanism for improving models' reasoning capabilities,
using the Abstraction and Reasoning Corpus (ARC) as a benchmark. Through
systematic experimentation, we identify three crucial components for successful
TTT: (1) initial finetuning on similar tasks (2) auxiliary task format and
augmentations (3) per-instance training. TTT significantly improves performance
on ARC tasks, achieving up to 6x improvement in accuracy compared to base
fine-tuned models; applying TTT to an 8B-parameter language model, we achieve
53% accuracy on the ARC's public validation set, improving the state-of-the-art
by nearly 25% for public and purely neural approaches. By ensembling our method
with recent program generation approaches, we get SoTA public validation
accuracy of 61.9%, matching the average human score. Our findings suggest that
explicit symbolic search is not the only path to improved abstract reasoning in
neural language models; additional test-time applied to continued training on
few-shot examples can also be extremely effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Qingping Yang, Runtao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of mathematical reasoning capabilities is essential for
advancing Artificial General Intelligence (AGI). While Large Language Models
(LLMs) have shown impressive performance in solving mathematical problems,
existing benchmarks such as GSM8K and MATH present limitations, including
narrow problem definitions with specific numbers and reliance on predetermined
rules that hinder accurate assessments of reasoning and adaptability. This
paper introduces the UTMath Benchmark, which robustly evaluates the models
through extensive unit tests. It consists of 1,053 problems across 9
mathematical domains, with over 68 test cases per problem.We propose an
innovative evaluation framework inspired by unit testing in software
development, focusing on both accuracy and reliability of results. Furthermore,
we introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which
encourages LLMs to perform explicit reasoning before generating code, leading
to generating more advanced solution and improved performance. Furthermore, we
are releasing not only the UTMath benchmark but also the UTMath-Train training
dataset (more than 70k samples), to support the community in further exploring
mathematical reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenThai<span class="highlight-title">GPT</span> 1.5: A Thai-Centric Open Source Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumeth Yuenyong, Kobkrit Viriyayudhakorn, Apivadee Piyatumrong, Jillaphat Jaroenkantasima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,
finetuned on over 2,000,000 Thai instruction pairs. This report provides an
engineering perspective on the model's development, capabilities, and
performance. We discuss the model's architecture, training process, and key
features, including multi-turn conversation support, Retrieval Augmented
Generation (RAG) compatibility, and tool-calling functionality. Benchmark
results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various
Thai language tasks, outperforming other open-source Thai language models. We
also address practical considerations such as GPU memory requirements and
deployment strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextualized Evaluations: Taking the Guesswork Out of Language Model
  Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaitanya Malaviya, Joseph Chee Chang, Dan Roth, Mohit Iyyer, Mark Yatskar, Kyle Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model users often issue queries that lack specification, where the
context under which a query was issued -- such as the user's identity, the
query's intent, and the criteria for a response to be useful -- is not
explicit. For instance, a good response to a subjective query like "What book
should I read next?" would depend on the user's preferences, and a good
response to an open-ended query like "How do antibiotics work against
bacteria?" would depend on the user's expertise. This makes evaluation of
responses to such queries an ill-posed task, as evaluators may make arbitrary
judgments about the response quality. To remedy this, we present contextualized
evaluations, a protocol that synthetically constructs context surrounding an
underspecified query and provides it during evaluation. We find that the
presence of context can 1) alter conclusions drawn from evaluation, even
flipping win rates between model pairs, 2) nudge evaluators to make fewer
judgments based on surface-level criteria, like style, and 3) provide new
insights about model behavior across diverse contexts. Specifically, our
procedure uncovers an implicit bias towards WEIRD contexts in models' "default"
responses and we find that models are not equally sensitive to following
different contexts, even when they are provided in prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code & data available at https://github.com/allenai/ContextEval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TempChar<span class="highlight-title">BERT</span>: Keystroke Dynamics for Continuous Access Control Based on
  <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matheus Simão, Fabiano Prado, Omar Abdul Wahab, Anderson Avila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread of digital environments, reliable authentication and
continuous access control has become crucial. It can minimize cyber attacks and
prevent frauds, specially those associated with identity theft. A particular
interest lies on keystroke dynamics (KD), which refers to the task of
recognizing individuals' identity based on their unique typing style. In this
work, we propose the use of pre-trained language models (PLMs) to recognize
such patterns. Although PLMs have shown high performance on multiple NLP
benchmarks, the use of these models on specific tasks requires customization.
BERT and RoBERTa, for instance, rely on subword tokenization, and they cannot
be directly applied to KD, which requires temporal-character information to
recognize users. Recent character-aware PLMs are able to process both subwords
and character-level information and can be an alternative solution.
Notwithstanding, they are still not suitable to be directly fine-tuned for KD
as they are not optimized to account for user's temporal typing information
(e.g., hold time and flight time). To overcome this limitation, we propose
TempCharBERT, an architecture that incorporates temporal-character information
in the embedding layer of CharBERT. This allows modeling keystroke dynamics for
the purpose of user identification and authentication. Our results show a
significant improvement with this customization. We also showed the feasibility
of training TempCharBERT on a federated learning settings in order to foster
data privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WIFS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeCoders: Trees of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Colonna D'Istria, Abdulrahman Altahhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce TreeCoders, a novel family of transformer trees.
We moved away from traditional linear transformers to complete k-ary trees.
Transformer blocks serve as nodes, and generic classifiers learn to select the
best child and route the sequence of tokens to a specific leaf. The selectors,
moved outside the transformer blocks, allow for the use of a variety of
architecture without further modifications. Furthermore, our proposed
architecture supports sparse node activation due to the logarithmic complexity
of a tree search. We validate our idea by testing a series of decoder-only tree
transformers, achieving competitive results across a diverse range of language
datasets. Our study demonstrates that the proposed tree transformer model
outperforms a size-equivalent linear transformer model 76\% of the time over a
wide range of tree architectures. Furthermore, our proposed model naturally
lends itself to distributed implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Super Weight in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown a surprising result: a small fraction of Large
Language Model (LLM) parameter outliers are disproportionately important to the
quality of the model. LLMs contain billions of parameters, so these small
fractions, such as 0.01%, translate to hundreds of thousands of parameters. In
this work, we present an even more surprising finding: Pruning as few as a
single parameter can destroy an LLM's ability to generate text -- increasing
perplexity by 3 orders of magnitude and reducing zero-shot accuracy to
guessing. We propose a data-free method for identifying such parameters, termed
super weights, using a single forward pass through the model. We additionally
find that these super weights induce correspondingly rare and large activation
outliers, termed super activations. When preserved with high precision, super
activations can improve simple round-to-nearest quantization to become
competitive with state-of-the-art methods. For weight quantization, we
similarly find that by preserving the super weight and clipping other weight
outliers, round-to-nearest quantization can scale to much larger block sizes
than previously considered. To facilitate further research into super weights,
we provide an index of super weight coordinates for common, openly available
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Generation from Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to intervene on
these models. To understand the impact of interventions precisely, it is useful
to examine counterfactuals -- e.g., how a given sentence would have appeared
had it been generated by the model following a specific intervention. We
highlight that counterfactual reasoning is conceptually distinct from
interventions, as articulated in Pearl's causal hierarchy. Based on this
observation, we propose a framework for generating true string counterfactuals
by reformulating language models as Generalized Structural-equation. Models
using the Gumbel-max trick. This allows us to model the joint distribution over
original strings and their counterfactuals resulting from the same
instantiation of the sampling noise. We develop an algorithm based on hindsight
Gumbel sampling that allows us to infer the latent noise variables and generate
counterfactuals of observed strings. Our experiments demonstrate that the
approach produces meaningful counterfactuals while at the same time showing
that commonly used intervention techniques have considerable undesired side
effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More Expressive Attention with Negative Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention can shift the token deletion and copying
function from a static OV matrix to dynamic QK inner products, with the OV
matrix now focusing more on refinement or modification. The attention head can
simultaneously delete, copy, or retain tokens by assigning them negative,
positive, or minimal attention weights, respectively. As a result, a single
attention head becomes more flexible and expressive. (2) Cog Attention improves
the model's robustness against representational collapse, which can occur when
earlier tokens are over-squashed into later positions, leading to homogeneous
representations. Negative weights reduce effective information paths from
earlier to later tokens, helping to mitigate this issue. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models for language modeling and U-ViT diffusion models for image
generation. Experiments show that models using Cog Attention exhibit superior
performance compared to those employing traditional softmax attention modules.
Our approach suggests a promising research direction for rethinking and
breaking the entrenched constraints of traditional softmax attention, such as
the requirement for non-negative weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Memorization of Factoids in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models can absorb a massive amount of knowledge through
pretraining, but pretraining is inefficient for acquiring long-tailed or
specialized facts. Therefore, fine-tuning on specialized or new knowledge that
reflects changes in the world has become popular, though it risks disrupting
the model's original capabilities. We study this fragility in the context of
continual memorization, where the model is trained on a small set of long-tail
factoids (factual associations) and must retain these factoids after multiple
stages of subsequent training on other datasets. Through extensive experiments,
we show that LLMs suffer from forgetting across a wide range of subsequent
tasks, and simple replay techniques do not fully prevent forgetting, especially
when the factoid datasets are trained in the later stages. We posit that there
are two ways to alleviate forgetting: 1) protect the memorization process as
the model learns the factoids, or 2) reduce interference from training in later
stages. With this insight, we develop an effective mitigation strategy: REMIX
(Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic
data sampled from pretraining corpora or even randomly generated word sequences
during each stage, despite being unrelated to the memorized factoids in the
first stage. REMIX can recover performance from severe forgetting, often
outperforming replay-based methods that have access to the factoids from the
first stage. We then analyze how REMIX alters the learning process and find
that successful forgetting prevention is associated with a pattern: the model
stores factoids in earlier layers than usual and diversifies the set of layers
that store these factoids. The efficacy of REMIX invites further investigation
into the underlying dynamics of memorization and forgetting, opening exciting
possibilities for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Primer on Word Embeddings: AI Techniques for Text Analysis in Social
  Work 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian E. Perron, Kelley A. Rivenburgh, Bryan G. Victor, Zia Qi, Hui Luan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embeddings represent a transformative technology for analyzing text data
in social work research, offering sophisticated tools for understanding case
notes, policy documents, research literature, and other text-based materials.
This methodological paper introduces word embeddings to social work
researchers, explaining how these mathematical representations capture meaning
and relationships in text data more effectively than traditional keyword-based
approaches. We discuss fundamental concepts, technical foundations, and
practical applications, including semantic search, clustering, and retrieval
augmented generation. The paper demonstrates how embeddings can enhance
research workflows through concrete examples from social work practice, such as
analyzing case notes for housing instability patterns and comparing social work
licensing examinations across languages. While highlighting the potential of
embeddings for advancing social work research, we acknowledge limitations
including information loss, training data constraints, and potential biases. We
conclude that successfully implementing embedding technologies in social work
requires developing domain-specific models, creating accessible tools, and
establishing best practices aligned with social work's ethical principles. This
integration can enhance our ability to analyze complex patterns in text data
while supporting more effective services and interventions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingbo Mo, Shun Jiang, Akash Maharaj, Bernard Hishamunda, Yunyao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-Oriented Dialogue (TOD) systems assist users in completing tasks through
natural language interactions, often relying on a single-layered workflow
structure for slot-filling in public tasks, such as hotel bookings. However, in
enterprise environments, which involve rich domain-specific knowledge, TOD
systems face challenges due to task complexity and the lack of standardized
documentation. In this work, we introduce HierTOD, an enterprise TOD system
driven by hierarchical goals and can support composite workflows. By focusing
on goal-driven interactions, our system serves a more proactive role,
facilitating mixed-initiative dialogue and improving task completion. Equipped
with components for natural language understanding, composite goal retriever,
dialogue management, and response generation, backed by a well-organized data
service with domain knowledge base and retrieval engine, HierTOD delivers
efficient task assistance. Furthermore, our system implementation unifies two
TOD paradigms: slot-filling for information collection and step-by-step
guidance for task execution. Our human study demonstrates the effectiveness and
helpfulness of HierTOD in performing both paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text
  Embeddings Must Adapt <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Anderson, Mano Vikash Janardhanan, Jason He, Wei Cheng, Charlie Flanagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial documents are filled with specialized terminology, arcane jargon,
and curious acronyms that pose challenges for general-purpose text embeddings.
Yet, few text embeddings specialized for finance have been reported in the
literature, perhaps in part due to a lack of public datasets and benchmarks. We
present BAM embeddings, a set of text embeddings finetuned on a carefully
constructed dataset of 14.3M query-passage pairs. Demonstrating the benefits of
domain-specific training, BAM embeddings achieve Recall@1 of 62.8% on a
held-out test set, vs. only 39.2% for the best general-purpose text embedding
from OpenAI. Further, BAM embeddings increase question answering accuracy by 8%
on FinanceBench and show increased sensitivity to the finance-specific elements
that are found in detailed, forward-looking and company and date-specific
queries. To support further research we describe our approach in detail,
quantify the importance of hard negative mining and dataset scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Hui Huang, Weixun Wang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Xuepeng Liu, Dekai Sun, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New LLM evaluation benchmarks are important to align with the rapid
development of Large Language Models (LLMs). In this work, we present Chinese
SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality
ability of language models to answer short questions, and Chinese SimpleQA
mainly has five properties (i.e., Chinese, Diverse, High-quality, Static,
Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6
major topics with 99 diverse subtopics. Second, we conduct a comprehensive
quality control process to achieve high-quality questions and answers, where
the reference answers are static and cannot be changed over time. Third,
following SimpleQA, the questions and answers are very short, and the grading
process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we
perform a comprehensive evaluation on the factuality abilities of existing
LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to
better understand the Chinese factuality abilities of their models and
facilitate the growth of foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval or Global Context Understanding? On Many-Shot In-Context
  Learning for Long-Context Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijian Zou, Muhammad Khalifa, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have demonstrated an improved capacity to handle
long-context information, yet existing long-context benchmarks primarily
measure LMs' retrieval abilities with extended inputs, e.g., pinpointing a
short phrase from long-form text. Therefore, they may fall short when
evaluating models' global context understanding capacity, such as synthesizing
and reasoning over content across input to generate the response. In this
paper, we study long-context language model (LCLM) evaluation through many-shot
in-context learning (ICL). Concretely, we identify the skills each ICL task
requires, and examine models' long-context capabilities on them. We first ask:
What types of ICL tasks benefit from additional demonstrations, and are these
tasks effective at evaluating LCLMs? We find that classification and
summarization tasks show notable performance improvements with additional
demonstrations, while translation and reasoning tasks do not exhibit clear
trends. This suggests the classification tasks predominantly test models'
retrieval skills. Next, we ask: To what extent does each task require retrieval
skills versus global context understanding from LCLMs? We develop metrics to
categorize ICL tasks into two groups: (i) retrieval tasks that require strong
retrieval ability to pinpoint relevant examples, and (ii) global context
understanding tasks that necessitate a deeper comprehension of the full input.
We find that not all datasets can effectively evaluate these long-context
capabilities. To address this gap, we introduce a new many-shot ICL benchmark,
MANYICLBENCH, designed to characterize LCLMs' retrieval and global context
understanding capabilities separately. Benchmarking 11 open-weight LCLMs with
MANYICLBENCH, we find that while state-of-the-art models perform well in
retrieval tasks up to 64k tokens, many show significant drops in global context
tasks at just 16k tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking LLMs' Judgments with No Gold Standard 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the GEM (Generative Estimator for Mutual Information), an
evaluation metric for assessing language generation by Large Language Models
(LLMs), particularly in generating informative judgments, without the need for
a gold standard reference. GEM broadens the scenarios where we can benchmark
LLM generation performance-from traditional ones, like machine translation and
summarization, where gold standard references are readily available, to
subjective tasks without clear gold standards, such as academic peer review.
  GEM uses a generative model to estimate mutual information between candidate
and reference responses, without requiring the reference to be a gold standard.
In experiments on a human-annotated dataset, GEM demonstrates competitive
correlations with human scores compared to the state-of-the-art GPT-4o
Examiner, and outperforms all other baselines. Additionally, GEM is more robust
against strategic manipulations, such as rephrasing or elongation, which can
artificially inflate scores under a GPT-4o Examiner.
  We also present GRE-bench (Generating Review Evaluation Benchmark) which
evaluates LLMs based on how well they can generate high-quality peer reviews
for academic research papers. Because GRE-bench is based upon GEM, it inherits
its robustness properties. Additionally, GRE-bench circumvents data
contamination problems (or data leakage) by using the continuous influx of new
open-access research papers and peer reviews each year. We show GRE-bench
results of various popular LLMs on their peer review capabilities using the
ICLR2023 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruben Härle, Felix Friedrich, Manuel Brack, Björn Deiseroth, Patrick Schramowski, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating human-like text, but their output may not be aligned with the user
or even produce harmful content. This paper presents a novel approach to detect
and steer concepts such as toxicity before generation. We introduce the Sparse
Conditioned Autoencoder (SCAR), a single trained module that extends the
otherwise untouched LLM. SCAR ensures full steerability, towards and away from
concepts (e.g., toxic content), without compromising the quality of the model's
text generation on standard evaluation benchmarks. We demonstrate the effective
application of our approach through a variety of concepts, including toxicity,
safety, and writing style alignment. As such, this work establishes a robust
framework for controlling LLM generations, ensuring their ethical and safe
deployment in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building a Taiwanese Mandarin Spoken Language Model: A First Attempt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report presents our initial attempt to build a spoken large
language model (LLM) for Taiwanese Mandarin, specifically tailored to enable
real-time, speech-to-speech interaction in multi-turn conversations. Our
end-to-end model incorporates a decoder-only transformer architecture and aims
to achieve seamless interaction while preserving the conversational flow,
including full-duplex capabilities allowing simultaneous speaking and
listening. The paper also details the training process, including data
preparation with synthesized dialogues and adjustments for real-time
interaction. We also developed a platform to evaluate conversational fluency
and response coherence in multi-turn dialogues. We hope the release of the
report can contribute to the future development of spoken LLMs in Taiwanese
Mandarin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Neural Networks as Recognizers of Formal Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the computational power of neural network architectures in
terms of formal language theory remains a crucial line of research, as it
describes lower and upper bounds on the reasoning capabilities of modern AI.
However, when empirically testing these bounds, existing work often leaves a
discrepancy between experiments and the formal claims they are meant to
support. The problem is that formal language theory pertains specifically to
recognizers: machines that receive a string as input and classify whether it
belongs to a language. On the other hand, it is common to instead use proxy
tasks that are similar in only an informal sense, such as language modeling or
sequence-to-sequence transduction. We correct this mismatch by training and
evaluating neural networks directly as binary classifiers of strings, using a
general method that can be applied to a wide variety of languages. As part of
this, we extend an algorithm recently proposed by Sn{\ae}bjarnarson et al.
(2024) to do length-controlled sampling of strings from regular languages, with
much better asymptotic time complexity than previous methods. We provide
results on a variety of languages across the Chomsky hierarchy for three neural
architectures: a simple RNN, an LSTM, and a causally-masked transformer. We
find that the RNN and LSTM often outperform the transformer, and that auxiliary
training objectives such as language modeling can help, although no single
objective uniformly improves performance across languages and architectures.
Our contributions will facilitate theoretically sound empirical testing of
language recognition claims in future work. We have released our datasets as a
benchmark called FLaRe (Formal Language Recognition), along with our code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 2 figures. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> verbatim in-context retrieval across time and scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristijan Armeni, Marko Pranjić, Senja Pollak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To predict upcoming text, language models must in some cases retrieve
in-context information verbatim. In this report, we investigated how the
ability of language models to retrieve arbitrary in-context nouns developed
during training (across time) and as language models trained on the same
dataset increase in size (across scale). We then asked whether learning of
in-context retrieval correlates with learning of more challenging zero-shot
benchmarks. Furthermore, inspired by semantic effects in human short-term
memory, we evaluated the retrieval with respect to a major semantic component
of target nouns, namely whether they denote a concrete or abstract entity, as
rated by humans. We show that verbatim in-context retrieval developed in a
sudden transition early in the training process, after about 1% of the training
tokens. This was observed across model sizes (from 14M and up to 12B
parameters), and the transition occurred slightly later for the two smallest
models. We further found that the development of verbatim in-context retrieval
is positively correlated with the learning of zero-shot benchmarks. Around the
transition point, all models showed the advantage of retrieving concrete nouns
as opposed to abstract nouns. In all but two smallest models, the advantage
dissipated away toward the end of training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to Conference on Natural Language Learning 2024
  (https://www.conll.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Response and Emergence of Induction in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niclas Luick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While induction is considered a key mechanism for in-context learning in
LLMs, understanding its precise circuit decomposition beyond toy models remains
elusive. Here, we study the emergence of induction behavior within LLMs by
probing their response to weak single-token perturbations of the residual
stream. We find that LLMs exhibit a robust, universal regime in which their
response remains scale-invariant under changes in perturbation strength,
thereby allowing us to quantify the build-up of token correlations throughout
the model. By applying our method, we observe signatures of induction behavior
within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across
all models, we find that these induction signatures gradually emerge within
intermediate layers and identify the relevant model sections composing this
behavior. Our results provide insights into the collective interplay of
components within LLMs and serve as a benchmark for large-scale circuit
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network pruning is a set of computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used pruning paradigm has focused on pruning and re-training, which
nowadays is inconvenient due to the vast amount of pre-trained models, which
are in any case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
sparse models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAl}, a \emph{top-up}
algorithm that can be used on top of any given pruning algorithm for LLMs, that
modifies the block-wise and row-wise sparsity ratios to maximize the
\emph{neuron alignment} among activations. Moreover, differently from existing
methods, our approach adaptively selects the best parameters for the block-wise
and row-wise sparsity ratios w.r.t. to the model and the desired sparsity
(given as input), and requires \emph{no re-training}. We test our method on 4
different LLM families and 3 different sparsity ratios, showing how it
consistently outperforms the latest state-of-the-art techniques. The code is
available at https://github.com/eliacunegatti/NeuroAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minion: A Technology Probe for Resolving Value Conflicts through
  Expert-Driven and User-Driven Strategies in AI Companion Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianzhe Fan, Qing Xiao, Xuhui Zhou, Yuran Su, Zhicong Lu, Maarten Sap, Hong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI companions based on large language models can role-play and converse very
naturally. When value conflicts arise between the AI companion and the user, it
may offend or upset the user. Yet, little research has examined such conflicts.
We first conducted a formative study that analyzed 151 user complaints about
conflicts with AI companions, providing design implications for our study.
Based on these, we created Minion, a technology probe to help users resolve
human-AI value conflicts. Minion applies a user-empowerment intervention method
that provides suggestions by combining expert-driven and user-driven conflict
resolution strategies. We conducted a technology probe study, creating 40 value
conflict scenarios on Character.AI and Talkie. 22 participants completed 274
tasks and successfully resolved conflicts 94.16% of the time. We summarize user
responses, preferences, and needs in resolving value conflicts, and propose
design implications to reduce conflicts and empower users to resolve them more
effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIFBench: Evaluating the Instruction Following Performance and Stability
  of Large Language Models in Long-Context Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to advance in natural language
processing (NLP), their ability to stably follow instructions in long-context
inputs has become crucial for real-world applications. While existing
benchmarks assess various LLM capabilities, they rarely focus on
instruction-following in long-context scenarios or stability on different
inputs. In response, we introduce the Long-context Instruction-Following
Benchmark (LIFBench), a scalable dataset designed to evaluate LLMs'
instruction-following capabilities and stability across long contexts. LIFBench
comprises three long-context scenarios and eleven diverse tasks, supported by
2,766 instructions generated through an automated expansion method across three
dimensions: length, expression, and variables. For evaluation, we propose
LIFEval, a rubric-based assessment framework that provides precise, automated
scoring of complex LLM responses without relying on LLM-assisted evaluations or
human judgments. This approach facilitates a comprehensive analysis of model
performance and stability across various perspectives. We conduct extensive
experiments on 20 notable LLMs across six length intervals, analyzing their
instruction-following capabilities and stability. Our work contributes LIFBench
and LIFEval as robust tools for assessing LLM performance in complex,
long-context settings, providing insights that can inform future LLM
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniHR: Hierarchical Representation Learning for Unified Knowledge Graph
  Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Liu, Mingyang Chen, Yin Hua, Zhuo Chen, Ziqi Liu, Lei Liang, Huajun Chen, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond-triple fact representations including hyper-relational facts with
auxiliary key-value pairs, temporal facts with additional timestamps, and
nested facts implying relationships between facts, are gaining significant
attention. However, existing link prediction models are usually designed for
one specific type of facts, making it difficult to generalize to other fact
representations. To overcome this limitation, we propose a Unified Hierarchical
Representation learning framework (UniHR) for unified knowledge graph link
prediction. It consists of a unified Hierarchical Data Representation (HiDR)
module and a unified Hierarchical Structure Learning (HiSL) module as graph
encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested
factual KGs into triple-based representations. Then HiSL incorporates
intra-fact and inter-fact message passing, focusing on enhancing the semantic
information within individual facts and enriching the structural information
between facts. Experimental results across 7 datasets from 3 types of KGs
demonstrate that our UniHR outperforms baselines designed for one specific kind
of KG, indicating strong generalization capability of HiDR form and the
effectiveness of HiSL module. Code and data are available at
https://github.com/Lza12a/UniHR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token2Wave 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides an in-depth analysis of Token2Wave, a novel token
representation method derived from the Wave Network, designed to capture both
global and local semantics of input text through wave-inspired complex vectors.
In Token2Wave, each token is represented with a magnitude component, capturing
the global semantics of the entire input text, and a phase component, encoding
the relationships between individual tokens and the global semantics. Building
on prior research that demonstrated the effectiveness of wave-like operations,
such as interference and modulation, during forward propagation, this study
investigates the convergence behavior, backpropagation characteristics, and
embedding independence within the Token2Wave framework. A detailed
computational complexity analysis shows that Token2Wave can significantly
reduce video memory usage and training time compared to BERT. Gradient
comparisons for the [CLS] token, total input text, and classifier parameters
further highlight Token2Wave's unique characteristics. This research offers new
insights into wave-based token representations, demonstrating their potential
to enable efficient and computationally friendly language model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual
  Alignment with Human Smell Experiences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhong, Zetao Zhou, Christopher Dawes, Giada Brianz, Marianna Obrist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning AI with human intent is important, yet perceptual alignment-how AI
interprets what we see, hear, or smell-remains underexplored. This work focuses
on olfaction, human smell experiences. We conducted a user study with 40
participants to investigate how well AI can interpret human descriptions of
scents. Participants performed "sniff and describe" interactive tasks, with our
designed AI system attempting to guess what scent the participants were
experiencing based on their descriptions. These tasks evaluated the Large
Language Model's (LLMs) contextual understanding and representation of scent
relationships within its internal states - high-dimensional embedding space.
Both quantitative and qualitative methods were used to evaluate the AI system's
performance. Results indicated limited perceptual alignment, with biases
towards certain scents, like lemon and peppermint, and continued failing to
identify others, like rosemary. We discuss these findings in light of human-AI
alignment advancements, highlighting the limitations and opportunities for
enhancing HCI systems with multisensory experience integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cancer-Answer: Empowering Cancer Care with Advanced Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gastrointestinal (GI) tract cancers account for a substantial portion of the
global cancer burden, where early diagnosis is critical for improved management
and patient outcomes. The complex aetiologies and overlapping symptoms across
GI cancers often delay diagnosis, leading to suboptimal treatment strategies.
Cancer-related queries are crucial for timely diagnosis, treatment, and patient
education, as access to accurate, comprehensive information can significantly
influence outcomes. However, the complexity of cancer as a disease, combined
with the vast amount of available data, makes it difficult for clinicians and
patients to quickly find precise answers. To address these challenges, we
leverage large language models (LLMs) such as GPT-3.5 Turbo to generate
accurate, contextually relevant responses to cancer-related queries.
Pre-trained with medical data, these models provide timely, actionable insights
that support informed decision-making in cancer diagnosis and care, ultimately
improving patient outcomes. We calculate two metrics: A1 (which represents the
fraction of entities present in the model-generated answer compared to the gold
standard) and A2 (which represents the linguistic correctness and
meaningfulness of the model-generated answer with respect to the gold
standard), achieving maximum values of 0.546 and 0.881, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Conversational System for Differential
  Diagnosis of GI Cancer)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Electroencephalogram-based Multi-class Decoding of Attended Speakers'
  Direction with Audio Spatial Spectrum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanming Zhang, Jing Lu, Zhibin Lin, Fei Chen, Haoliang Du, Xia Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding the directional focus of an attended speaker from listeners'
electroencephalogram (EEG) signals is essential for developing brain-computer
interfaces to improve the quality of life for individuals with hearing
impairment. Previous works have concentrated on binary directional focus
decoding, i.e., determining whether the attended speaker is on the left or
right side of the listener. However, a more precise decoding of the exact
direction of the attended speaker is necessary for effective speech processing.
Additionally, audio spatial information has not been effectively leveraged,
resulting in suboptimal decoding results. In this paper, we observe that, on
our recently presented dataset with 15-class directional focus, models relying
exclusively on EEG inputs exhibits significantly lower accuracy when decoding
the directional focus in both leave-one-subject-out and leave-one-trial-out
scenarios. By integrating audio spatial spectra with EEG features, the decoding
accuracy can be effectively improved. We employ the CNN, LSM-CNN, and
EEG-Deformer models to decode the directional focus from listeners' EEG signals
with the auxiliary audio spatial spectra. The proposed Sp-Aux-Deformer model
achieves notable 15-class decoding accuracies of 57.48% and 61.83% in
leave-one-subject-out and leave-one-trial-out scenarios, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVQAScore: Efficient Video Question Answering Data Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liang, Zirong Chen, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question-answering (QA) is a core task in video understanding.
Evaluating the quality of video QA and video caption data quality for training
video large language models (VideoLLMs) is an essential challenge. Although
various methods have been proposed for assessing video caption quality, there
remains a lack of dedicated evaluation methods for Video QA. To address this
gap, we introduce EVQAScore, a reference-free method that leverages keyword
extraction to assess both video caption and video QA data quality.
Additionally, we incorporate frame sampling and rescaling techniques to enhance
the efficiency and robustness of our evaluation, this enables our score to
evaluate the quality of extremely long videos. Our approach achieves
state-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for
Spearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on
the VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using
EVQAScore for data selection, we achieved SOTA results with only 12.5\% of the
original data volume, outperforming the previous SOTA method PAC-S and 100\% of
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongSafetyBench: Long-Context LLMs Struggle with Safety Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models (LLMs), the sequence length of
these models continues to increase, drawing significant attention to
long-context language models. However, the evaluation of these models has been
primarily limited to their capabilities, with a lack of research focusing on
their safety. Existing work, such as ManyShotJailbreak, has to some extent
demonstrated that long-context language models can exhibit safety concerns.
However, the methods used are limited and lack comprehensiveness. In response,
we introduce \textbf{LongSafetyBench}, the first benchmark designed to
objectively and comprehensively evaluate the safety of long-context models.
LongSafetyBench consists of 10 task categories, with an average length of
41,889 words. After testing eight long-context language models on
LongSafetyBench, we found that existing models generally exhibit insufficient
safety capabilities. The proportion of safe responses from most mainstream
long-context LLMs is below 50\%. Moreover, models' safety performance in
long-context scenarios does not always align with that in short-context
scenarios. Further investigation revealed that long-context models tend to
overlook harmful content within lengthy texts. We also proposed a simple yet
effective solution, allowing open-source models to achieve performance
comparable to that of top-tier closed-source models. We believe that
LongSafetyBench can serve as a valuable benchmark for evaluating the safety
capabilities of long-context language models. We hope that our work will
encourage the broader community to pay attention to the safety of long-context
models and contribute to the development of solutions to improve the safety of
long-context LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChuLo: Chunk-Level Key Information Representation for Long Document
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11119v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11119v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Soyeon Caren Han, Yue Dai, Feiqi Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have achieved remarkable success in various Natural
Language Processing (NLP) tasks, yet their ability to handle long documents is
constrained by computational limitations. Traditional approaches, such as
truncating inputs, sparse self-attention, and chunking, attempt to mitigate
these issues, but they often lead to information loss and hinder the model's
ability to capture long-range dependencies. In this paper, we introduce ChuLo,
a novel chunk representation method for long document classification that
addresses these limitations. Our ChuLo groups input tokens using unsupervised
keyphrase extraction, emphasizing semantically important keyphrase based chunk
to retain core document content while reducing input length. This approach
minimizes information loss and improves the efficiency of Transformer-based
models. Preserving all tokens in long document understanding, especially token
classification tasks, is especially important to ensure that fine-grained
annotations, which depend on the entire sequence context, are not lost. We
evaluate our method on multiple long document classification tasks and long
document token classification tasks, demonstrating its effectiveness through
comprehensive qualitative and quantitative analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to a conference and is currently under
  review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decomposition of surprisal: Unified computational model of ERP
  components in language processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Li, Richard Futrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The functional interpretation of language-related ERP components has been a
central debate in psycholinguistics for decades. We advance an
information-theoretic model of human language processing in the brain in which
incoming linguistic input is processed at first shallowly and later with more
depth, with these two kinds of information processing corresponding to distinct
electroencephalographic signatures. Formally, we show that the information
content (surprisal) of a word in context can be decomposed into two quantities:
(A) shallow surprisal, which signals shallow processing difficulty for a word,
and corresponds with the N400 signal; and (B) deep surprisal, which reflects
the discrepancy between shallow and deep representations, and corresponds to
the P600 signal and other late positivities. Both of these quantities can be
estimated straightforwardly using modern NLP models. We validate our theory by
successfully simulating ERP patterns elicited by a variety of linguistic
manipulations in previously-reported experimental data from six experiments,
with successful novel qualitative and quantitative predictions. Our theory is
compatible with traditional cognitive theories assuming a `good-enough' shallow
representation stage, but with a precise information-theoretic formulation. The
model provides an information-theoretic model of ERP components grounded on
cognitive processes, and brings us closer to a fully-specified
neuro-computational model of language processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrAttack: <span class="highlight-title">Prompt</span> Decomposition and Reconstruction Makes Powerful LLM
  Jailbreakers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16914v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16914v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safety alignment of Large Language Models (LLMs) is vulnerable to both
manual and automated jailbreak attacks, which adversarially trigger LLMs to
output harmful content. However, current methods for jailbreaking LLMs, which
nest entire harmful prompts, are not effective at concealing malicious intent
and can be easily identified and rejected by well-aligned LLMs. This paper
discovers that decomposing a malicious prompt into separated sub-prompts can
effectively obscure its underlying malicious intent by presenting it in a
fragmented, less detectable form, thereby addressing these limitations. We
introduce an automatic prompt \textbf{D}ecomposition and
\textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack).
DrAttack includes three key components: (a) `Decomposition' of the original
prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly
by in-context learning with semantically similar but harmless reassembling
demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts'
synonyms that maintain the original intent while jailbreaking LLMs. An
extensive empirical study across multiple open-source and closed-source LLMs
demonstrates that, with a significantly reduced number of queries, DrAttack
obtains a substantial gain of success rate over prior SOTA prompt-only
attackers. Notably, the success rate of 78.0\% on GPT-4 with merely 15 queries
surpassed previous art by 33.1\%. The project is available at
https://github.com/xirui-li/DrAttack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWE-bench: Can Language Models Resolve Real-World GitHub Issues? <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have outpaced our ability to evaluate them effectively, but
for their future development it is essential to study the frontier of their
capabilities. We find real-world software engineering to be a rich,
sustainable, and challenging testbed for evaluating the next generation of
language models. To this end, we introduce SWE-bench, an evaluation framework
consisting of $2,294$ software engineering problems drawn from real GitHub
issues and corresponding pull requests across $12$ popular Python repositories.
Given a codebase along with a description of an issue to be resolved, a
language model is tasked with editing the codebase to address the issue.
Resolving issues in SWE-bench frequently requires understanding and
coordinating changes across multiple functions, classes, and even files
simultaneously, calling for models to interact with execution environments,
process extremely long contexts and perform complex reasoning that goes far
beyond traditional code generation tasks. Our evaluations show that both
state-of-the-art proprietary models and our fine-tuned model SWE-Llama can
resolve only the simplest issues. The best-performing model, Claude 2, is able
to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps
towards LMs that are more practical, intelligent, and autonomous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data, code, and leaderboard are available at https://www.swebench.com
  ICLR 2024, https://openreview.net/forum?id=VTF8yNQM66</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Dynamics of Emotion and Cognition in Human Translation:
  Integrating the Task Segment Framework and the HOF Taxonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Carl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper develops a novel generative model of human translation processes
grounded in empirical translation process data. Assuming three processes that
unfold concurrently in the translating mind, it integrates the Task Segment
Framework (Munoz & Apfelthaler 2022) and the HOF taxonomy (Carl et al 2024)
into a coherent architecture: uninterrupted translation production is caused by
routinized/automated processes, cognitive/reflective interventions lead to
longer keystroke pauses, while emotional/affective states of the mind are
identified by distinctive gazing patterns. Utilizing data from the CRITT
Translation Process Research Database (TPR-DB), the paper illustrates how the
temporal structure of keystroke and gazing data can be related to the three
assumed hidden mental processes that are believed to cause the observable data.
The paper relates this embedded generative model with Robinsons (2023)
ideosomatic theory of translation, opening exciting, new theoretical horizons
for Cognitive Translation Studies, grounded in empirical data and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple is Effective: The Roles of Graphs and Large Language Models in
  Knowledge-Graph-Based Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mufei Li, Siqi Miao, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate strong reasoning abilities but face
limitations such as hallucinations and outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by
grounding LLM outputs in structured external knowledge from KGs. However,
current KG-based RAG frameworks still struggle to optimize the trade-off
between retrieval effectiveness and efficiency in identifying a suitable amount
of relevant graph information for the LLM to digest. We introduce SubgraphRAG,
extending the KG-based RAG framework that retrieves subgraphs and leverages
LLMs for reasoning and answer prediction. Our approach innovatively integrates
a lightweight multilayer perceptron with a parallel triple-scoring mechanism
for efficient and flexible subgraph retrieval while encoding directional
structural distances to enhance retrieval effectiveness. The size of retrieved
subgraphs can be flexibly adjusted to match the query's need and the downstream
LLM's capabilities. This design strikes a balance between model complexity and
reasoning power, enabling scalable and generalizable retrieval processes.
Notably, based on our retrieved subgraphs, smaller LLMs like
Llama3.1-8B-Instruct deliver competitive results with explainable reasoning,
while larger models like GPT-4o achieve state-of-the-art accuracy compared with
previous baselines -- all without fine-tuning. Extensive evaluations on the
WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,
accuracy, and reliability by reducing hallucinations and improving response
grounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/Graph-COM/SubgraphRAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extrinsically-Focused Evaluation of Omissions in Medical Summarization <span class="chip">ML4H 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Schumacher, Daniel Rosenthal, Dhruv Naik, Varun Nair, Luladay Price, Geoffrey Tso, Anitha Kannan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown promise in safety-critical
applications such as healthcare, yet the ability to quantify performance has
lagged. An example of this challenge is in evaluating a summary of the
patient's medical record. A resulting summary can enable the provider to get a
high-level overview of the patient's health status quickly. Yet, a summary that
omits important facts about the patient's record can produce a misleading
picture. This can lead to negative consequences on medical decision-making. We
propose MED-OMIT as a metric to explore this challenge. We focus on using
provider-patient history conversations to generate a subjective (a summary of
the patient's history) as a case study. We begin by discretizing facts from the
dialogue and identifying which are omitted from the subjective. To determine
which facts are clinically relevant, we measure the importance of each fact to
a simulated differential diagnosis. We compare MED-OMIT's performance to that
of clinical experts and find broad agreement We use MED-OMIT to evaluate LLM
performance on subjective generation and find some LLMs (gpt-4 and
llama-3.1-405b) work well with little effort, while others (e.g. Llama 2)
perform worse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ML4H 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan
  Arabic Dialect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guokan Shang, Hadi Abdine, Yousef Khoubrane, Amr Mohamed, Yassine Abbahaddou, Sofiane Ennadir, Imane Momayiz, Xuguang Ren, Eric Moulines, Preslav Nakov, Michalis Vazirgiannis, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Atlas-Chat, the first-ever collection of LLMs specifically
developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as
Darija, we construct our instruction dataset by consolidating existing Darija
language resources, creating novel datasets both manually and synthetically,
and translating English instructions with stringent quality control.
Atlas-Chat-2B, 9B, and 27B models, fine-tuned on the dataset, exhibit superior
ability in following Darija instructions and performing standard NLP tasks.
Notably, our models outperform both state-of-the-art and Arabic-specialized
LLMs like LLaMa, Jais, and AceGPT, e.g., our 9B model gains a 13% performance
boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation
suite for Darija covering both discriminative and generative tasks.
Furthermore, we perform an experimental analysis of various fine-tuning
strategies and base model choices to determine optimal configurations. All our
resources are publicly accessible, and we believe our work offers comprehensive
design methodologies of instruction-tuning for low-resource languages, which
are often neglected in favor of data-rich languages by contemporary LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge
  into LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Wu, Kevin Wu, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is great interest in fine-tuning frontier large language models (LLMs)
to inject new information and update existing knowledge. While commercial LLM
fine-tuning APIs from providers such as OpenAI and Google promise flexible
adaptation for various applications, the efficacy of fine-tuning remains
unclear. In this study, we introduce FineTuneBench, an evaluation framework and
dataset for understanding how well commercial fine-tuning APIs can successfully
learn new and updated knowledge. We analyze five frontier LLMs with
commercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro,
on their effectiveness in two settings: (1) ingesting novel information, such
as recent news events and new people profiles, and (2) updating existing
knowledge, such as updated medical guidelines and code frameworks. Our results
reveal substantial shortcomings in all the models' abilities to effectively
learn new information through fine-tuning, with an average generalization
accuracy of 37% across all models. When updating existing knowledge, such as
incorporating medical guideline updates, commercial fine-tuning APIs show even
more limited capability (average generalization accuracy of 19%). Overall,
fine-tuning GPT-4o mini is the most effective for infusing new knowledge and
updating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs
for Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or
update existing knowledge. These findings underscore a major shortcoming in
using current commercial fine-tuning services to achieve reliable knowledge
infusion in common scenarios. We open source the FineTuneBench dataset at
https://github.com/kevinwu23/StanfordFineTuneBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs as Method Actors: A Model for <span class="highlight-title">Prompt</span> Engineering and Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Doyle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce "Method Actors" as a mental model for guiding LLM prompt
engineering and prompt architecture. Under this mental model, LLMs should be
thought of as actors; prompts as scripts and cues; and LLM responses as
performances. We apply this mental model to the task of improving LLM
performance at playing Connections, a New York Times word puzzle game that
prior research identified as a challenging benchmark for evaluating LLM
reasoning. Our experiments with GPT-4o show that a "Method Actors" approach can
significantly improve LLM performance over both a vanilla and "Chain of
Thoughts" approach. A vanilla approach solves 27% of Connections puzzles in our
dataset and a "Chain of Thoughts" approach solves 41% of puzzles, whereas our
strongest "Method Actor" approach solves 86% of puzzles. We also test OpenAI's
newest model designed specifically for complex reasoning tasks, o1-preview.
When asked to solve a puzzle all at once, o1-preview solves 79% of Connections
puzzles in our dataset, and when allowed to build puzzle solutions one guess at
a time over multiple API calls, o1-preview solves 100% of the puzzles.
Incorporating a "Method Actor" prompt architecture increases the percentage of
puzzles that o1-preview solves perfectly from 76% to 87%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Sound Symbolism in Audio-visual Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Cheng Tseng, Yi-Jen Shih, David Harwath, Raymond Mooney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual pre-trained models have gained substantial attention recently
and demonstrated superior performance on various audio-visual tasks. This study
investigates whether pre-trained audio-visual models demonstrate non-arbitrary
associations between sounds and visual representations$\unicode{x2013}$known as
sound symbolism$\unicode{x2013}$which is also observed in humans. We developed
a specialized dataset with synthesized images and audio samples and assessed
these models using a non-parametric approach in a zero-shot setting. Our
findings reveal a significant correlation between the models' outputs and
established patterns of sound symbolism, particularly in models trained on
speech data. These results suggest that such models can capture sound-meaning
connections akin to human language processing, providing insights into both
cognitive architectures and machine learning strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grounding Large Language Models In Embodied Environment With Imperfect
  World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolan Liu, Jishen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite a widespread success in various applications, large language models
(LLMs) often stumble when tackling basic physical reasoning or executing
robotics tasks, due to a lack of direct experience with the physical nuances of
the real world. To address these issues, we propose a Grounding Large language
model with Imperfect world MOdel (GLIMO), which utilizes proxy world models
such as simulators to collect and synthesize trining data. GLIMO incorporates
an LLM agent-based data generator to automatically create high-quality and
diverse instruction datasets. The generator includes an iterative self-refining
module for temporally consistent experience sampling, a diverse set of
question-answering instruction seeds, and a retrieval-augmented generation
module for reflecting on prior experiences. Comprehensive experiments show that
our approach improve the performance of strong open-source LLMs like LLaMA-3
with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$
across three different benchmarks, respectively. The performance is able to
compete with or surpass their larger counterparts such as GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>s represent belief state geometry in their residual stream 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam S. Shai, Sarah E. Marzen, Lucas Teixeira, Alexander Gietelink Oldenziel, Paul M. Riechers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What computational structure are we building into large language models when
we train them on next-token prediction? Here, we present evidence that this
structure is given by the meta-dynamics of belief updating over hidden states
of the data-generating process. Leveraging the theory of optimal prediction, we
anticipate and then find that belief states are linearly represented in the
residual stream of transformers, even in cases where the predicted belief state
geometry has highly nontrivial fractal structure. We investigate cases where
the belief state geometry is represented in the final residual stream or
distributed across the residual streams of multiple layers, providing a
framework to explain these observations. Furthermore we demonstrate that the
inferred belief states contain information about the entire future, beyond the
local next-token prediction that the transformers are explicitly trained on.
Our work provides a general framework connecting the structure of training data
to the geometric structure of activations inside transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWE-agent: Agent-Computer Interfaces Enable Automated Software
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15793v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15793v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) agents are increasingly being used to automate
complicated tasks in digital environments. Just as humans benefit from powerful
software applications, such as integrated development environments, for complex
tasks like software engineering, we posit that LM agents represent a new
category of end users with their own needs and abilities, and would benefit
from specially-built interfaces to the software they use. We investigate how
interface design affects the performance of language model agents. As a result
of this exploration, we introduce SWE-agent: a system that facilitates LM
agents to autonomously use computers to solve software engineering tasks.
SWE-agent's custom agent-computer interface (ACI) significantly enhances an
agent's ability to create and edit code files, navigate entire repositories,
and execute tests and other programs. We evaluate SWE-agent on SWE-bench and
HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate
of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art
achieved with non-interactive LMs. Finally, we provide insight on how the
design of the ACI can impact agents' behavior and performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, data, and demo available at https://swe-agent.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking LLM Memorization through the Lens of Adversarial Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15146v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15146v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) trained on web-scale datasets raise substantial
concerns regarding permissible data usage. One major question is whether these
models "memorize" all their training data or they integrate many data sources
in some way more akin to how a human would learn and synthesize information.
The answer hinges, to a large degree, on how we define memorization. In this
work, we propose the Adversarial Compression Ratio (ACR) as a metric for
assessing memorization in LLMs. A given string from the training data is
considered memorized if it can be elicited by a prompt (much) shorter than the
string itself -- in other words, if these strings can be "compressed" with the
model by computing adversarial prompts of fewer tokens. The ACR overcomes the
limitations of existing notions of memorization by (i) offering an adversarial
view of measuring memorization, especially for monitoring unlearning and
compliance; and (ii) allowing for the flexibility to measure memorization for
arbitrary strings at a reasonably low compute. Our definition serves as a
practical tool for determining when model owners may be violating terms around
data usage, providing a potential legal tool and a critical lens through which
to address such scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://locuslab.github.io/acr-memorization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hire a Linguist!: Learning Endangered Languages with In-Context
  Linguistic Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexun Zhang, Yee Man Choi, Zhenqiao Song, Taiqi He, William Yang Wang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can large language models (LLMs) process and translate endangered
languages? Many languages lack a large corpus to train a decent LLM; therefore
existing LLMs rarely perform well in unseen, endangered languages. On the
contrary, we observe that 2000 endangered languages, though without a large
corpus, have a grammar book or a dictionary. We propose LINGOLLM, a
training-free approach to enable an LLM to process unseen languages that hardly
occur in its pre-training. Our key insight is to demonstrate linguistic
knowledge of an unseen language in an LLM's prompt, including a dictionary, a
grammar book, and morphologically analyzed input text. We implement LINGOLLM on
top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks
across 8 endangered or low-resource languages. Our results show that LINGOLLM
elevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language
directions. Our findings demonstrate the tremendous value of linguistic
knowledge in the age of LLMs for endangered languages. Our data, code, and
model generations can be found at https://github.com/LLiLab/llm4endangeredlang.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stronger Random Baselines for In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory Yauney, David Mimno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the in-context learning classification performance of language
models poses challenges due to small dataset sizes, extensive prompt-selection
using the validation set, and intentionally difficult tasks that lead to
near-random performance. The standard random baseline--the expected accuracy of
guessing labels uniformly at random--is stable when the evaluation set is used
only once or when the dataset is large. We account for the common practice of
validation set reuse and existing small datasets with a stronger random
baseline: the expected maximum accuracy across multiple random classifiers.
When choosing the best prompt demonstrations across six quantized language
models applied to 16 BIG-bench Lite tasks, more than 20% of the few-shot
results that exceed the standard baseline do not exceed this stronger random
baseline. When held-out test sets are available, this stronger baseline is also
a better predictor of held-out performance than the standard baseline, avoiding
unnecessary test set evaluations. This maximum random baseline provides an
easily calculated drop-in replacement for the standard baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolving to the Future: Unseen Event Adaptive Fake News Detection on
  Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Zhang, Zhixun Li, Qiang Liu, Shu Wu, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of social media, the wide dissemination of fake
news on social media is increasingly threatening both individuals and society.
One of the unique challenges for fake news detection on social media is how to
detect fake news on future events. Recently, numerous fake news detection
models that utilize textual information and the propagation structure of posts
have been proposed. Unfortunately, most of the existing approaches can hardly
handle this challenge since they rely heavily on event-specific features for
prediction and cannot generalize to unseen events. To address this, we
introduce \textbf{F}uture \textbf{AD}aptive \textbf{E}vent-based Fake news
Detection (FADE) framework. Specifically, we train a target predictor through
an adaptive augmentation strategy and graph contrastive learning to obtain
higher-quality features and make more accurate overall predictions.
Simultaneously, we independently train an event-only predictor to obtain biased
predictions. We further mitigate event bias by subtracting the event-only
predictor's output from the target predictor's output to obtain the final
prediction. Encouraging results from experiments designed to emulate real-world
social media conditions validate the effectiveness of our method in comparison
to existing state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference
  Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference tuning of large language models (LLMs) relies on high-quality
human preference data, which is often expensive and time-consuming to gather.
While existing methods can use trained reward models or proprietary model as
judges for preference annotation, they have notable drawbacks: training reward
models remain dependent on initial human data, and using proprietary model
imposes license restrictions that inhibits commercial usage. In this paper, we
introduce customized density ratio (CDR), a training-free and highly effective
method that leverages off-the-shelf LLMs for preference data annotation. Our
approach uses the log-density ratio between a better-aligned LLM and a less
aligned LLM as a reward signal. We explores 221 different LLMs pairs and
empirically demonstrate that increasing the performance gap between paired LLMs
correlates with better reward generalization. Furthermore, we show that
tailoring the density ratio reward function with specific criteria and
preference exemplars enhances performance across domains and within target
areas.
  In our experiment using density ratio from a pair of Mistral-7B models, CDR
achieves a RewardBench score of 82.6, outperforming the best trained reward
functions from same model class and demonstrating competitive performance
against SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR
to annotate an on-policy preference dataset with which we preference tune
Llama-3-8B-Instruct with SimPO. Using reward signals from two relatively weak
models, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate on
ArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0,
along with a score of 8.0 on MT-Bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hire Me or Not? Examining Language Model's Behavior with Occupation
  Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the impressive performance in various downstream tasks, large language
models (LLMs) have been widely integrated into production pipelines, like
recruitment and recommendation systems. A known issue of models trained on
natural language data is the presence of human biases, which can impact the
fairness of the system. This paper investigates LLMs' behavior with respect to
gender stereotypes, in the context of occupation decision making. Our framework
is designed to investigate and quantify the presence of gender stereotypes in
LLMs' behavior via multi-round question answering. Inspired by prior works, we
construct a dataset by leveraging a standard occupation classification
knowledge base released by authoritative agencies. We tested three LLMs
(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models
exhibit gender stereotypes analogous to human biases, but with different
preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may
imply the current alignment methods are insufficient for debiasing and could
introduce new biases contradicting the traditional gender stereotypes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to
  Address Shortcut Shifts in Natural Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ukyo Honda, Tatsushi Oka, Peinan Zhang, Masato Mita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent models for natural language understanding are inclined to exploit
simple patterns in datasets, commonly known as shortcuts. These shortcuts hinge
on spurious correlations between labels and latent features existing in the
training data. At inference time, shortcut-dependent models are likely to
generate erroneous predictions under distribution shifts, particularly when
some latent features are no longer correlated with the labels. To avoid this,
previous studies have trained models to eliminate the reliance on shortcuts. In
this study, we explore a different direction: pessimistically aggregating the
predictions of a mixture-of-experts, assuming each expert captures relatively
different latent features. The experimental results demonstrate that our
post-hoc control over the experts significantly enhances the model's robustness
to the distribution shift in shortcuts. Besides, we show that our approach has
some practical advantages. We also analyze our model and provide results to
support the assumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures (the layout differs from the MIT Press
  publication version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Augmentation: <span class="highlight-title">Self-Supervised</span> Learning with Transformations in
  Activation Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rickard Brüel-Gabrielsson, Tongzhou Wang, Manel Baradad, Justin Solomon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Deep Augmentation, an approach to implicit data augmentation
using dropout or PCA to transform a targeted layer within a neural network to
improve performance and generalization. We demonstrate Deep Augmentation
through extensive experiments on contrastive learning tasks in NLP, computer
vision, and graph learning. We observe substantial performance gains with
Transformers, ResNets, and Graph Neural Networks as the underlying models in
contrastive learning, but observe inverse effects on the corresponding
supervised problems. Our analysis suggests that Deep Augmentation alleviates
co-adaptation between layers, a problem exhibited by self-supervised learning
where ground truth labels are not available. We use this observation to
formulate a method for selecting which layer to target; in particular, our
experimentation reveals that targeting deeper layers with Deep Augmentation
outperforms augmenting the input data. The simple network- and
modality-agnostic nature of this approach enables its integration into various
machine learning pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recent Advances in Named Entity Recognition: A Comprehensive <span class="highlight-title">Survey</span> and
  Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imed Keraghel, Stanislas Morbieu, Mohamed Nadif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition seeks to extract substrings within a text that name
real-world objects and to determine their type (for example, whether they refer
to persons or organizations). In this survey, we first present an overview of
recent popular approaches, including advancements in Transformer-based methods
and Large Language Models (LLMs) that have not had much coverage in other
surveys. In addition, we discuss reinforcement learning and graph-based
approaches, highlighting their role in enhancing NER performance. Second, we
focus on methods designed for datasets with scarce annotations. Third, we
evaluate the performance of the main NER implementations on a variety of
datasets with differing characteristics (as regards their domain, their size,
and their number of classes). We thus provide a deep comparison of algorithms
that have never been considered together. Our experiments shed some light on
how the characteristics of datasets affect the behavior of the methods we
compare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09824v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09824v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph generation is a fundamental task that has been extensively studied in
social, technological, and scientific analysis. For modeling the dynamic graph
evolution process, traditional rule-based methods struggle to capture community
structures within graphs, while deep learning methods only focus on fitting
training graphs. This limits existing graph generators to producing graphs that
adhere to predefined rules or closely resemble training datasets, achieving
poor performance in dynamic graph generation. Given that graphs are abstract
representations arising from pairwise interactions in human activities, a
realistic simulation of human-wise interaction could provide deeper insights
into the graph evolution mechanism. With the increasing recognition of large
language models (LLMs) in simulating human behavior, we introduce
GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic
graph generation. Without training or fine-tuning process of LLM, our framework
effectively replicates seven macro-level structural characteristics in
established network science theories while surpassing existing baselines in
graph expansion tasks by 31\% on specific evaluation metrics. Through node
classification task, we validate GAG effectively preserves characteristics of
real-world network for node-wise textual features in generated text-rich graph.
Furthermore, by incorporating parallel acceleration, GAG supports generating
graphs with up to nearly 100,000 nodes or 10 million edges through large-scale
LLM-based agent simulation, with a minimum speed-up of 90.4\%. The source code
is available at https://anonymous.4open.science/r/GraphAgent-2206.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pediatrics<span class="highlight-title">GPT</span>: Large Language Models as Chinese Medical Assistants for
  Pediatric Applications <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19266v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19266v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing intelligent pediatric consultation systems offers promising
prospects for improving diagnostic efficiency, especially in China, where
healthcare resources are scarce. Despite recent advances in Large Language
Models (LLMs) for Chinese medicine, their performance is sub-optimal in
pediatric applications due to inadequate instruction data and vulnerable
training procedures. To address the above issues, this paper builds PedCorpus,
a high-quality dataset of over 300,000 multi-task instructions from pediatric
textbooks, guidelines, and knowledge graph resources to fulfil diverse
diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the
first Chinese pediatric LLM assistant built on a systematic and robust training
pipeline. In the continuous pre-training phase, we introduce a hybrid
instruction pre-training mechanism to mitigate the internal-injected knowledge
inconsistency of LLMs for medical domain adaptation. Immediately, the
full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the
general medical knowledge schema into the models. After that, we devise a
direct following preference optimization to enhance the generation of
pediatrician-like humanistic responses. In the parameter-efficient secondary
SFT phase, a mixture of universal-specific experts strategy is presented to
resolve the competency conflict between medical generalist and pediatric
expertise mastery. Extensive results based on the metrics, GPT-4, and doctor
evaluations on distinct doctor downstream tasks show that PediatricsGPT
consistently outperforms previous Chinese medical LLMs. Our model and dataset
will be open-source for community development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical
  Large Language Model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning LLMs for FL-free Program Repair <span class="chip">ICSE'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjielong Xu, Ying Fu, Shin Hwei Tan, Pinjia He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved decent results on automated
program repair (APR). However, the next token prediction training objective of
decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction
objective of current infilling-style methods, which impedes LLMs from fully
leveraging pre-trained knowledge for program repair. In addition, while some
LLMs can locate and repair bugs in certain functions using the related
artifacts (e.g., test cases), existing methods still depend on statement-level
fault localization methods to provide a list of buggy hunks for repair. This
restriction hinders LLMs from exploring potential patches beyond the given
locations.
  In this paper, we investigate a new approach to adapt LLMs to program repair.
Our core insight is that LLM's APR capability can be greatly improved by simply
aligning the output to their training objective and allowing them to refine the
whole program without first identifying faulty statements. Based on this
insight, we designed D4C, a straightforward prompting framework for APR. D4C
can repair 180 bugs correctly in Defects4J, with each patch being sampled only
10 times. This surpasses the SOTA APR methods with perfect fault localization
by 10% and reduces the patch sampling number by 90%. Our findings reveal that
(1) objective alignment is crucial for fully exploiting LLM's pre-trained
capability, and (2) replacing the traditional localize-buggy-hunks-then-repair
workflow with direct debugging is more effective for LLM-based APR methods.
Thus, we believe this paper introduces a new mindset for harnessing LLMs in
APR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UstanceBR: a social media language resource for stance prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06374v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06374v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camila Pereira, Matheus Pavan, Sungwon Yoon, Ricelli Ramos, Pablo Costa, Lais Cavalheiro, Ivandre Paraboni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces UstanceBR, a multimodal corpus in the Brazilian
Portuguese Twitter domain for target-based stance prediction. The corpus
comprises 86.8 k labelled stances towards selected target topics, and extensive
network information about the users who published these stances on social
media. In this article we describe the corpus multimodal data, and a number of
usage examples in both in-domain and zero-shot stance prediction based on text-
and network-related information, which are intended to provide initial baseline
results for future studies in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Diversity Matters for Robust Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Bukharin, Shiyang Li, Zhengyang Wang, Jingfeng Yang, Bing Yin, Xian Li, Chao Zhang, Tuo Zhao, Haoming Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that by curating high quality and diverse instruction
tuning datasets, we can significantly improve instruction-following
capabilities. However, creating such datasets is difficult and most works rely
on manual curation or proprietary language models. Automatic data curation is
difficult as it is still not clear how we can define diversity for instruction
tuning, how diversity and quality depend on one other, and how we can optimize
dataset quality and diversity. To resolve these issue, we propose a new
algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple
method to simultaneously control dataset diversity and quality, allowing us to
conduct an in-depth study on the effect of diversity and quality on instruction
tuning performance. From this study we draw two key insights (1) there is a
natural tradeoff between data diversity and quality and (2) increasing data
diversity significantly improves the worst case instruction following
performance, therefore improving robustness. We validate the performance of
QDIT on several large scale instruction tuning datasets, where we find it can
substantially improve worst and average case performance compared to
quality-driven data selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wave Network: An Ultra-Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02674v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02674v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a complex
vector to represent each token, encoding both global and local semantics of the
input text. A complex vector consists of two components: a magnitude vector
representing the global semantics of the input text, and a phase vector
capturing the relationships between individual tokens and global semantics.
Experiments on the AG News text classification task demonstrate that, when
generating complex vectors from randomly initialized token embeddings, our
single-layer Wave Network achieves 90.91% accuracy with wave interference and
91.66% with wave modulation - outperforming a single Transformer layer using
BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching
the accuracy of the pre-trained and fine-tuned BERT base model (94.64%).
Additionally, compared to BERT base, the Wave Network reduces video memory
usage and training time by 77.34% and 85.62% during wave modulation. In
summary, we used a 2.4-million-parameter small language model to achieve
accuracy comparable to a 100-million-parameter BERT model in text
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Lexical Representation for Interpretable Visual-Language
  Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Li, Yikai Wang, Yanwei Fu, Dongyu Ru, Zheng Zhang, Tong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's
groundbreaking work. Although CLIP performs well, the typical direct latent
feature alignment lacks clarity in its representation and similarity scores. On
the other hand, lexical representation, a vector whose element represents the
similarity between the sample and a word from the vocabulary, is a natural
sparse representation and interpretable, providing exact matches for individual
words. However, lexical representations are difficult to learn due to no
ground-truth supervision and false-discovery issues, and thus requires complex
design to train effectively. In this paper, we introduce LexVLA, a more
interpretable VLA framework by learning a unified lexical representation for
both modalities without complex design. We use DINOv2 as our visual model for
its local-inclined features and Llama 2, a generative language model, to
leverage its in-context lexical prediction ability. To avoid the false
discovery, we propose an overuse penalty to refrain the lexical representation
from falsely frequently activating meaningless words. We demonstrate that these
two pre-trained uni-modal models can be well-aligned by fine-tuning on the
modest multi-modal dataset and avoid intricate training configurations. On
cross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal
dataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M)
and those trained from scratch on even bigger datasets (e.g., 1.1B data,
including CC-12M). We conduct extensive experiments to analyze LexVLA. Codes
are available at https://github.com/Clementine24/LexVLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual
  Language Modeling <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, Luke Zettlemoyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major consideration in multilingual language modeling is how to best
represent languages with diverse vocabularies and scripts. Although
contemporary text encoding methods cover most of the world's writing systems,
they exhibit bias towards the high-resource languages of the Global West. As a
result, texts of underrepresented languages tend to be segmented into long
sequences of linguistically meaningless units. To address the disparities, we
introduce a new paradigm that encodes the same information with segments of
consistent size across diverse languages. Our encoding convention (MYTE) is
based on morphemes, as their inventories are more balanced across languages
than characters, which are used in previous methods. We show that MYTE produces
shorter encodings for all 99 analyzed languages, with the most notable
improvements for non-European languages and non-Latin scripts. This, in turn,
improves multilingual LM performance and diminishes the perplexity gap
throughout diverse languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11032v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11032v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Written texts reflect an author's perspective, making the thorough analysis
of literature a key research method in fields such as the humanities and social
sciences. However, conventional text mining techniques like sentiment analysis
and topic modeling are limited in their ability to capture the hierarchical
narrative structures that reveal deeper argumentative patterns. To address this
gap, we propose a method that leverages large language models (LLMs) to extract
and organize these structures into a hierarchical framework. We validate this
approach by analyzing public opinions on generative AI collected by Japan's
Agency for Cultural Affairs, comparing the narratives of supporters and
critics. Our analysis provides clearer visualization of the factors influencing
divergent opinions on generative AI, offering deeper insights into the
structures of agreement and disagreement.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XPoint: A <span class="highlight-title">Self-Supervised</span> Visual-State-Space based Architecture for
  Multispectral Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismail Can Yagmur, Hasan F. Ates, Bahadir K. Gunturk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate multispectral image matching presents significant challenges due to
non-linear intensity variations across spectral modalities, extreme viewpoint
changes, and the scarcity of labeled datasets. Current state-of-the-art methods
are typically specialized for a single spectral difference, such as
visibleinfrared, and struggle to adapt to other modalities due to their
reliance on expensive supervision, such as depth maps or camera poses. To
address the need for rapid adaptation across modalities, we introduce XPoint, a
self-supervised, modular image-matching framework designed for adaptive
training and fine-tuning on aligned multispectral datasets, allowing users to
customize key components based on their specific tasks. XPoint employs
modularity and self-supervision to allow for the adjustment of elements such as
the base detector, which generates pseudoground truth keypoints invariant to
viewpoint and spectrum variations. The framework integrates a VMamba encoder,
pretrained on segmentation tasks, for robust feature extraction, and includes
three joint decoder heads: two are dedicated to interest point and descriptor
extraction; and a task-specific homography regression head imposes geometric
constraints for superior performance in tasks like image registration. This
flexible architecture enables quick adaptation to a wide range of modalities,
demonstrated by training on Optical-Thermal data and fine-tuning on settings
such as visual-near infrared, visual-infrared, visual-longwave infrared, and
visual-synthetic aperture radar. Experimental results show that XPoint
consistently outperforms or matches state-ofthe-art methods in feature matching
and image registration tasks across five distinct multispectral datasets. Our
source code is available at https://github.com/canyagmur/XPoint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 1 table, Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T2-Only Prostate Cancer Prediction by Meta-Learning from Bi-Parametric
  MR Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixi Yi, Yipei Wang, Natasha Thorley, Alexander Ng, Shonit Punwani, Veeru Kasivisvanathan, Dean C. Barratt, Shaheer Ullah Saeed, Yipeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current imaging-based prostate cancer diagnosis requires both MR T2-weighted
(T2w) and diffusion-weighted imaging (DWI) sequences, with additional sequences
for potentially greater accuracy improvement. However, measuring diffusion
patterns in DWI sequences can be time-consuming, prone to artifacts and
sensitive to imaging parameters. While machine learning (ML) models have
demonstrated radiologist-level accuracy in detecting prostate cancer from these
two sequences, this study investigates the potential of ML-enabled methods
using only the T2w sequence as input during inference time. We first discuss
the technical feasibility of such a T2-only approach, and then propose a novel
ML formulation, where DWI sequences - readily available for training purposes -
are only used to train a meta-learning model, which subsequently only uses T2w
sequences at inference. Using multiple datasets from more than 3,000 prostate
cancer patients, we report superior or comparable performance in localising
radiologist-identified prostate cancer using our proposed T2-only models,
compared with alternative models using T2-only or both sequences as input. Real
patient cases are presented and discussed to demonstrate, for the first time,
the exclusively true-positive cases from models with different input sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/wxyi057/MetaT2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-Space Semantic Invariance: Enhanced OOD Detection for Open-Set
  Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoliang Wang, Chen Zhao, Feng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set domain generalization addresses a real-world challenge: training a
model to generalize across unseen domains (domain generalization) while also
detecting samples from unknown classes not encountered during training
(open-set recognition). However, most existing approaches tackle these issues
separately, limiting their practical applicability. To overcome this
limitation, we propose a unified framework for open-set domain generalization
by introducing Feature-space Semantic Invariance (FSI). FSI maintains semantic
consistency across different domains within the feature space, enabling more
accurate detection of OOD instances in unseen domains. Additionally, we adopt a
generative model to produce synthetic data with novel domain styles or class
labels, enhancing model robustness. Initial experiments show that our method
improves AUROC by 9.1% to 18.9% on ColoredMNIST, while also significantly
increasing in-distribution classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE BigData 2024, Ph.D. Forum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning Client Pruning for Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Morafah, Hojin Chang, Chen Chen, Bill Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative model training across
decentralized edge devices while preserving data privacy. However, existing FL
methods often assume clean annotated datasets, impractical for
resource-constrained edge devices. In reality, noisy labels are prevalent,
posing significant challenges to FL performance. Prior approaches attempt label
correction and robust training techniques but exhibit limited efficacy,
particularly under high noise levels. This paper introduces ClipFL (Federated
Learning Client Pruning), a novel framework addressing noisy labels from a
fresh perspective. ClipFL identifies and excludes noisy clients based on their
performance on a clean validation dataset, tracked using a Noise Candidacy
Score (NCS). The framework comprises three phases: pre-client pruning to
identify potential noisy clients and calculate their NCS, client pruning to
exclude a percentage of clients with the highest NCS, and post-client pruning
for fine-tuning the global model with standard FL on clean clients. Empirical
evaluation demonstrates ClipFL's efficacy across diverse datasets and noise
levels, achieving accurate noisy client identification, superior performance,
faster convergence, and reduced communication costs compared to
state-of-the-art FL methods. Our code is available at
https://github.com/MMorafah/ClipFL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization of Brady-Yong Algorithm for Fast Hough Transform to
  Arbitrary Image Size <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danil Kazimirov, Dmitry Nikolaev, Ekaterina Rybakova, Arseniy Terekhin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, the Hough (discrete Radon) transform (HT/DRT) has proved to be an
extremely powerful and widespread tool harnessed in a number of application
areas, ranging from general image processing to X-ray computed tomography.
Efficient utilization of the HT to solve applied problems demands its
acceleration and increased accuracy. Along with this, most fast algorithms for
computing the HT, especially the pioneering Brady-Yong algorithm, operate on
power-of-two size input images and are not adapted for arbitrary size images.
This paper presents a new algorithm for calculating the HT for images of
arbitrary size. It generalizes the Brady-Yong algorithm from which it inherits
the optimal computational complexity. Moreover, the algorithm allows to compute
the HT with considerably higher accuracy compared to the existing algorithm.
Herewith, the paper provides a theoretical analysis of the computational
complexity and accuracy of the proposed algorithm. The conclusions of the
performed experiments conform with the theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures. Accepted to Symposium on Pattern Recognition and
  Applications 2024 (SPRA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Variational Autoencoders for Medical Image Generation: A
  Comprehensive Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khadija Rais, Mohamed Amroune, Abdelmadjid Benmachiche, Mohamed Yassine Haouam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational autoencoder (VAE) is one of the most common techniques in the
field of medical image generation, where this architecture has shown advanced
researchers in recent years and has developed into various architectures. VAE
has advantages including improving datasets by adding samples in smaller
datasets and in datasets with imbalanced classes, and this is how data
augmentation works. This paper provides a comprehensive review of studies on
VAE in medical imaging, with a special focus on their ability to create
synthetic images close to real data so that they can be used for data
augmentation. This study reviews important architectures and methods used to
develop VAEs for medical images and provides a comparison with other generative
models such as GANs on issues such as image quality, and low diversity of
generated samples. We discuss recent developments and applications in several
medical fields highlighting the ability of VAEs to improve segmentation and
classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for associated mpeg file, see
  https://worldresearchlibrary.org/proceeding.php?pid=6945</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fusion Balancing Through Game-Theoretic Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kontras, Thomas Strypsteen, Christos Chatzichristos, Paul P. Liang, Matthew Blaschko, Maarten De Vos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning can complete the picture of information extraction by
uncovering key dependencies between data sources. However, current systems fail
to fully leverage multiple modalities for optimal performance. This has been
attributed to modality competition, where modalities strive for training
resources, leaving some underoptimized. We show that current balancing methods
struggle to train multimodal models that surpass even simple baselines, such as
ensembles. This raises the question: how can we ensure that all modalities in
multimodal training are sufficiently trained, and that learning from new
modalities consistently improves performance? This paper proposes the
Multimodal Competition Regularizer (MCR), a new loss component inspired by
mutual information (MI) decomposition designed to prevent the adverse effects
of competition in multimodal training. Our key contributions are: 1)
Introducing game-theoretic principles in multimodal learning, where each
modality acts as a player competing to maximize its influence on the final
outcome, enabling automatic balancing of the MI terms. 2) Refining lower and
upper bounds for each MI term to enhance the extraction of task-relevant unique
and shared information across modalities. 3) Suggesting latent space
permutations for conditional MI estimation, significantly improving
computational efficiency. MCR outperforms all previously suggested training
strategies and is the first to consistently improve multimodal learning beyond
the ensemble baseline, clearly demonstrating that combining modalities leads to
significant performance gains on both synthetic and large real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures, 4 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $SE(3)$ Equivariant Ray Embeddings for Implicit Multi-View Depth
  Estimation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinshuang Xu, Dian Chen, Katherine Liu, Sergey Zakharov, Rares Ambrus, Kostas Daniilidis, Vitor Guizilini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating inductive bias by embedding geometric entities (such as rays)
as input has proven successful in multi-view learning. However, the methods
adopting this technique typically lack equivariance, which is crucial for
effective 3D learning. Equivariance serves as a valuable inductive prior,
aiding in the generation of robust multi-view features for 3D scene
understanding. In this paper, we explore the application of equivariant
multi-view learning to depth estimation, not only recognizing its significance
for computer vision and robotics but also addressing the limitations of
previous research. Most prior studies have either overlooked equivariance in
this setting or achieved only approximate equivariance through data
augmentation, which often leads to inconsistencies across different reference
frames. To address this issue, we propose to embed $SE(3)$ equivariance into
the Perceiver IO architecture. We employ Spherical Harmonics for positional
encoding to ensure 3D rotation equivariance, and develop a specialized
equivariant encoder and decoder within the Perceiver IO architecture. To
validate our model, we applied it to the task of stereo depth estimation,
achieving state of the art results on real-world datasets without explicit
geometric constraints or extensive data augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence-Informed Handheld Breast Ultrasound for
  Screening: A Systematic <span class="highlight-title">Review</span> of Diagnostic Test Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arianna Bunnell, Dustin Valdez, Fredrik Strand, Yannik Glaser, Peter Sadowski, John A. Shepherd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background. Breast cancer screening programs using mammography have led to
significant mortality reduction in high-income countries. However, many low-
and middle-income countries lack resources for mammographic screening. Handheld
breast ultrasound (BUS) is a low-cost alternative but requires substantial
training. Artificial intelligence (AI) enabled BUS may aid in both the
detection (perception) and classification (interpretation) of breast cancer.
Materials and Methods. This review (CRD42023493053) is reported in accordance
with the PRISMA (Preferred Reporting Items for Systematic Reviews and
Meta-Analysis) and SWiM (Synthesis Without Meta-analysis) guidelines. PubMed
and Google Scholar were searched from January 1, 2016 to December 12, 2023. A
meta-analysis was not attempted. Studies are grouped according to their AI task
type, application time, and AI task. Study quality is assessed using the
QUality Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2) tool. Results.
Of 763 candidate studies, 314 total full texts were reviewed. 34 studies are
included. The AI tasks of included studies are as follows: 1 frame selection, 6
detection, 11 segmentation, and 16 classification. In total, 5.7 million BUS
images from over 185,000 patients were used for AI training or validation. A
single study included a prospective testing set. 79% of studies were at high or
unclear risk of bias. Conclusion. There has been encouraging development of AI
for BUS. Despite studies demonstrating high performance across all identified
tasks, the evidence supporting AI-enhanced BUS generally lacks robustness.
High-quality model validation will be key to realizing the potential for
AI-enhanced BUS in increasing access to screening in resource-limited
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPU-Accelerated Inverse Lithography Towards High Quality Curvy Mask
  Generation <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Yang, Haoxing Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse Lithography Technology (ILT) has emerged as a promising solution for
photo mask design and optimization. Relying on multi-beam mask writers, ILT
enables the creation of free-form curvilinear mask shapes that enhance printed
wafer image quality and process window. However, a major challenge in
implementing curvilinear ILT for large-scale production is mask rule checking,
an area currently under development by foundries and EDA vendors. Although
recent research has incorporated mask complexity into the optimization process,
much of it focuses on reducing e-beam shots, which does not align with the
goals of curvilinear ILT. In this paper, we introduce a GPU-accelerated ILT
algorithm that improves not only contour quality and process window but also
the precision of curvilinear mask shapes. Our experiments on open benchmarks
demonstrate a significant advantage of our algorithm over leading academic ILT
engines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, Accepted by International Symposium on Physical
  Design (ISPD), 2025, Austin TX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLOctolyzer: Fully automatic analysis toolkit for segmentation and
  feature extracting in scanning laser ophthalmoscopy images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Burke, Samuel Gibbon, Justin Engelmann, Adam Threlfall, Ylenia Giarratano, Charlene Hamid, Stuart King, Ian J. C. MacCormick, Tom MacGillivray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: The purpose of this study was to introduce SLOctolyzer: an
open-source analysis toolkit for en face retinal vessels in infrared
reflectance scanning laser ophthalmoscopy (SLO) images.
  Methods: SLOctolyzer includes two main modules: segmentation and measurement.
The segmentation module uses deep learning methods to delineate retinal
anatomy, and detects the fovea and optic disc, whereas the measurement module
quantifies the complexity, density, tortuosity, and calibre of the segmented
retinal vessels. We evaluated the segmentation module using unseen data and
measured its reproducibility.
  Results: SLOctolyzer's segmentation module performed well against unseen
internal test data (Dice for all-vessels = 0.91; arteries = 0.84; veins = 0.85;
optic disc = 0.94; and fovea = 0.88). External validation against severe
retinal pathology showed decreased performance (Dice for arteries = 0.72; veins
= 0.75; and optic disc = 0.90). SLOctolyzer had good reproducibility (mean
difference for fractal dimension = -0.001; density = -0.0003; calibre = -0.32
microns; and tortuosity density = 0.001). SLOctolyzer can process a 768 x 768
pixel macula-centred SLO image in under 20 seconds and a disc-centred SLO image
in under 30 seconds using a laptop CPU.
  Conclusions: To our knowledge, SLOctolyzer is the first open-source tool to
convert raw SLO images into reproducible and clinically meaningful retinal
vascular parameters. SLO images are captured simultaneous to optical coherence
tomography (OCT), and we believe SLOctolyzer will be useful for extracting
retinal vascular measurements from large OCT image sets and linking them to
ocular or systemic diseases. It requires no specialist knowledge or proprietary
software, and allows manual correction of segmentations and re-computing of
vascular metrics. SLOctolyzer is freely available at
https://github.com/jaburke166/SLOctolyzer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 6 tables + Supplementary (9 pages, 13 figures, 4
  tables, 2 code listings). Accepted and published at ARVO Translational Vision
  Science and Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Diverse Device Heterogeneous Federated Learning via Task
  Arithmetic Knowledge Integration <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Morafah, Vyacheslav Kungurtsev, Hojin Chang, Chen Chen, Bill Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning has emerged as a promising paradigm for collaborative
machine learning, while preserving user data privacy. Despite its potential,
standard FL lacks support for diverse heterogeneous device prototypes, which
vary significantly in model and dataset sizes -- from small IoT devices to
large workstations. This limitation is only partially addressed by existing
knowledge distillation techniques, which often fail to transfer knowledge
effectively across a broad spectrum of device prototypes with varied
capabilities. This failure primarily stems from two issues: the dilution of
informative logits from more capable devices by those from less capable ones,
and the use of a single integrated logits as the distillation target across all
devices, which neglects their individual learning capacities and and the unique
contributions of each. To address these challenges, we introduce TAKFL, a novel
KD-based framework that treats the knowledge transfer from each device
prototype's ensemble as a separate task, independently distilling each to
preserve its unique contributions and avoid dilution. TAKFL also incorporates a
KD-based self-regularization technique to mitigate the issues related to the
noisy and unsupervised ensemble distillation process. To integrate the
separately distilled knowledge, we introduce an adaptive task arithmetic
knowledge integration process, allowing each student model to customize the
knowledge integration for optimal performance. Additionally, we present
theoretical results demonstrating the effectiveness of task arithmetic in
transferring knowledge across heterogeneous devices with varying capacities.
Comprehensive evaluations of our method across both CV and NLP tasks
demonstrate that TAKFL achieves SOTA results in a variety of datasets and
settings, significantly outperforming existing KD-based methods Code is
released at https://github.com/MMorafah/TAKFL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocCa: Visual <span class="highlight-title">Pretrain</span>ing with Location-aware Captioners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Wan, Michael Tschannen, Yongqin Xian, Filip Pavetic, Ibrahim Alabdulmohsin, Xiao Wang, André Susano Pinto, Andreas Steiner, Lucas Beyer, Xiaohua Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning has been shown as an effective pretraining method similar to
contrastive pretraining. However, the incorporation of location-aware
information into visual pretraining remains an area with limited research. In
this paper, we propose a simple visual pretraining method with location-aware
captioners (LocCa). LocCa uses a simple image captioner task interface, to
teach a model to read out rich information, i.e. bounding box coordinates, and
captions, conditioned on the image pixel input. Thanks to the multitask
capabilities of an encoder-decoder architecture, we show that an image
captioner can easily handle multiple tasks during pretraining. Our experiments
demonstrate that LocCa outperforms standard captioners significantly on
localization downstream tasks while maintaining comparable performance on
holistic tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeodesicPSIM: Predicting the Quality of Static Mesh with Texture Map via
  Geodesic Patch Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04928v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04928v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Yang, Joel Jung, Xiaozhong Xu, Shan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Static meshes with texture maps have attracted considerable attention in both
industrial manufacturing and academic research, leading to an urgent
requirement for effective and robust objective quality evaluation. However,
current model-based static mesh quality metrics have obvious limitations: most
of them only consider geometry information, while color information is ignored,
and they have strict constraints for the meshes' geometrical topology. Other
metrics, such as image-based and point-based metrics, are easily influenced by
the prepossessing algorithms, e.g., projection and sampling, hampering their
ability to perform at their best. In this paper, we propose Geodesic Patch
Similarity (GeodesicPSIM), a novel model-based metric to accurately predict
human perception quality for static meshes. After selecting a group keypoints,
1-hop geodesic patches are constructed based on both the reference and
distorted meshes cleaned by an effective mesh cleaning algorithm. A two-step
patch cropping algorithm and a patch texture mapping module refine the size of
1-hop geodesic patches and build the relationship between the mesh geometry and
color information, resulting in the generation of 1-hop textured geodesic
patches. Three types of features are extracted to quantify the distortion:
patch color smoothness, patch discrete mean curvature, and patch pixel color
average and variance. To the best of our knowledge, GeodesicPSIM is the first
model-based metric especially designed for static meshes with texture maps.
GeodesicPSIM provides state-of-the-art performance in comparison with
image-based, point-based, and video-based metrics on a newly created and
challenging database. We also prove the robustness of GeodesicPSIM by
introducing different settings of hyperparameters. Ablation studies also
exhibit the effectiveness of three proposed features and the patch cropping
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Sound Symbolism in Audio-visual Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Cheng Tseng, Yi-Jen Shih, David Harwath, Raymond Mooney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual pre-trained models have gained substantial attention recently
and demonstrated superior performance on various audio-visual tasks. This study
investigates whether pre-trained audio-visual models demonstrate non-arbitrary
associations between sounds and visual representations$\unicode{x2013}$known as
sound symbolism$\unicode{x2013}$which is also observed in humans. We developed
a specialized dataset with synthesized images and audio samples and assessed
these models using a non-parametric approach in a zero-shot setting. Our
findings reveal a significant correlation between the models' outputs and
established patterns of sound symbolism, particularly in models trained on
speech data. These results suggest that such models can capture sound-meaning
connections akin to human language processing, providing insights into both
cognitive architectures and machine learning strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teeth3DS+: An Extended Benchmark for Intraoral 3D Scans Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achraf Ben-Hamadou, Nour Neifar, Ahmed Rekik, Oussama Smaoui, Firas Bouzguenda, Sergi Pujades, Edmond Boyer, Edouard Ladroit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intraoral 3D scans analysis is a fundamental aspect of Computer-Aided
Dentistry (CAD) systems, playing a crucial role in various dental applications,
including teeth segmentation, detection, labeling, and dental landmark
identification. Accurate analysis of 3D dental scans is essential for
orthodontic and prosthetic treatment planning, as it enables automated
processing and reduces the need for manual adjustments by dental professionals.
However, developing robust automated tools for these tasks remains a
significant challenge due to the limited availability of high-quality public
datasets and benchmarks. This article introduces Teeth3DS+, the first
comprehensive public benchmark designed to advance the field of intraoral 3D
scan analysis. Developed as part of the 3DTeethSeg 2022 and 3DTeethLand 2024
MICCAI challenges, Teeth3DS+ aims to drive research in teeth identification,
segmentation, labeling, 3D modeling, and dental landmarks identification. The
dataset includes at least 1,800 intraoral scans (containing 23,999 annotated
teeth) collected from 900 patients, covering both upper and lower jaws
separately. All data have been acquired and validated by experienced
orthodontists and dental surgeons with over five years of expertise. Detailed
instructions for accessing the dataset are available at
https://crns-smartvision.github.io/teeth3ds
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Draft</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music Discovery Dialogue Generation Using Human Intent Analysis and
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeungHeon Doh, Keunwoo Choi, Daeyong Kwon, Taesu Kim, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A conversational music retrieval system can help users discover music that
matches their preferences through dialogue. To achieve this, a conversational
music retrieval system should seamlessly engage in multi-turn conversation by
1) understanding user queries and 2) responding with natural language and
retrieved music. A straightforward solution would be a data-driven approach
utilizing such conversation logs. However, few datasets are available for the
research and are limited in terms of volume and quality. In this paper, we
present a data generation framework for rich music discovery dialogue using a
large language model (LLM) and user intents, system actions, and musical
attributes. This is done by i) dialogue intent analysis using grounded theory,
ii) generating attribute sequences via cascading database filtering, and iii)
generating utterances using large language models. By applying this framework
to the Million Song dataset, we create LP-MusicDialog, a Large Language Model
based Pseudo Music Dialogue dataset, containing over 288k music conversations
using more than 319k music items. Our evaluation shows that the synthetic
dataset is competitive with an existing, small human dialogue dataset in terms
of dialogue consistency, item relevance, and naturalness. Furthermore, using
the dataset, we train a conversational music retrieval model and show promising
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 25th International Society for Music
  Information Retrieval Conference (ISMIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Shapley index for music streaming platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Bergantiños, Juan D. Moreno-Ternero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study an index to measure the popularity of artists in music streaming
platforms. This index, which can be used to allocate the amount raised via paid
subscriptions among participating artists, is based on the Shapley value, a
centerpiece in cooperative game theory. We characterize this Shapley index
combining several axioms formalizing principles with normative appeal. This
permits to place the index in the literature, as an alternative to the
well-known (and widely used in the industry) pro-rata and user-centric indices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Liu, Liang Zhang, Qian Li, Jianghua Wu, Guangxu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has shown impressive capability in
providing reliable answer predictions and addressing hallucination problems. A
typical RAG implementation uses powerful retrieval models to extract external
information and large language models (LLMs) to generate answers. In contrast,
recent LLM-based retrieval has gained attention for its substantial
improvements in information retrieval (IR) due to the LLMs' semantic
understanding capability. However, directly applying LLM to RAG systems
presents challenges. This may cause feature locality problems as massive
parametric knowledge can hinder effective usage of global information across
the corpus; for example, an LLM-based retriever often inputs document summaries
instead of full documents. Moreover, various pre-trained tasks in LLMs
introduce variance, further weakening performance as a retriever.
  To address these issues, we propose a novel two-stage fine-tuning
architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever
is constructed by integrating LoRA-based representation learning to tackle
feature locality issues. To enhance retrieval performance, we develop two
patterns (invariant and variant patterns) and an invariance loss to reduce LLM
variance. In the generation stage, a refined fine-tuning method is employed to
improve LLM accuracy in generating answers based on retrieved information.
Experimental results show that Invar-RAG significantly outperforms existing
baselines across three open-domain question answering (ODQA) datasets. Code is
available in the Supplementary Material for reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikiya Takehi, Ellen M. Voorhees, Tetsuya Sakai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test collections are information retrieval tools that allow researchers to
quickly and easily evaluate ranking algorithms. While test collections have
become an integral part of IR research, the process of data creation involves
significant efforts in manual annotations, which often makes it very expensive
and time-consuming. Thus, the test collections could become small when the
budget is limited, which may lead to unstable evaluations. As an alternative,
recent studies have proposed the use of large language models (LLMs) to
completely replace human assessors. However, while LLMs seem to somewhat
correlate with human judgments, they are not perfect and often show bias.
Moreover, even if a well-performing LLM or prompt is found on one dataset,
there is no guarantee that it will perform similarly in practice, due to
difference in tasks and data. Thus a complete replacement with LLMs is argued
to be too risky and not fully trustable.
  Thus, in this paper, we propose \textbf{L}LM-\textbf{A}ssisted
\textbf{R}elevance \textbf{A}ssessments (\textbf{LARA}), an effective method to
balance manual annotations with LLM annotations, which helps to make a rich and
reliable test collection. We use the LLM's predicted relevance probabilities in
order to select the most profitable documents to manually annotate under a
budget constraint. While solely relying on LLM's predicted probabilities to
manually annotate performs fairly well, with theoretical reasoning, LARA guides
the human annotation process even more effectively via online calibration
learning. Then, using the calibration model learned from the limited manual
annotations, LARA debiases the LLM predictions to annotate the remaining
non-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc
datasets show that LARA outperforms the alternative solutions under almost any
budget constraint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Conditional Expert Selection Network for Multi-domain
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuiyao Dong, Xingyu Lou, Feng Liu, Ruian Wang, Wenyi Yu, Ping Wang, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MOE) has recently become the de facto standard in
Multi-domain recommendation (MDR) due to its powerful expressive ability.
However, such MOE-based method typically employs all experts for each instance,
leading to scalability issue and low-discriminability between domains and
experts. Furthermore, the design of commonly used domain-specific networks
exacerbates the scalability issues. To tackle the problems, We propose a novel
method named CESAA consists of Conditional Expert Selection (CES) Module and
Adaptive Expert Aggregation (AEA) Module to tackle these challenges.
Specifically, CES first combines a sparse gating strategy with domain-shared
experts. Then AEA utilizes mutual information loss to strengthen the
correlations between experts and specific domains, and significantly improve
the distinction between experts. As a result, only domain-shared experts and
selected domain-specific experts are activated for each instance, striking a
balance between computational efficiency and model performance. Experimental
results on both public ranking and industrial retrieval datasets verify the
effectiveness of our method in MDR tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model in Medical Informatics: Direct Classification and
  Enhanced Text Representations for Automatic ICD Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyd Boukhers, AmeerAli Khan, Qusai Ramadan, Cong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the complexity of accurately classifying International
Classification of Diseases (ICD) codes from medical discharge summaries is
challenging due to the intricate nature of medical documentation. This paper
explores the use of Large Language Models (LLM), specifically the LLAMA
architecture, to enhance ICD code classification through two methodologies:
direct application as a classifier and as a generator of enriched text
representations within a Multi-Filter Residual Convolutional Neural Network
(MultiResCNN) framework. We evaluate these methods by comparing them against
state-of-the-art approaches, revealing LLAMA's potential to significantly
improve classification outcomes by providing deep contextual insights into
medical texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at the 2024 IEEE International Conference on Bioinformatics
  and Biomedicine (BIBM 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AssistRAG: Boosting the Potential of Large Language Models with an
  Intelligent Information Assistant <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Zhou, Zheng Liu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as "hallucination". Initial retrieval-augmented
generation (RAG) methods like the "Retrieve-Read" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 (poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting the Targeted Transferability of Adversarial Examples via
  Salient Region & Weighted Feature Drop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanjun Xu, Linghui Li, Kaiguo Yuan, Bingyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks can be vulnerable to adversarially crafted examples,
presenting significant risks to practical applications. A prevalent approach
for adversarial attacks relies on the transferability of adversarial examples,
which are generated from a substitute model and leveraged to attack unknown
black-box models. Despite various proposals aimed at improving transferability,
the success of these attacks in targeted black-box scenarios is often hindered
by the tendency for adversarial examples to overfit to the surrogate models. In
this paper, we introduce a novel framework based on Salient region & Weighted
Feature Drop (SWFD) designed to enhance the targeted transferability of
adversarial examples. Drawing from the observation that examples with higher
transferability exhibit smoother distributions in the deep-layer outputs, we
propose the weighted feature drop mechanism to modulate activation values
according to weights scaled by norm distribution, effectively addressing the
overfitting issue when generating adversarial examples. Additionally, by
leveraging salient region within the image to construct auxiliary images, our
method enables the adversarial example's features to be transferred to the
target category in a model-agnostic manner, thereby enhancing the
transferability. Comprehensive experiments confirm that our approach
outperforms state-of-the-art methods across diverse configurations. On average,
the proposed SWFD raises the attack success rate for normally trained models
and robust models by 16.31% and 7.06% respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge
  into LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Wu, Kevin Wu, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is great interest in fine-tuning frontier large language models (LLMs)
to inject new information and update existing knowledge. While commercial LLM
fine-tuning APIs from providers such as OpenAI and Google promise flexible
adaptation for various applications, the efficacy of fine-tuning remains
unclear. In this study, we introduce FineTuneBench, an evaluation framework and
dataset for understanding how well commercial fine-tuning APIs can successfully
learn new and updated knowledge. We analyze five frontier LLMs with
commercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro,
on their effectiveness in two settings: (1) ingesting novel information, such
as recent news events and new people profiles, and (2) updating existing
knowledge, such as updated medical guidelines and code frameworks. Our results
reveal substantial shortcomings in all the models' abilities to effectively
learn new information through fine-tuning, with an average generalization
accuracy of 37% across all models. When updating existing knowledge, such as
incorporating medical guideline updates, commercial fine-tuning APIs show even
more limited capability (average generalization accuracy of 19%). Overall,
fine-tuning GPT-4o mini is the most effective for infusing new knowledge and
updating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs
for Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or
update existing knowledge. These findings underscore a major shortcoming in
using current commercial fine-tuning services to achieve reliable knowledge
infusion in common scenarios. We open source the FineTuneBench dataset at
https://github.com/kevinwu23/StanfordFineTuneBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Know Your Neighborhood: General and Zero-Shot Capable Binary Function
  Search Powered by Call Graphlets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Collyer, Tim Watson, Iain Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary code similarity detection is an important problem with applications in
areas such as malware analysis, vulnerability research and license violation
detection. This paper proposes a novel graph neural network architecture
combined with a novel graph data representation called call graphlets. A call
graphlet encodes the neighborhood around each function in a binary executable,
capturing the local and global context through a series of statistical
features. A specialized graph neural network model operates on this graph
representation, learning to map it to a feature vector that encodes semantic
binary code similarities using deep-metric learning. The proposed approach is
evaluated across five distinct datasets covering different architectures,
compiler tool chains, and optimization levels. Experimental results show that
the combination of call graphlets and the novel graph neural network
architecture achieves comparable or state-of-the-art performance compared to
baseline techniques across cross-architecture, mono-architecture and zero shot
tasks. In addition, our proposed approach also performs well when evaluated
against an out-of-domain function inlining task. The work provides a general
and effective graph neural network-based solution for conducting binary code
similarity detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Under-Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CROLoss: Towards a Customizable Loss for Retrieval Models in Recommender
  Systems <span class="chip">CIKM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02971v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02971v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxiang Tang, Wentao Bai, Guilin Li, Xialong Liu, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large-scale recommender systems, retrieving top N relevant candidates
accurately with resource constrain is crucial. To evaluate the performance of
such retrieval models, Recall@N, the frequency of positive samples being
retrieved in the top N ranking, is widely used. However, most of the
conventional loss functions for retrieval models such as softmax cross-entropy
and pairwise comparison methods do not directly optimize Recall@N. Moreover,
those conventional loss functions cannot be customized for the specific
retrieval size N required by each application and thus may lead to sub-optimal
performance. In this paper, we proposed the Customizable Recall@N Optimization
Loss (CROLoss), a loss function that can directly optimize the Recall@N metrics
and is customizable for different choices of N. This proposed CROLoss
formulation defines a more generalized loss function space, covering most of
the conventional loss functions as special cases. Furthermore, we develop the
Lambda method, a gradient-based method that invites more flexibility and can
further boost the system performance. We evaluate the proposed CROLoss on two
public benchmark datasets. The results show that CROLoss achieves SOTA results
over conventional loss functions for both datasets with various choices of
retrieval size N. CROLoss has been deployed onto our online E-commerce
advertising platform, where a fourteen-day online A/B test demonstrated that
CROLoss contributes to a significant business revenue growth of 4.75%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures. Accepted by by CIKM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entity Extraction from High-Level Corruption Schemes via Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Koletsis, Panagiotis-Konstantinos Gemos, Christos Chronis, Iraklis Varlamis, Vasilis Efthymiou, Georgios Th. Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of financial crime that has been observed in recent years has
created an increasing concern around the topic and many people, organizations
and governments are more and more frequently trying to combat it. Despite the
increase of interest in this area, there is a lack of specialized datasets that
can be used to train and evaluate works that try to tackle those problems. This
article proposes a new micro-benchmark dataset for algorithms and models that
identify individuals and organizations, and their multiple writings, in news
articles, and presents an approach that assists in its creation. Experimental
efforts are also reported, using this dataset, to identify individuals and
organizations in financial-crime-related articles using various low-billion
parameter Large Language Models (LLMs). For these experiments, standard metrics
(Accuracy, Precision, Recall, F1 Score) are reported and various prompt
variants comprising the best practices of prompt engineering are tested. In
addition, to address the problem of ambiguous entity mentions, a simple, yet
effective LLM-based disambiguation method is proposed, ensuring that the
evaluation aligns with reality. Finally, the proposed approach is compared
against a widely used state-of-the-art open-source baseline, showing the
superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OAEI-LLM: A Benchmark <span class="highlight-title">Dataset</span> for Understanding Large Language Model
  Hallucinations in Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14038v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14038v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations of large language models (LLMs) commonly occur in
domain-specific downstream tasks, with no exception in ontology matching (OM).
The prevalence of using LLMs for OM raises the need for benchmarks to better
understand LLM hallucinations. The OAEI-LLM dataset is an extended version of
the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate
LLM-specific hallucinations in OM tasks. We outline the methodology used in
dataset construction and schema extension, and provide examples of potential
use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TF-DCon: Leveraging Large Language Models (LLMs) to Empower
  Training-Free <span class="highlight-title">Dataset</span> Condensation for Content-Based Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai Liu, Qing Li, Xiao-Ming Wu, Ke Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern techniques in Content-based Recommendation (CBR) leverage item content
information to provide personalized services to users, but suffer from
resource-intensive training on large datasets. To address this issue, we
explore the dataset condensation for textual CBR in this paper. The goal of
dataset condensation is to synthesize a small yet informative dataset, upon
which models can achieve performance comparable to those trained on large
datasets. While existing condensation approaches are tailored to classification
tasks for continuous data like images or embeddings, direct application of them
to CBR has limitations. To bridge this gap, we investigate efficient dataset
condensation for content-based recommendation. Inspired by the remarkable
abilities of large language models (LLMs) in text comprehension and generation,
we leverage LLMs to empower the generation of textual content during
condensation. To handle the interaction data involving both users and items, we
devise a dual-level condensation method: content-level and user-level. At
content-level, we utilize LLMs to condense all contents of an item into a new
informative title. At user-level, we design a clustering-based synthesis
module, where we first utilize LLMs to extract user interests. Then, the user
interests and user embeddings are incorporated to condense users and generate
interactions for condensed users. Notably, the condensation paradigm of this
method is forward and free from iterative optimization on the synthesized
dataset. Extensive empirical findings from our study, conducted on three
authentic datasets, substantiate the efficacy of the proposed method.
Particularly, we are able to approximate up to 97% of the original performance
while reducing the dataset size by 95% (i.e., on dataset MIND).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An updated version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Just Label the Repeats for In-The-Wild Audio-to-Score Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irmak Bukey, Michael Feffer, Chris Donahue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an efficient workflow for high-quality offline alignment of
in-the-wild performance audio and corresponding sheet music scans (images).
Recent work on audio-to-score alignment extends dynamic time warping (DTW) to
be theoretically able to handle jumps in sheet music induced by repeat
signs-this method requires no human annotations, but we show that it often
yields low-quality alignments. As an alternative, we propose a workflow and
interface that allows users to quickly annotate jumps (by clicking on repeat
signs), requiring a small amount of human supervision but yielding much higher
quality alignments on average. Additionally, we refine audio and score feature
representations to improve alignment quality by: (1) integrating measure
detection into the score feature representation, and (2) using raw onset
prediction probabilities from a music transcription model instead of piano
roll. We propose an evaluation protocol for audio-to-score alignment that
computes the distance between the estimated and ground truth alignment in units
of measures. Under this evaluation, we find that our proposed jump annotation
workflow and improved feature representations together improve alignment
accuracy by 150% relative to prior work (33% to 82%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25th International Society for Music Information Retrieval
  Conference, San Francisco, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fusion Balancing Through Game-Theoretic Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kontras, Thomas Strypsteen, Christos Chatzichristos, Paul P. Liang, Matthew Blaschko, Maarten De Vos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning can complete the picture of information extraction by
uncovering key dependencies between data sources. However, current systems fail
to fully leverage multiple modalities for optimal performance. This has been
attributed to modality competition, where modalities strive for training
resources, leaving some underoptimized. We show that current balancing methods
struggle to train multimodal models that surpass even simple baselines, such as
ensembles. This raises the question: how can we ensure that all modalities in
multimodal training are sufficiently trained, and that learning from new
modalities consistently improves performance? This paper proposes the
Multimodal Competition Regularizer (MCR), a new loss component inspired by
mutual information (MI) decomposition designed to prevent the adverse effects
of competition in multimodal training. Our key contributions are: 1)
Introducing game-theoretic principles in multimodal learning, where each
modality acts as a player competing to maximize its influence on the final
outcome, enabling automatic balancing of the MI terms. 2) Refining lower and
upper bounds for each MI term to enhance the extraction of task-relevant unique
and shared information across modalities. 3) Suggesting latent space
permutations for conditional MI estimation, significantly improving
computational efficiency. MCR outperforms all previously suggested training
strategies and is the first to consistently improve multimodal learning beyond
the ensemble baseline, clearly demonstrating that combining modalities leads to
significant performance gains on both synthetic and large real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures, 4 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low Complexity Learning-based Lossless Event-based Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmadreza Sezavar, Catarina Brites, Joao Ascenso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are a cutting-edge type of visual sensors that capture data by
detecting brightness changes at the pixel level asynchronously. These cameras
offer numerous benefits over conventional cameras, including high temporal
resolution, wide dynamic range, low latency, and lower power consumption.
However, the substantial data rates they produce require efficient compression
techniques, while also fulfilling other typical application requirements, such
as the ability to respond to visual changes in real-time or near real-time.
Additionally, many event-based applications demand high accuracy, making
lossless coding desirable, as it retains the full detail of the sensor data.
Learning-based methods show great potential due to their ability to model the
unique characteristics of event data thus allowing to achieve high compression
rates. This paper proposes a low-complexity lossless coding solution based on
the quadtree representation that outperforms traditional compression algorithms
in efficiency and speed, ensuring low computational complexity and minimal
delay for real-time applications. Experimental results show that the proposed
method delivers better compression ratios, i.e., with fewer bits per event, and
lower computational complexity compared to current lossless data compression
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hierarchical Compression Technique for 3D Gaussian Splatting
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Huang, Wenjie Huang, Qi Yang, Yiling Xu, Zhu li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (GS) demonstrates excellent rendering quality and
generation speed in novel view synthesis. However, substantial data size poses
challenges for storage and transmission, making 3D GS compression an essential
technology. Current 3D GS compression research primarily focuses on developing
more compact scene representations, such as converting explicit 3D GS data into
implicit forms. In contrast, compression of the GS data itself has hardly been
explored. To address this gap, we propose a Hierarchical GS Compression (HGSC)
technique. Initially, we prune unimportant Gaussians based on importance scores
derived from both global and local significance, effectively reducing
redundancy while maintaining visual quality. An Octree structure is used to
compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical
attribute compression strategy by employing a KD-tree to partition the 3D GS
into multiple blocks. We apply farthest point sampling to select anchor
primitives within each block and others as non-anchor primitives with varying
Levels of Details (LoDs). Anchor primitives serve as reference points for
predicting non-anchor primitives across different LoDs to reduce spatial
redundancy. For anchor primitives, we use the region adaptive hierarchical
transform to achieve near-lossless compression of various attributes. For
non-anchor primitives, each is predicted based on the k-nearest anchor
primitives. To further minimize prediction errors, the reconstructed LoD and
anchor primitives are combined to form new anchor primitives to predict the
next LoD. Our method notably achieves superior compression quality and a
significant data size reduction of over 4.5 times compared to the
state-of-the-art compression method on small scenes datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JPEG AI Image Compression Visual Artifacts: Detection Methods and
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daria Tsereh, Mark Mirgaleev, Ivan Molodetskikh, Roman Kazantsev, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based image compression methods have improved in recent years and
started to outperform traditional codecs. However, neural-network approaches
can unexpectedly introduce visual artifacts in some images. We therefore
propose methods to separately detect three types of artifacts (texture and
boundary degradation, color change, and text corruption), to localize the
affected regions, and to quantify the artifact strength. We consider only those
regions that exhibit distortion due solely to the neural compression but that a
traditional codec recovers successfully at a comparable bitrate. We employed
our methods to collect artifacts for the JPEG AI verification model with
respect to HM-18.0, the H.265 reference software. We processed about 350,000
unique images from the Open Images dataset using different compression-quality
parameters; the result is a dataset of 46,440 artifacts validated through
crowd-sourced subjective assessment. Our proposed dataset and methods are
valuable for testing neural-network-based image codecs, identifying bugs in
these codecs, and enhancing their performance. We make source code of the
methods and the dataset publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss-tolerant neural video codec aware congestion control for real time
  video communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxu Xia, Hanchen Li, Junchen Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Because of reinforcement learning's (RL) ability to automatically create more
adaptive controlling logics beyond the hand-crafted heuristics, numerous effort
has been made to apply RL to congestion control (CC) design for real time video
communication (RTC) applications and has successfully shown promising benefits
over the rule-based RTC CCs. Online reinforcement learning is often adopted to
train the RL models so the models can directly adapt to real network
environments. However, its trail-and-error manner can also cause catastrophic
degradation of the quality of experience (QoE) of RTC application at run time.
Thus, safeguard strategies such as falling back to hand-crafted heuristics can
be used to run along with RL models to guarantee the actions explored in the
training sensible, despite that these safeguard strategies interrupt the
learning process and make it more challenging to discover optimal RL policies.
  The recent emergence of loss-tolerant neural video codecs (NVC) naturally
provides a layer of protection for the online learning of RL-based congestion
control because of its resilience to packet losses, but such packet loss
resilience have not been fully exploited in prior works yet. In this paper, we
present a reinforcement learning (RL) based congestion control which can be
aware of and takes advantage of packet loss tolerance characteristic of NVCs
via reward in online RL learning. Through extensive evaluation on various
videos and network traces in a simulated environment, we demonstrate that our
NVC-aware CC running with the loss-tolerant NVC reduces the training time by
41\% compared to other prior RL-based CCs. It also boosts the mean video
quality by 0.3 to 1.6dB, lower the tail frame delay by 3 to 200ms, and reduces
the video stalls by 20\% to 77\% in comparison with other baseline RTC CCs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking Illusions: Understanding Human Perception of Audiovisual
  Deepfakes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of contemporary deepfakes has attracted significant attention
in machine learning research, as artificial intelligence (AI) generated
synthetic media increases the incidence of misinterpretation and is difficult
to distinguish from genuine content. Currently, machine learning techniques
have been extensively studied for automatically detecting deepfakes. However,
human perception has been less explored. Malicious deepfakes could ultimately
cause public and social problems. Can we humans correctly perceive the
authenticity of the content of the videos we watch? The answer is obviously
uncertain; therefore, this paper aims to evaluate the human ability to discern
deepfake videos through a subjective study. We present our findings by
comparing human observers to five state-ofthe-art audiovisual deepfake
detection models. To this end, we used gamification concepts to provide 110
participants (55 native English speakers and 55 non-native English speakers)
with a webbased platform where they could access a series of 40 videos (20 real
and 20 fake) to determine their authenticity. Each participant performed the
experiment twice with the same 40 videos in different random orders. The videos
are manually selected from the FakeAVCeleb dataset. We found that all AI models
performed better than humans when evaluated on the same 40 videos. The study
also reveals that while deception is not impossible, humans tend to
overestimate their detection capabilities. Our experimental results may help
benchmark human versus machine performance, advance forensics analysis, and
enable adaptive countermeasures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Summarization: Towards Entity-Aware Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hammad A. Ayyubi, Tianqi Liu, Arsha Nagrani, Xudong Lin, Mingda Zhang, Anurag Arnab, Feng Han, Yukun Zhu, Jialu Liu, Shih-Fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing popular video captioning benchmarks and models deal with generic
captions devoid of specific person, place or organization named entities. In
contrast, news videos present a challenging setting where the caption requires
such named entities for meaningful summarization. As such, we propose the task
of summarizing news video directly to entity-aware captions. We also release a
large-scale dataset, VIEWS (VIdeo NEWS), to support research on this task.
Further, we propose a method that augments visual information from videos with
context retrieved from external world knowledge to generate entity-aware
captions. We demonstrate the effectiveness of our approach on three video
captioning models. We also show that our approach generalizes to existing news
image captions dataset. With all the extensive experiments and insights, we
believe we establish a solid basis for future research on this challenging
task.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-10T00:00:00Z">2024-11-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Mixcode Popular Songs with Artificial Intelligence: Concepts,
  Plans, and Speculations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Kaushik, Kayla Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music is a potent form of expression that can communicate, accentuate or even
create the emotions of an individual or a collective. Both historically and in
contemporary experiences, musical expression was and is commonly
instrumentalized for social, political and/or economic purposes. Generative
artificial intelligence provides a wealth of both opportunities and challenges
with regard to music and its role in society. This paper discusses a proposed
project integrating artificial intelligence and popular music, with the
ultimate goal of creating a powerful tool for implementing music for social
transformation, education, healthcare, and emotional well-being. Given that it
is being presented at the outset of a collaboration between a computer
scientist/data analyst and an ethnomusicologist/social anthropologist. it is
mainly conceptual and somewhat speculative in nature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link to the paper:https://aimc2024.pubpub.org/pub/rdulfbve/release/1
  Published in The International Conference on AI and Musical Creativity at the
  University of Oxford (2024) https://aimc2024.pubpub.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric Learning for Tag Recommendation: Tackling Data Sparsity and Cold
  Start Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanshuai Luo, Rui Wang, Yaxin Liang, Ankai Liang, Wenyi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of digital information, personalized recommendation
systems have become an indispensable part of Internet services, especially in
the fields of e-commerce, social media, and online entertainment. However,
traditional collaborative filtering and content-based recommendation methods
have limitations in dealing with data sparsity and cold start problems,
especially in the face of largescale heterogeneous data, which makes it
difficult to meet user expectations. This paper proposes a new label
recommendation algorithm based on metric learning, which aims to overcome the
challenges of traditional recommendation systems by learning effective distance
or similarity metrics to capture the subtle differences between user
preferences and item features. Experimental results show that the algorithm
outperforms baseline methods including local response metric learning (LRML),
collaborative metric learning (CML), and adaptive tensor factorization (ATF)
based on adversarial learning on multiple evaluation metrics. In particular, it
performs particularly well in the accuracy of the first few recommended items,
while maintaining high robustness and maintaining high recommendation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Knowing When You Don't Know": A Multilingual Relevance Assessment
  <span class="highlight-title">Dataset</span> for Robust Retrieval-Augmented Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11361v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11361v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)
output by leveraging external knowledge sources to reduce factual
hallucinations. However, prior work lacks a comprehensive evaluation of
different language families, making it challenging to evaluate LLM robustness
against errors in external retrieved knowledge. To overcome this, we establish
NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across
18 typologically diverse languages. NoMIRACL includes both a non-relevant and a
relevant subset. Queries in the non-relevant subset contain passages judged as
non-relevant, whereas queries in the relevant subset include at least a single
judged relevant passage. We measure relevance assessment using: (i)
hallucination rate, measuring model tendency to hallucinate, when the answer is
not present in passages in the non-relevant subset, and (ii) error rate,
measuring model inaccuracy to recognize relevant passages in the relevant
subset.In our work, we observe that most models struggle to balance the two
capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination
rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can
achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is
observed to provide the best tradeoff on both subsets, highlighting future work
necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are
available at: https://github.com/project-miracl/nomiracl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QDA-SQL: Questions Enhanced Dialogue Augmentation for Multi-Turn
  Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinggang Sun, Ziming Guo, Haining Yu, Chuanyi Liu, Xiang Li, Bingxuan Wang, Xiangzhan Yu, Tiancheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) for specific domain tasks has
achieved great success in Text-to-SQL tasks. However, these fine-tuned models
often face challenges with multi-turn Text-to-SQL tasks caused by ambiguous or
unanswerable questions. It is desired to enhance LLMs to handle multiple types
of questions in multi-turn Text-to-SQL tasks. To address this, we propose a
novel data augmentation method, called QDA-SQL, which generates multiple types
of multi-turn Q\&A pairs using LLMs. In QDA-SQL, we introduce a method
incorporating validation and correction mechanisms to handle complex multi-turn
Text-to-SQL tasks. Experimental results demonstrate that QDA-SQL enables
fine-tuned models to exhibit higher performance on SQL statement accuracy and
enhances their ability to handle complex, unanswerable questions in multi-turn
Text-to-SQL tasks. The generation script and test set are released at
https://github.com/mcxiaoxiao/QDA-SQL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SBI-RAG: Enhancing Math Word Problem Solving for Students through
  Schema-Based Instruction and Retrieval-Augmented Generation <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prakhar Dixit, Tim Oates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many students struggle with math word problems (MWPs), often finding it
difficult to identify key information and select the appropriate mathematical
operations. Schema-based instruction (SBI) is an evidence-based strategy that
helps students categorize problems based on their structure, improving
problem-solving accuracy. Building on this, we propose a Schema-Based
Instruction Retrieval-Augmented Generation (SBI-RAG) framework that
incorporates a large language model (LLM). Our approach emphasizes step-by-step
reasoning by leveraging schemas to guide solution generation. We evaluate its
performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo,
and introduce a "reasoning score" metric to assess solution quality. Our
findings suggest that SBI-RAG enhances reasoning clarity and facilitates a more
structured problem-solving process potentially providing educational benefits
for students.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 4th MATH-AI Workshop at NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LML-DAP: Language Model Learning a <span class="highlight-title">Dataset</span> for Data-Augmented Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18957v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18957v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification tasks are typically handled using Machine Learning (ML)
models, which lack a balance between accuracy and interpretability. This paper
introduces a new approach for classification tasks using Large Language Models
(LLMs) in an explainable method. Unlike ML models, which rely heavily on data
cleaning and feature engineering, this method streamlines the process using
LLMs. This paper proposes a method called "Language Model Learning (LML)"
powered by a new method called "Data-Augmented Prediction (DAP)." The
classification is performed by LLMs using a method similar to that used by
humans who manually explore and understand the data to decide classifications.
In the process of LML, a dataset is summarized and evaluated to determine the
features leading to each label the most. In the DAP process, the system uses
the data summary and a row of the testing dataset to automatically generate a
query to retrieve relevant rows from the dataset for context-aware
classification. LML and DAP unlock new possibilities in areas that require
explainable and context-aware decisions by ensuring satisfactory accuracy even
with complex data. The system scored an accuracy above 90% in some test cases,
confirming the effectiveness and potential of the system to outperform ML
models in various scenarios. The source code is available at
https://github.com/Pro-GenAI/LML-DAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Made the abstract and the content clearer</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMD: Autoregressive Motion Diffusion <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09381v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09381v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Han, Hao Peng, Minjing Dong, Yi Ren, Yixuan Shen, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion generation aims to produce plausible human motion sequences
according to various conditional inputs, such as text or audio. Despite the
feasibility of existing methods in generating motion based on short prompts and
simple motion patterns, they encounter difficulties when dealing with long
prompts or complex motions. The challenges are two-fold: 1) the scarcity of
human motion-captured data for long prompts and complex motions. 2) the high
diversity of human motions in the temporal domain and the substantial
divergence of distributions from conditional modalities, leading to a
many-to-many mapping problem when generating motion with complex and long
texts. In this work, we address these gaps by 1) elaborating the first dataset
pairing long textual descriptions and 3D complex motions (HumanLong3D), and 2)
proposing an autoregressive motion diffusion model (AMD). Specifically, AMD
integrates the text prompt at the current timestep with the text prompt and
action sequences at the previous timestep as conditional information to predict
the current action sequences in an iterative manner. Furthermore, we present
its generalization for X-to-Motion with "No Modality Left Behind", enabling the
generation of high-definition and high-fidelity human motions based on
user-defined modality input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAAI2024. Official Code:
  https://github.com/fluide1022/AMD</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-09T00:00:00Z">2024-11-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GuidelineGuard: An Agentic Framework for Medical Note Evaluation with
  Guideline Adherence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MD Ragib Shahriyear
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although rapid advancements in Large Language Models (LLMs) are facilitating
the integration of artificial intelligence-based applications and services in
healthcare, limited research has focused on the systematic evaluation of
medical notes for guideline adherence. This paper introduces GuidelineGuard, an
agentic framework powered by LLMs that autonomously analyzes medical notes,
such as hospital discharge and office visit notes, to ensure compliance with
established healthcare guidelines. By identifying deviations from recommended
practices and providing evidence-based suggestions, GuidelineGuard helps
clinicians adhere to the latest standards from organizations like the WHO and
CDC. This framework offers a novel approach to improving documentation quality
and reducing clinical errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotative Indexing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles L. A. Clarke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces annotative indexing, a novel framework that unifies and
generalizes traditional inverted indexes, column stores, object stores, and
graph databases. As a result, annotative indexing can provide the underlying
indexing framework for databases that support knowledge graphs, entity
retrieval, semi-structured data, and ranked retrieval. While we primarily focus
on human language data in the form of text, annotative indexing is sufficiently
general to support a range of other datatypes, and we provide examples of
SQL-like queries over a JSON store that includes numbers and dates. Taking
advantage of the flexibility of annotative indexing, we also demonstrate a
fully dynamic annotative index incorporating support for ACID properties of
transactions with hundreds of multiple concurrent readers and writers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Eric Gaussier, Juntao Li, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) like Llama has
significantly advanced information retrieval (IR) systems. However, using LLMs
for long documents, as in RankLLaMA, remains challenging due to computational
complexity, especially concerning input token length. Furthermore, the internal
mechanisms of LLMs during ranking are still not fully understood. In this
paper, we first explore the internal workings of LLMs during relevance
judgement and identify that specific attention heads play a crucial role in
aligning relevant tokens. This observation inspires us to revisit the block
pre-ranking strategy used in KeyB, which remains state-of-the-art (SOTA) on the
TREC 2019 DL document ranking dataset. Building on these insights, we develop
KeyB2, an advanced long document IR approach that integrates block pre-ranking
with the performance of LLMs. KeyB2 efficiently identifies and processes the
most relevant blocks, reducing computational costs and improving ranking
effectiveness. Additionally, we introduce a new bi-encoder block matching
strategy for KeyB2. Comprehensive experiments on long-document datasets,
including TREC 2019 DL, Robust04, and MLDR-zh, show that KeyB2 outperforms
baselines like RankLLaMA and KeyB by reducing reranking time and GPU memory
usage while enhancing retrieval performance, achieving new SOTA results on TREC
2019 DL with higher NDCG@10 and MAP scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Retrieval-Augmented Generation for University Knowledge
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshia Hemmat, Kianoosh Vadaei, Mohammad Hassan Heydari, Afsaneh Fatemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative approach using Retrieval-Augmented
Generation (RAG) pipelines with Large Language Models (LLMs) to enhance
information retrieval and query response systems for university-related
question answering. By systematically extracting data from the university
official webpage and employing advanced prompt engineering techniques, we
generate accurate, contextually relevant responses to user queries.
  We developed a comprehensive university benchmark, UniversityQuestionBench
(UQB), to rigorously evaluate our system performance, based on common key
metrics in the filed of RAG pipelines, assessing accuracy and reliability
through various metrics and real-world scenarios. Our experimental results
demonstrate significant improvements in the precision and relevance of
generated responses, enhancing user experience and reducing the time required
to obtain relevant answers. In summary, this paper presents a novel application
of RAG pipelines and LLMs, supported by a meticulously prepared university
benchmark, offering valuable insights into advanced AI techniques for academic
data retrieval and setting the stage for future research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table, Submitted to 15th IKT conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpret the Internal States of Recommendation Model with Sparse
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayin Wang, Xiaoyu Zhang, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable recommendation systems are important to enhance transparency,
accuracy, and fairness. Beyond result-level explanations, model-level
interpretations can provide valuable insights that allow developers to optimize
system designs and implement targeted improvements. However, most current
approaches depend on specialized model designs, which often lack generalization
capabilities. Given the various kinds of recommendation models, existing
methods have limited ability to effectively interpret them. To address this
issue, we propose RecSAE, an automatic, generalizable probing method for
interpreting the internal states of Recommendation models with Sparse
AutoEncoder. RecSAE serves as a plug-in module that does not affect original
models during interpretations, while also enabling predictable modifications to
their behaviors based on interpretation results. Firstly, we train an
autoencoder with sparsity constraints to reconstruct internal activations of
recommendation models, making the RecSAE latents more interpretable and
monosemantic than the original neuron activations. Secondly, we automated the
construction of concept dictionaries based on the relationship between latent
activations and input item sequences. Thirdly, RecSAE validates these
interpretations by predicting latent activations on new item sequences using
the concept dictionary and deriving interpretation confidence scores from
precision and recall. We demonstrate RecSAE's effectiveness on two datasets,
identifying hundreds of highly interpretable concepts from pure ID-based
models. Latent ablation studies further confirm that manipulating latent
concepts produces corresponding changes in model output behavior, underscoring
RecSAE's utility for both understanding and targeted tuning recommendation
models. Code and data are publicly available at
https://github.com/Alice1998/RecSAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Snippet-based Conversational Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Sun, Naoki Otani, Hannah Kim, Dan Zhang, Nikita Bhutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Recommender Systems (CRS) engage users in interactive
dialogues to gather preferences and provide personalized recommendations.
Traditionally, CRS rely on pre-defined attributes or expensive, domain-specific
annotated datasets to guide conversations, which limits flexibility and
adaptability across domains. In this work, we introduce SnipRec, a novel CRS
that enhances dialogues and recommendations by extracting diverse expressions
and preferences from user-generated content (UGC) like customer reviews. Using
large language models, SnipRec maps user responses and UGC to concise snippets,
which are used to generate clarification questions and retrieve relevant items.
Our approach eliminates the need for domain-specific training, making it
adaptable to new domains and effective without prior knowledge of user
preferences. Extensive experiments on the Yelp dataset demonstrate the
effectiveness of snippet-based representations against document and
sentence-based representations. Additionally, SnipRec is able to improve
Hits@10 by 0.25 over the course of five conversational turns, underscoring the
efficiency of SnipRec in capturing user preferences through multi-turn
conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lambda: Learning Matchable Prior For Entity Alignment with Unlabeled
  Dangling Cases <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yin, Liyao Xiang, Dong Ding, Yuheng He, Yihan Wu, Xinbing Wang, Chenghu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the entity alignment (EA) problem with unlabeled dangling
cases, meaning that partial entities have no counterparts in the other
knowledge graph (KG), and this type of entity remains unlabeled. To address
this challenge, we propose the framework \textit{Lambda} for dangling detection
and then entity alignment. Lambda features a GNN-based encoder called KEESA
with spectral contrastive learning for EA and a positive-unlabeled learning
algorithm for dangling detection called iPULE. iPULE offers theoretical
guarantees of unbiasedness, uniform deviation bounds, and convergence.
Experimental results demonstrate that each component contributes to overall
performances that are superior to baselines, even when baselines additionally
exploit 30\% of dangling entities labeled for training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024 as a poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Learnable Clustering for Intent Learning in Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05975v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05975v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Xinwang Liu, Shengju Yu, Kejun Zhang, Wenliang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent learning, which aims to learn users' intents for user understanding
and item recommendation, has become a hot research spot in recent years.
However, existing methods suffer from complex and cumbersome alternating
optimization, limiting performance and scalability. To this end, we propose a
novel intent learning method termed \underline{ELCRec}, by unifying behavior
representation learning into an \underline{E}nd-to-end \underline{L}earnable
\underline{C}lustering framework, for effective and efficient
\underline{Rec}ommendation. Concretely, we encode user behavior sequences and
initialize the cluster centers (latent intents) as learnable neurons. Then, we
design a novel learnable clustering module to separate different cluster
centers, thus decoupling users' complex intents. Meanwhile, it guides the
network to learn intents from behaviors by forcing behavior embeddings close to
cluster centers. This allows simultaneous optimization of recommendation and
clustering via mini-batch data. Moreover, we propose intent-assisted
contrastive learning by using cluster centers as self-supervision signals,
further enhancing mutual promotion. Both experimental results and theoretical
analyses demonstrate the superiority of ELCRec from six perspectives. Compared
to the runner-up, ELCRec improves NDCG@5 by 8.9\% and reduces computational
costs by 22.5\% on the Beauty dataset. Furthermore, due to the scalability and
universal applicability, we deploy this method on the industrial recommendation
system with 130 million page views and achieve promising results. The codes are
available on GitHub (https://github.com/yueliu1999/ELCRec). A collection
(papers, codes, datasets) of deep group recommendation/intent learning methods
is available on GitHub
(https://github.com/yueliu1999/Awesome-Deep-Group-Recommendation).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-08T00:00:00Z">2024-11-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Document Financial Question Answering using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shalin Shah, Srikanth Ryali, Ramasubbu Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two new methods for multi-document financial question answering.
First, a method that uses semantic tagging, and then, queries the index to get
the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that
uses semantic tagging, and, retrieves knowledge graph triples from a graph
database, as context. KG_RAG uses knowledge graphs constructed using a small
model that is fine-tuned using knowledge distillation using a large teacher
model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,
NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of
questions in the data consists of 111 complex questions including many esoteric
questions that are difficult to answer and the answers are not completely
obvious. As evaluation metrics, we use overall scores as well as segmented
scores for measurement including the faithfulness, relevance, correctness,
similarity, an LLM based overall score and the rouge scores as well as a
similarity of embeddings. We find that both methods outperform plain RAG
significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The effect of different feature selection methods on models created with
  XGBoost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Neyra, Vishal B. Siramshetty, Huthaifa I. Ashqar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the effect that different feature selection methods have
on models created with XGBoost, a popular machine learning algorithm with
superb regularization methods. It shows that three different ways for reducing
the dimensionality of features produces no statistically significant change in
the prediction accuracy of the model. This suggests that the traditional idea
of removing the noisy training data to make sure models do not overfit may not
apply to XGBoost. But it may still be viable in order to reduce computational
complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucination with ZeroG: An Advanced Knowledge Management
  Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anantha Sharma, Sheeba Elizabeth John, Fatemeh Rezapoor Nikroo, Krupali Bhatt, Mrunal Zambre, Aditi Wikhe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of digital documents presents significant challenges in efficient
management and knowledge extraction. Traditional methods often struggle with
complex documents, leading to issues such as hallucinations and high latency in
responses from Large Language Models (LLMs). ZeroG, an innovative approach,
significantly mitigates these challenges by leveraging knowledge distillation
and prompt tuning to enhance model performance.
  ZeroG utilizes a smaller model that replicates the behavior of a larger
teacher model, ensuring contextually relevant and grounded responses, by
employing a black-box distillation approach, it creates a distilled dataset
without relying on intermediate features, optimizing computational efficiency.
This method significantly enhances accuracy and reduces response times,
providing a balanced solution for modern document management.
  Incorporating advanced techniques for document ingestion and metadata
utilization, ZeroG improves the accuracy of question-and-answer systems. The
integration of graph databases and robust metadata management further
streamlines information retrieval, allowing for precise and context-aware
responses. By transforming how organizations interact with complex data, ZeroG
enhances productivity and user experience, offering a scalable solution for the
growing demands of digital document management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">BERT</span>rend: Neural Topic Modeling for Emerging Trends Detection <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allaa Boutaleb, Jerome Picault, Guillaume Grosjean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and tracking emerging trends and weak signals in large, evolving
text corpora is vital for applications such as monitoring scientific
literature, managing brand reputation, surveilling critical infrastructure and
more generally to any kind of text-based event detection. Existing solutions
often fail to capture the nuanced context or dynamically track evolving
patterns over time. BERTrend, a novel method, addresses these limitations using
neural topic modeling in an online setting. It introduces a new metric to
quantify topic popularity over time by considering both the number of documents
and update frequency. This metric classifies topics as noise, weak, or strong
signals, flagging emerging, rapidly growing topics for further investigation.
Experimentation on two large real-world datasets demonstrates BERTrend's
ability to accurately detect and track meaningful weak signals while filtering
out noise, offering a comprehensive solution for monitoring emerging trends in
large-scale, evolving text corpora. The method can also be used for
retrospective analysis of past events. In addition, the use of Large Language
Models together with BERTrend offers efficient means for the interpretability
of trends of events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures, FuturED 2024: Workshop on Future of Event
  Detection (CoLocated with EMNLP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing High-Level Song Descriptors towards Natural Language-Based
  Music Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena V. Epure, Gabriel Meseguer Brocal, Darius Afchar, Romain Hennequin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems relying on Language Models (LMs) have gained popularity
in assisting users to navigate large catalogs. LMs often exploit item
high-level descriptors, i.e. categories or consumption contexts, from training
data or user preferences. This has been proven effective in domains like movies
or products. However, in the music domain, understanding how effectively LMs
utilize song descriptors for natural language-based music recommendation is
relatively limited. In this paper, we assess LMs effectiveness in recommending
songs based on user natural language descriptions and items with descriptors
like genres, moods, and listening contexts. We formulate the recommendation
task as a dense retrieval problem and assess LMs as they become increasingly
familiar with data pertinent to the task and domain. Our findings reveal
improved performance as LMs are fine-tuned for general language similarity,
information retrieval, and mapping longer descriptions to shorter, high-level
descriptors in music.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why These Documents? Explainable Generative Retrieval with Hierarchical
  Category Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangam Lee, Ryang Heo, SeongKu Kang, Susik Yoon, Jinyoung Yeo, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval has recently emerged as a new alternative of traditional
information retrieval approaches. However, existing generative retrieval
methods directly decode docid when a query is given, making it impossible to
provide users with explanations as an answer for "Why this document is
retrieved?". To address this limitation, we propose Hierarchical Category
Path-Enhanced Generative Retrieval(HyPE), which enhances explainability by
generating hierarchical category paths step-by-step before decoding docid. HyPE
leverages hierarchical category paths as explanation, progressing from broad to
specific semantic categories. This approach enables diverse explanations for
the same document depending on the query by using shared category paths between
the query and the document, and provides reasonable explanation by reflecting
the document's semantic structure through a coarse-to-fine manner. HyPE
constructs category paths with external high-quality semantic hierarchy,
leverages LLM to select appropriate candidate paths for each document, and
optimizes the generative retrieval model with path-augmented dataset. During
inference, HyPE utilizes path-aware reranking strategy to aggregate diverse
topic information, allowing the most relevant documents to be prioritized in
the final ranked list of docids. Our extensive experiments demonstrate that
HyPE not only offers a high level of explainability but also improves the
retrieval performance in the document retrieval task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying and Decomposing Compound Ingredients in Meal Plans Using
  Large Language Models <span class="chip">KR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Kopitar, Leon Bedrac, Larissa J Strath, Jiang Bian, Gregor Stiglic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the effectiveness of Large Language Models in meal
planning, focusing on their ability to identify and decompose compound
ingredients. We evaluated three models-GPT-4o, Llama-3 (70b), and Mixtral
(8x7b)-to assess their proficiency in recognizing and breaking down complex
ingredient combinations. Preliminary results indicate that while Llama-3 (70b)
and GPT-4o excels in accurate decomposition, all models encounter difficulties
with identifying essential elements like seasonings and oils. Despite strong
overall performance, variations in accuracy and completeness were observed
across models. These findings underscore LLMs' potential to enhance
personalized nutrition but highlight the need for further refinement in
ingredient decomposition. Future research should address these limitations to
improve nutritional recommendations and health outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge
  Delivery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dincy R. Arikkat, Abhinav M., Navya Binu, Parvathi M., Navya Biju, K. S. Arunima, Vinod P., Rafidha Rehiman K. A., Mauro Conti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of cyber security, intelligent chatbots are
gaining prominence. Artificial Intelligence, Machine Learning, and Natural
Language Processing empower these chatbots to handle user inquiries and deliver
threat intelligence. This helps cyber security knowledge readily available to
both professionals and the public. Traditional rule-based chatbots often lack
flexibility and struggle to adapt to user interactions. In contrast, Large
Language Model-based chatbots offer contextually relevant information across
multiple domains and adapt to evolving conversational contexts. In this work,
we develop IntellBot, an advanced cyber security Chatbot built on top of
cutting-edge technologies like Large Language Models and Langchain alongside a
Retrieval-Augmented Generation model to deliver superior capabilities. This
chatbot gathers information from diverse data sources to create a comprehensive
knowledge base covering known vulnerabilities, recent cyber attacks, and
emerging threats. It delivers tailored responses, serving as a primary hub for
cyber security insights. By providing instant access to relevant information
and resources, this IntellBot enhances threat intelligence, incident response,
and overall security posture, saving time and empowering users with knowledge
of cyber security best practices. Moreover, we analyzed the performance of our
copilot using a two-stage evaluation strategy. We achieved BERT score above 0.8
by indirect approach and a cosine similarity score ranging from 0.8 to 1, which
affirms the accuracy of our copilot. Additionally, we utilized RAGAS to
evaluate the RAG model, and all evaluation metrics consistently produced scores
above 0.77, highlighting the efficacy of our system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashara Akhtar, Michael Schlichtkrull, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current automated fact-checking (AFC) approaches commonly evaluate evidence
either implicitly via the predicted verdicts or by comparing retrieved evidence
with a predefined closed knowledge source, such as Wikipedia. However, these
methods suffer from limitations, resulting from their reliance on evaluation
metrics developed for different purposes and constraints imposed by closed
knowledge sources. Recent advances in natural language generation (NLG)
evaluation offer new possibilities for evidence assessment. In this work, we
introduce Ev2R, an evaluation framework for AFC that comprises three types of
approaches for evidence evaluation: reference-based, proxy-reference, and
reference-less. We evaluate their effectiveness through agreement with human
ratings and adversarial tests, and demonstrate that prompt-based scorers,
particularly those leveraging LLMs and reference evidence, outperform
traditional evaluation approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Multi-Domain Task-Oriented Dialogue System with Offline
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dharmendra Prajapat, Durga Toshniwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) system is designed to accomplish user-defined
tasks through dialogues. The TOD system has progressed towards end-to-end
modeling by leveraging pre-trained large language models. Fine-tuning the
pre-trained language models using only supervised learning leads to the
exposure bias and token loss problem and it deviates the models from completing
the user's task. To address these issues, we propose a TOD system that
leverages a unified pre-trained language model, GPT2, as a base model. It is
optimized using supervised learning and reinforcement learning (RL). The issues
in the TOD system are mitigated using a non-differentiable reward function. The
reward is calculated using the weighted sum of the success rate and BLEU
evaluation metrics. The success rate and BLEU metrics in reward calculation
guide the language model for user task completion while ensuring a coherent and
fluent response. Our model is acquired by fine-tuning a pre-trained model on
the dialogue-session level which comprises user utterance, belief state, system
act, and system response. Experimental results on MultiWOZ2.1 demonstrate that
our model increases the inform rate by 1.60% and the success rate by 3.17%
compared to the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logic Query of Thoughts: Guiding Large Language Models to Answer Complex
  Logic Queries with Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04264v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04264v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Eunice Chan, Yangqiu Song, Jingrui He, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the superb performance in many tasks, large language models (LLMs)
bear the risk of generating hallucination or even wrong answers when confronted
with tasks that demand the accuracy of knowledge. The issue becomes even more
noticeable when addressing logic queries that require multiple logic reasoning
steps. On the other hand, knowledge graph (KG) based question answering methods
are capable of accurately identifying the correct answers with the help of
knowledge graph, yet its accuracy could quickly deteriorate when the knowledge
graph itself is sparse and incomplete. It remains a critical challenge on how
to integrate knowledge graph reasoning with LLMs in a mutually beneficial way
so as to mitigate both the hallucination problem of LLMs as well as the
incompleteness issue of knowledge graphs. In this paper, we propose
'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs
with knowledge graph based logic query reasoning. LGOT seamlessly combines
knowledge graph reasoning and LLMs, effectively breaking down complex logic
queries into easy to answer subquestions. Through the utilization of both
knowledge graph reasoning and LLMs, it successfully derives answers for each
subquestion. By aggregating these results and selecting the highest quality
candidate answers for each step, LGOT achieves accurate results to complex
questions. Our experimental findings demonstrate substantial performance
enhancements, with up to 20% improvement over ChatGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cluster-based Graph Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Liu, Shuai Zhao, Zhiyong Cheng, Liqiang Nie, Mohan Kankanhalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolution Networks (GCNs) have significantly succeeded in learning
user and item representations for recommendation systems. The core of their
efficacy is the ability to explicitly exploit the collaborative signals from
both the first- and high-order neighboring nodes. However, most existing
GCN-based methods overlook the multiple interests of users while performing
high-order graph convolution. Thus, the noisy information from unreliable
neighbor nodes (e.g., users with dissimilar interests) negatively impacts the
representation learning of the target node. Additionally, conducting graph
convolution operations without differentiating high-order neighbors suffers the
over-smoothing issue when stacking more layers, resulting in performance
degradation. In this paper, we aim to capture more valuable information from
high-order neighboring nodes while avoiding noise for better representation
learning of the target node. To achieve this goal, we propose a novel GCN-based
recommendation model, termed Cluster-based Graph Collaborative Filtering
(ClusterGCF). This model performs high-order graph convolution on
cluster-specific graphs, which are constructed by capturing the multiple
interests of users and identifying the common interests among them.
Specifically, we design an unsupervised and optimizable soft node clustering
approach to classify user and item nodes into multiple clusters. Based on the
soft node clustering results and the topology of the user-item interaction
graph, we assign the nodes with probabilities for different clusters to
construct the cluster-specific graphs. To evaluate the effectiveness of
ClusterGCF, we conducted extensive experiments on four publicly available
datasets. Experimental results demonstrate that our model can significantly
improve recommendation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Generative Agents in Recommendation <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10108v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10108v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are the cornerstone of today's information dissemination,
yet a disconnect between offline metrics and online performance greatly hinders
their development. Addressing this challenge, we envision a recommendation
simulator, capitalizing on recent breakthroughs in human-level intelligence
exhibited by Large Language Models (LLMs). We propose Agent4Rec, a user
simulator in recommendation, leveraging LLM-empowered generative agents
equipped with user profile, memory, and actions modules specifically tailored
for the recommender system. In particular, these agents' profile modules are
initialized using real-world datasets (e.g. MovieLens, Steam, Amazon-Book),
capturing users' unique tastes and social traits; memory modules log both
factual and emotional memories and are integrated with an emotion-driven
reflection mechanism; action modules support a wide variety of behaviors,
spanning both taste-driven and emotion-driven actions. Each agent interacts
with personalized recommender models in a page-by-page manner, relying on a
pre-implemented collaborative filtering-based recommendation algorithm. We
delve into both the capabilities and limitations of Agent4Rec, aiming to
explore an essential research question: ``To what extent can LLM-empowered
generative agents faithfully simulate the behavior of real, autonomous humans
in recommender systems?'' Extensive and multi-faceted evaluations of Agent4Rec
highlight both the alignment and deviation between agents and user-personalized
preferences. Beyond mere performance comparison, we explore insightful
experiments, such as emulating the filter bubble effect and discovering the
underlying causal relationships in recommendation tasks. Our codes are
available at https://github.com/LehengTHU/Agent4Rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2024 perspective paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Noise Resilient for QoS Prediction with Probabilistic Deep
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02580v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02580v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Wang, Xiaohong Zhang, Ze Shi Li, Sheng Huang, Meng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate Quality of Service (QoS) prediction is essential for enhancing user
satisfaction in web recommendation systems, yet existing prediction models
often overlook feature noise, focusing predominantly on label noise. In this
paper, we present the Probabilistic Deep Supervision Network (PDS-Net), a
robust framework designed to effectively identify and mitigate feature noise,
thereby improving QoS prediction accuracy. PDS-Net operates with a dual-branch
architecture: the main branch utilizes a decoder network to learn a
Gaussian-based prior distribution from known features, while the second branch
derives a posterior distribution based on true labels. A key innovation of
PDS-Net is its condition-based noise recognition loss function, which enables
precise identification of noisy features in objects (users or services). Once
noisy features are identified, PDS-Net refines the feature's prior
distribution, aligning it with the posterior distribution, and propagates this
adjusted distribution to intermediate layers, effectively reducing noise
interference. Extensive experiments conducted on two real-world QoS datasets
demonstrate that PDS-Net consistently outperforms existing models, achieving an
average improvement of 8.91% in MAE on Dataset D1 and 8.32% on Dataset D2
compared to the ate-of-the-art. These results highlight PDS-Net's ability to
accurately capture complex user-service relationships and handle feature noise,
underscoring its robustness and versatility across diverse QoS prediction
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limpeh ga li gong: Challenges in Singlish Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luo Qi Chan, Lynnette Hui Xian Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singlish, or Colloquial Singapore English, is a language formed from oral and
social communication within multicultural Singapore. In this work, we work on a
fundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS)
tagging of Singlish sentences. For our analysis, we build a parallel Singlish
dataset containing direct English translations and POS tags, with translation
and POS annotation done by native Singlish speakers. Our experiments show that
automatic transition- and transformer- based taggers perform with only $\sim
80\%$ accuracy when evaluated against human-annotated POS labels, suggesting
that there is indeed room for improvement on computation analysis of the
language. We provide an exposition of challenges in Singlish annotation: its
inconsistencies in form and semantics, the highly context-dependent particles
of the language, its structural unique expressions, and the variation of the
language on different mediums. Our task definition, resultant labels and
results reflects the challenges in analysing colloquial languages formulated
from a variety of dialects, and paves the way for future studies beyond POS
tagging.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Noise in AudioVisual Integration: Evidence from
  Artificial Neural Networks that Exhibit the McGurk Effect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Grasse, Matthew S. Tata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are able to fuse information from both auditory and visual modalities
to help with understanding speech. This is frequently demonstrated through an
phenomenon known as the McGurk Effect, during which a listener is presented
with incongruent auditory and visual speech that fuse together into the percept
of an illusory intermediate phoneme. Building on a recent framework that
proposes how to address developmental 'why' questions using artificial neural
networks, we evaluated a set of recent artificial neural networks trained on
audiovisual speech by testing them with audiovisually incongruent words
designed to elicit the McGurk effect. We compared networks trained on clean
speech to those trained on noisy speech, and discovered that training with
noisy speech led to an increase in both visual responses and McGurk responses
across all models. Furthermore, we observed that systematically increasing the
level of auditory noise during ANN training also increased the amount of
audiovisual integration up to a point, but at extreme noise levels, this
integration failed to develop. These results suggest that excessive noise
exposure during critical periods of audiovisual learning may negatively
influence the development of audiovisual speech integration. This work also
demonstrates that the McGurk effect reliably emerges untrained from the
behaviour of both supervised and unsupervised networks. This supports the
notion that artificial neural networks might be useful models for certain
aspects of perception and cognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interdisciplinary Translations: Sensory Perception as a Universal
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xindi Kang, Xuanyang Huang, Mingdong Song, Varvara Guljajeva, JoAnn Kuchera-Morin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates sensory perception's pivotal role as a universal
communicative bridge across varied cultures and disciplines, and how it
manifests its value in the study of media art, human computer interaction and
artificial intelligence. By analyzing its function in non-verbal communication
through interactive systems, and drawing on the interpretive model in
translation studies where "sense" acts as a mediation between two languages,
this paper illustrates how interdisciplinary communication in media art and
human-computer interaction is afforded by the abstract language of human
sensory perception. Specific examples from traditional art, interactive media
art, HCI, communication, and translation studies demonstrate how sensory
feedback translates and conveys meaning across diverse modalities of expression
and how it fosters connections between humans, art, and technology. Pertaining
to this topic, this paper analyzes the impact of sensory feedback systems in
designing interactive experiences, and reveals the guiding role of sensory
perception in the design philosophy of AI systems. Overall, the study aims to
broaden the understanding of sensory perception's role in communication,
highlighting its significance in the evolution of interactive experiences and
its capacity to unify art, science, and the human experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the International Symposium of
  Electronic Arts 2024, and the proceedings version will be available at
  https://isea-archives.siggraph.org/publications/ with DOI to be added once
  published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rate-aware Compression for NeRF-based Volumetric Video <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Zhang, Guo Lu, Huanxiong Liang, Zhengxue Cheng, Anni Tang, Li Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The neural radiance fields (NeRF) have advanced the development of 3D
volumetric video technology, but the large data volumes they involve pose
significant challenges for storage and transmission. To address these problems,
the existing solutions typically compress these NeRF representations after the
training stage, leading to a separation between representation training and
compression. In this paper, we try to directly learn a compact NeRF
representation for volumetric video in the training stage based on the proposed
rate-aware compression framework. Specifically, for volumetric video, we use a
simple yet effective modeling strategy to reduce temporal redundancy for the
NeRF representation. Then, during the training phase, an implicit entropy model
is utilized to estimate the bitrate of the NeRF representation. This entropy
model is then encoded into the bitstream to assist in the decoding of the NeRF
representation. This approach enables precise bitrate estimation, thereby
leading to a compact NeRF representation. Furthermore, we propose an adaptive
quantization strategy and learn the optimal quantization step for the NeRF
representations. Finally, the NeRF representation can be optimized by using the
rate-distortion trade-off. Our proposed compression framework can be used for
different representations and experimental results demonstrate that our
approach significantly reduces the storage size with marginal distortion and
achieves state-of-the-art rate-distortion performance for volumetric video on
the HumanRF and ReRF datasets. Compared to the previous state-of-the-art method
TeTriRF, we achieved an approximately -80% BD-rate on the HumanRF dataset and
-60% BD-rate on the ReRF dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content-Adaptive Rate-Quality Curve Prediction Model in Media Processing
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibo Yin, Zhiyu Zhang, Peirong Ning, Qiubo Chen, Jing Chen, Quan Zhou, Li Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In streaming media services, video transcoding is a common practice to
alleviate bandwidth demands. Unfortunately, traditional methods employing a
uniform rate factor (RF) across all videos often result in significant
inefficiencies. Content-adaptive encoding (CAE) techniques address this by
dynamically adjusting encoding parameters based on video content
characteristics. However, existing CAE methods are often tightly coupled with
specific encoding strategies, leading to inflexibility. In this paper, we
propose a model that predicts both RF-quality and RF-bitrate curves, which can
be utilized to derive a comprehensive bitrate-quality curve. This approach
facilitates flexible adjustments to the encoding strategy without necessitating
model retraining. The model leverages codec features, content features, and
anchor features to predict the bitrate-quality curve accurately. Additionally,
we introduce an anchor suspension method to enhance prediction accuracy.
Experiments confirm that the actual quality metric (VMAF) of the compressed
video stays within 1 of the target, achieving an accuracy of 99.14%. By
incorporating our quality improvement strategy with the rate-quality curve
prediction model, we conducted online A/B tests, obtaining both +0.107%
improvements in video views and video completions and +0.064% app duration
time. Our model has been deployed on the Xiaohongshu App.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE VCIP 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ pTSE-T: Presentation Target Speaker Extraction using Unaligned Text Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Jiang, Xinyuan Qian, Jiahe Lei, Zexu Pan, Wei Xue, Xu-cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TSE(Target Speaker Extraction) aims to extract the clean speech of the target
speaker in an audio mixture, thus eliminating irrelevant background noise and
speech. While prior work has explored various auxiliary cues including
pre-recorded speech, visual information (e.g., lip motions and gestures), and
spatial information, the acquisition and selection of such strong cues are
infeasible in many practical scenarios. Unlike all existing work, in this
paper, we condition the TSE algorithm on semantic cues extracted from limited
and unaligned text content, such as condensed points from a presentation slide.
This method is particularly useful in scenarios like meetings, poster sessions,
or lecture presentations, where acquiring other cues in real-time is
challenging. To this end, we design two different networks. Specifically, our
proposed TPE fuses audio features with content-based semantic cues to
facilitate time-frequency mask generation to filter out extraneous noise, while
another proposal, namely TSR, employs the contrastive learning technique to
associate blindly separated speech signals with semantic cues. The experimental
results show the efficacy in accurately identifying the target speaker by
utilizing semantic cues derived from limited and unaligned text, resulting in
SI-SDRi of 12.16 dB, SDRi of 12.66 dB, PESQi of 0.830 and STOIi of 0.150,
respectively. Dataset and source code will be publicly available. Project demo
page: https://slideTSE.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-07T00:00:00Z">2024-11-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orbit: A Framework for Designing and Evaluating Multi-objective Rankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Yang, Tesi Xiao, Michael Shavlovsky, Christian Kästner, Tongshuang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning in production needs to balance multiple objectives: This is
particularly evident in ranking or recommendation models, where conflicting
objectives such as user engagement, satisfaction, diversity, and novelty must
be considered at the same time. However, designing multi-objective rankers is
inherently a dynamic wicked problem -- there is no single optimal solution, and
the needs evolve over time. Effective design requires collaboration between
cross-functional teams and careful analysis of a wide range of information. In
this work, we introduce Orbit, a conceptual framework for Objective-centric
Ranker Building and Iteration. The framework places objectives at the center of
the design process, to serve as boundary objects for communication and guide
practitioners for design and evaluation. We implement Orbit as an interactive
system, which enables stakeholders to interact with objective spaces directly
and supports real-time exploration and evaluation of design trade-offs. We
evaluate Orbit through a user study involving twelve industry practitioners,
showing that it supports efficient design space exploration, leads to more
informed decision-making, and enhances awareness of the inherent trade-offs of
multiple objectives. Orbit (1) opens up new opportunities of an
objective-centric design process for any multi-objective ML models, as well as
(2) sheds light on future designs that push practitioners to go beyond a narrow
metric-centric or example-centric mindset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightning IR: Straightforward Fine-tuning and Inference of
  <span class="highlight-title">Transformer</span>-based Language Models for Information Retrieval <span class="chip">WSDM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferdinand Schlatt, Maik Fröbe, Matthias Hagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A wide range of transformer-based language models have been proposed for
information retrieval tasks. However, fine-tuning and inference of these models
is often complex and requires substantial engineering effort. This paper
introduces Lightning IR, a PyTorch Lightning-based framework for fine-tuning
and inference of transformer-based language models for information retrieval.
Lightning IR provides a modular and extensible architecture that supports all
stages of an information retrieval pipeline: from fine-tuning and indexing to
searching and re-ranking. It is designed to be straightforward to use,
scalable, and reproducible. Lightning IR is available as open-source:
https://github.com/webis-de/lightning-ir.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a demo at WSDM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Calibrated Listwise Reranking with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Ren, Yuhao Wang, Kun Zhou, Wayne Xin Zhao, Wenjie Wang, Jing Liu, Ji-Rong Wen, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), with advanced linguistic capabilities, have
been employed in reranking tasks through a sequence-to-sequence approach. In
this paradigm, multiple passages are reranked in a listwise manner and a
textual reranked permutation is generated. However, due to the limited context
window of LLMs, this reranking paradigm requires a sliding window strategy to
iteratively handle larger candidate sets. This not only increases computational
costs but also restricts the LLM from fully capturing all the comparison
information for all candidates. To address these challenges, we propose a novel
self-calibrated listwise reranking method, which aims to leverage LLMs to
produce global relevance scores for ranking. To achieve it, we first propose
the relevance-aware listwise reranking framework, which incorporates explicit
list-view relevance scores to improve reranking efficiency and enable global
comparison across the entire candidate set. Second, to ensure the comparability
of the computed scores, we propose self-calibrated training that uses
point-view relevance assessments generated internally by the LLM itself to
calibrate the list-view relevance assessments. Extensive experiments and
comprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks
demonstrate the effectiveness and efficiency of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best Practices for Distilling Large Language Models into <span class="highlight-title">BERT</span> for Web
  Search Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dezhi Ye, Junwei Hu, Jiabin Fan, Bowen Tian, Jie Liu, Haijin Liang, Jin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have highlighted the significant potential of Large Language
Models (LLMs) as zero-shot relevance rankers. These methods predominantly
utilize prompt learning to assess the relevance between queries and documents
by generating a ranked list of potential documents. Despite their promise, the
substantial costs associated with LLMs pose a significant challenge for their
direct implementation in commercial search systems. To overcome this barrier
and fully exploit the capabilities of LLMs for text ranking, we explore
techniques to transfer the ranking expertise of LLMs to a more compact model
similar to BERT, using a ranking loss to enable the deployment of less
resource-intensive models. Specifically, we enhance the training of LLMs
through Continued Pre-Training, taking the query as input and the clicked title
and summary as output. We then proceed with supervised fine-tuning of the LLM
using a rank loss, assigning the final token as a representative of the entire
sentence. Given the inherent characteristics of autoregressive language models,
only the final token </s> can encapsulate all preceding tokens. Additionally,
we introduce a hybrid point-wise and margin MSE loss to transfer the ranking
knowledge from LLMs to smaller models like BERT. This method creates a viable
solution for environments with strict resource constraints. Both offline and
online evaluations have confirmed the efficacy of our approach, and our model
has been successfully integrated into a commercial web search engine as of
February 2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging LLMs to Enable Natural Language Search on Go-to-market
  Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Yao, Saurav Acharya, Priyaranjan Parida, Srinivas Attipalli, Ali Dasdan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enterprise searches require users to have complex knowledge of queries,
configurations, and metadata, rendering it difficult for them to access
information as needed. Most go-to-market (GTM) platforms utilize advanced
search, an interface that enables users to filter queries by various fields
using categories or keywords, which, historically, however, has proven to be
exceedingly cumbersome, as users are faced with seemingly hundreds of options,
fields, and buttons. Consequently, querying with natural language has long been
ideal, a notion further empowered by Large Language Models (LLMs).
  In this paper, we implement and evaluate a solution for the Zoominfo product
for sellers, which prompts the LLM with natural language, producing search
fields through entity extraction that are then converted into a search query.
The intermediary search fields offer numerous advantages for each query,
including the elimination of syntax errors, simpler ground truths, and an
intuitive format for the LLM to interpret.
  We paired this pipeline with many advanced prompt engineering strategies,
featuring an intricate system message, few-shot prompting, chain-of-thought
(CoT) reasoning, and execution refinement. Furthermore, we manually created the
ground truth for 500+ natural language queries, enabling the supervised
fine-tuning of Llama-3-8B-Instruct and the introduction of sophisticated
numerical metrics.
  Comprehensive experiments with closed, open source, and fine-tuned LLM models
were conducted through exact, Jaccard, cosine, and semantic similarity on
individual search entities to demonstrate the efficacy of our approach.
Overall, the most accurate closed model had an average accuracy of 97% per
query, with only one field performing under 90%, with comparable results
observed from the fine-tuned models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Competitive Search Relevance For Inference-Free Learned Sparse
  Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Geng, Dongyu Ru, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned sparse retrieval, which can efficiently perform retrieval through
mature inverted-index engines, has garnered growing attention in recent years.
Particularly, the inference-free sparse retrievers are attractive as they
eliminate online model inference in the retrieval phase thereby avoids huge
computational cost, offering reasonable throughput and latency. However, even
the state-of-the-art (SOTA) inference-free sparse models lag far behind in
terms of search relevance when compared to both sparse and dense siamese
models. Towards competitive search relevance for inference-free sparse
retrievers, we argue that they deserve dedicated training methods other than
using same ones with siamese encoders. In this paper, we propose two different
approaches for performance improvement. First, we introduce the IDF-aware FLOPS
loss, which introduces Inverted Document Frequency (IDF) to the sparsification
of representations. We find that it mitigates the negative impact of the FLOPS
regularization on search relevance, allowing the model to achieve a better
balance between accuracy and efficiency. Moreover, we propose a heterogeneous
ensemble knowledge distillation framework that combines siamese dense and
sparse retrievers to generate supervisory signals during the pre-training
phase. The ensemble framework of dense and sparse retriever capitalizes on
their strengths respectively, providing a strong upper bound for knowledge
distillation. To concur the diverse feedback from heterogeneous supervisors, we
normalize and then aggregate the outputs of the teacher models to eliminate
score scale differences. On the BEIR benchmark, our model outperforms existing
SOTA inference-free sparse model by \textbf{3.3 NDCG@10 score}. It exhibits
search relevance comparable to siamese sparse retrievers and client-side
latency only \textbf{1.1x that of BM25}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Concatenator: A Bayesian Approach To Real Time Concatenative
  Musaicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Tralie, Ben Cantil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ``The Concatenator,'' a real time system for audio-guided
concatenative synthesis. Similarly to Driedger et al.'s ``musaicing'' (or
``audio mosaicing'') technique, we concatenate a set number of windows within a
corpus of audio to re-create the harmonic and percussive aspects of a target
audio stream. Unlike Driedger's NMF-based technique, however, we instead use an
explicitly Bayesian point of view, where corpus window indices are hidden
states and the target audio stream is an observation. We use a particle filter
to infer the best hidden corpus states in real-time. Our transition model
includes a tunable parameter to control the time-continuity of corpus grains,
and our observation model allows users to prioritize how quickly windows change
to match the target. Because the computational complexity of the system is
independent of the corpus size, our system scales to corpora that are hours
long, which is an important feature in the age of vast audio data collections.
Within The Concatenator module itself, composers can vary grain length, fit to
target, and pitch shift in real time while reacting to the sounds they hear,
enabling them to rapidly iterate ideas. To conclude our work, we evaluate our
system with extensive quantitative tests of the effects of parameters, as well
as a qualitative evaluation with artistic insights. Based on the quality of the
results, we believe the real-time capability unlocks new avenues for musical
expression and control, suitable for live performance and modular synthesis
integration, which furthermore represents an essential breakthrough in
concatenative synthesis technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, Accepted for Publication in The International
  Society for Music Information Retrieval Proceedings, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RRADistill: Distilling LLMs' Passage Ranking Ability for Document
  Re-Ranking of Long-Tail Queries in a Search Engine <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nayoung Choi, Youngjune Lee, Gyu-Hwung Cho, Haeyu Jeong, Jungmin Kong, Saehun Kim, Keunchan Park, Jaeho Choi, Sarah Cho, Inchang Jeong, Gyohee Nam, Sunghoon Han, Wonil Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at understanding the semantic
relationships between queries and documents, even with lengthy and complex
long-tail queries. These queries are challenging for feedback-based rankings
due to sparse user engagement and limited feedback, making LLMs' ranking
ability highly valuable. However, the large size and slow inference of LLMs
necessitate the development of smaller, more efficient models (sLLMs).
Recently, integrating ranking label generation into distillation techniques has
become crucial, but existing methods underutilize LLMs' capabilities and are
cumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose
an efficient label generation pipeline and novel sLLM training methods for both
encoder and decoder models. We introduce an encoder-based method using a Term
Control Layer to capture term matching signals and a decoder-based model with a
ranking layer for enhanced understanding. A/B testing on a Korean-based search
platform, validates the effectiveness of our approach in improving re-ranking
for long-tail queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Industry Track. First two authors contributed
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Softmax Direct Preference Optimization for Recommendation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09215v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09215v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems aim to predict personalized rankings based on user
preference data. With the rise of Language Models (LMs), LM-based recommenders
have been widely explored due to their extensive world knowledge and powerful
reasoning abilities. Most of the LM-based recommenders convert historical
interactions into language prompts, pairing with a positive item as the target
response and fine-tuning LM with a language modeling loss. However, the current
objective fails to fully leverage preference data and is not optimized for
personalized ranking tasks, which hinders the performance of LM-based
recommenders. Inspired by the current advancement of Direct Preference
Optimization (DPO) in human preference alignment and the success of softmax
loss in recommendations, we propose Softmax-DPO (S-DPO) to instill ranking
information into the LM to help LM-based recommenders distinguish preferred
items from negatives, rather than solely focusing on positives. Specifically,
we incorporate multiple negatives in user preference data and devise an
alternative version of DPO loss tailored for LM-based recommenders, which is
extended from the traditional full-ranking Plackett-Luce (PL) model to partial
rankings and connected to softmax sampling strategies. Theoretically, we bridge
S-DPO with the softmax loss over negative sampling and find that it has an
inherent benefit of mining hard negatives, which assures its exceptional
capabilities in recommendation tasks. Empirically, extensive experiments
conducted on three real-world datasets demonstrate the superiority of S-DPO to
effectively model user preference and further boost recommendation performance
while providing better rewards for preferred items. Our codes are available at
https://github.com/chenyuxin1999/S-DPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking the Talk Does Not Entail Walking the Walk: On the Limits of
  Large Language Models in Lexical Entailment Recognition <span class="chip">EMNLP-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candida M. Greco, Lucio La Cava, Andrea Tagarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verbs form the backbone of language, providing the structure and meaning to
sentences. Yet, their intricate semantic nuances pose a longstanding challenge.
Understanding verb relations through the concept of lexical entailment is
crucial for comprehending sentence meanings and grasping verb dynamics. This
work investigates the capabilities of eight Large Language Models in
recognizing lexical entailment relations among verbs through differently
devised prompting strategies and zero-/few-shot settings over verb pairs from
two lexical databases, namely WordNet and HyperLex. Our findings unveil that
the models can tackle the lexical entailment recognition task with moderately
good performance, although at varying degree of effectiveness and under
different conditions. Also, utilizing few-shot prompting can enhance the
models' performance. However, perfectly solving the task arises as an unmet
challenge for all examined LLMs, which raises an emergence for further research
developments on this topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at The 2024 Conference on Empirical Methods
  in Natural Language Processing (EMNLP-2024) - Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartifyText: Automated Chart Generation from Data-Involved Texts via
  LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songheng Zhang, Lei Wang, Toby Jia-Jun Li, Qiaomu Shen, Yixin Cao, Yong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text documents with numerical values involved are widely used in various
applications such as scientific research, economy, public health and
journalism. However, it is difficult for readers to quickly interpret such
data-involved texts and gain deep insights. To fill this research gap, this
work aims to automatically generate charts to accurately convey the underlying
data and ideas to readers, which is essentially a challenging task. The
challenges originate from text ambiguities, intrinsic sparsity and uncertainty
of data in text documents, and subjective sentiment differences. Specifically,
we propose ChartifyText, a novel fully-automated approach that leverages Large
Language Models (LLMs) to convert complex data-involved texts to expressive
charts. It consists of two major modules: tabular data inference and expressive
chart generation. The tabular data inference module employs systematic prompt
engineering to guide the LLM (e.g., GPT-4) to infer table data, where data
ranges, uncertainties, missing data values and corresponding subjective
sentiments are explicitly considered. The expressive chart generation module
augments standard charts with intuitive visual encodings and concise texts to
accurately convey the underlying data and insights. We extensively evaluate the
effectiveness of ChartifyText on real-world data-involved text documents
through case studies, in-depth interviews with three visualization experts, and
a carefully-designed user study with 15 participants. The results demonstrate
the usefulness and effectiveness of ChartifyText in helping readers efficiently
and effectively make sense of data-involved texts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightRAG: Simple and Fast Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge sources, enabling more accurate and
contextually relevant responses tailored to user needs. However, existing RAG
systems have significant limitations, including reliance on flat data
representations and inadequate contextual awareness, which can lead to
fragmented answers that fail to capture complex inter-dependencies. To address
these challenges, we propose LightRAG, which incorporates graph structures into
text indexing and retrieval processes. This innovative framework employs a
dual-level retrieval system that enhances comprehensive information retrieval
from both low-level and high-level knowledge discovery. Additionally, the
integration of graph structures with vector representations facilitates
efficient retrieval of related entities and their relationships, significantly
improving response times while maintaining contextual relevance. This
capability is further enhanced by an incremental update algorithm that ensures
the timely integration of new data, allowing the system to remain effective and
responsive in rapidly changing data environments. Extensive experimental
validation demonstrates considerable improvements in retrieval accuracy and
efficiency compared to existing approaches. We have made our LightRAG
open-source and available at the link: https://github.com/HKUDS/LightRAG.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multi-purpose automatic editing system based on lecture semantics for
  remote education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panwen Hu, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote teaching has become popular recently due to its convenience and
safety, especially under extreme circumstances like a pandemic. However, online
students usually have a poor experience since the information acquired from the
views provided by the broadcast platforms is limited. One potential solution is
to show more camera views simultaneously, but it is technically challenging and
distracting for the viewers. Therefore, an automatic multi-camera
directing/editing system, which aims at selecting the most concerned view at
each time instance to guide the attention of online students, is in urgent
demand. However, existing systems mostly make simple assumptions and focus on
tracking the position of the speaker instead of the real lecture semantics, and
therefore have limited capacities to deliver optimal information flow. To this
end, this paper proposes an automatic multi-purpose editing system based on the
lecture semantics, which can both direct the multiple video streams for
real-time broadcasting and edit the optimal video offline for review purposes.
Our system directs the views by semantically analyzing the class events while
following the professional directing rules, mimicking a human director to
capture the regions of interest from the viewpoint of the onsite students. We
conduct both qualitative and quantitative analyses to verify the effectiveness
of the proposed system and its components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Sign Language Recognition System using Deep Learning with
  MediaPipe Holistic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharvani Srivastava, Sudhakar Singh,  Pooja, Shiv Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign languages are the language of hearing-impaired people who use visuals
like the hand, facial, and body movements for communication. There are
different signs and gestures representing alphabets, words, and phrases.
Nowadays approximately 300 sign languages are being practiced worldwide such as
American Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language
(ISL), and many more. Sign languages are dependent on the vocal language of a
place. Unlike vocal or spoken languages, there are no helping words in sign
language like is, am, are, was, were, will, be, etc. As only a limited
population is well-versed in sign language, this lack of familiarity of sign
language hinders hearing-impaired people from communicating freely and easily
with everyone. This issue can be addressed by a sign language recognition (SLR)
system which has the capability to translate the sign language into vocal
language. In this paper, a continuous SLR system is proposed using a deep
learning model employing Long Short-Term Memory (LSTM), trained and tested on
an ISL primary dataset. This dataset is created using MediaPipe Holistic
pipeline for tracking face, hand, and body movements and collecting landmarks.
The system recognizes the signs and gestures in real-time with 88.23% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, Wireless Pers Commun</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Concatenator: A Bayesian Approach To Real Time Concatenative
  Musaicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Tralie, Ben Cantil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ``The Concatenator,'' a real time system for audio-guided
concatenative synthesis. Similarly to Driedger et al.'s ``musaicing'' (or
``audio mosaicing'') technique, we concatenate a set number of windows within a
corpus of audio to re-create the harmonic and percussive aspects of a target
audio stream. Unlike Driedger's NMF-based technique, however, we instead use an
explicitly Bayesian point of view, where corpus window indices are hidden
states and the target audio stream is an observation. We use a particle filter
to infer the best hidden corpus states in real-time. Our transition model
includes a tunable parameter to control the time-continuity of corpus grains,
and our observation model allows users to prioritize how quickly windows change
to match the target. Because the computational complexity of the system is
independent of the corpus size, our system scales to corpora that are hours
long, which is an important feature in the age of vast audio data collections.
Within The Concatenator module itself, composers can vary grain length, fit to
target, and pitch shift in real time while reacting to the sounds they hear,
enabling them to rapidly iterate ideas. To conclude our work, we evaluate our
system with extensive quantitative tests of the effects of parameters, as well
as a qualitative evaluation with artistic insights. Based on the quality of the
results, we believe the real-time capability unlocks new avenues for musical
expression and control, suitable for live performance and modular synthesis
integration, which furthermore represents an essential breakthrough in
concatenative synthesis technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, Accepted for Publication in The International
  Society for Music Information Retrieval Proceedings, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIAST: A Multimodal Piano <span class="highlight-title">Dataset</span> with Audio, Symbolic and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayeon Bang, Eunjin Choi, Megan Finch, Seungheon Doh, Seolhee Lee, Gyeong-Hoon Lee, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While piano music has become a significant area of study in Music Information
Retrieval (MIR), there is a notable lack of datasets for piano solo music with
text labels. To address this gap, we present PIAST (PIano dataset with Audio,
Symbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy
of semantic tags, we collected 9,673 tracks from YouTube and added human
annotations for 2,023 tracks by music experts, resulting in two subsets:
PIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and
transcribed MIDI utilizing state-of-the-art piano transcription and beat
tracking models. Among many possible tasks with the multi-modal dataset, we
conduct music tagging and retrieval using both audio and MIDI data and report
baseline performances to demonstrate its potential as a valuable resource for
MIR research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 3rd Workshop on NLP for Music and
  Audio (NLP4MusA 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-06T00:00:00Z">2024-11-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ dsld: A Socially Relevant Tool for Teaching Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Abdullah, Arjun Ashok, Brandon Estrada, Norman Matloff, Aditya Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing power of data science can play a crucial role in addressing
social discrimination, necessitating nuanced understanding and effective
mitigation strategies of potential biases. Data Science Looks At Discrimination
(dsld) is an R and Python package designed to provide users with a
comprehensive toolkit of statistical and graphical methods for assessing
possible discrimination related to protected groups, such as race, gender, and
age. Our software offers techniques for discrimination analysis by identifying
and mitigating confounding variables, along with methods for reducing bias in
predictive models.
  In educational settings, dsld offers instructors powerful tools to teach
important statistical principles through motivating real world examples of
discrimination analysis. The inclusion of an 80-page Quarto book further
supports users, from statistics educators to legal professionals, in
effectively applying these analytical tools to real world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be submitted to the Journal of Statistics and Data Science
  Education</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducible Hybrid Time-Travel Retrieval in Evolving Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Staudinger, Florina Piroi, Andreas Rauber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are settings in which reproducibility of ranked lists is desirable,
such as when extracting a subset of an evolving document corpus for downstream
research tasks or in domains such as patent retrieval or in medical systematic
reviews, with high reproducibility expectations. However, as global term
statistics change when documents change or are added to a corpus, queries using
typical ranked retrieval models are not even reproducible for the parts of the
document corpus that have not changed. Thus, Boolean retrieval frequently
remains the mechanism of choice in such settings.
  We present a hybrid retrieval system combining Lucene for fast retrieval with
a column-store-based retrieval system maintaining a versioned and time-stamped
index. The latter component allows re-execution of previously posed queries
resulting in the same ranked list and further allows for time-travel queries
over evolving collection, as web archives, while maintaining the original
ranking. Thus, retrieval results in evolving document collections are fully
reproducible even when document collections and thus term statistics change.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Liu, Xueyu Hu, Shengyu Zhang, Jingyuan Chen, Fan Wu, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven to be an effective method for
mitigating hallucination issues inherent in large language models (LLMs).
Previous approaches typically train retrievers based on semantic similarity,
lacking optimization for RAG. More recent works have proposed aligning
retrievers with the preference signals of LLMs. However, these preference
signals are often difficult for dense retrievers, which typically have weaker
language capabilities, to understand and learn effectively. Drawing inspiration
from pedagogical theories like Guided Discovery Learning, we propose a novel
framework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the
language capabilities of LLMs to construct examples from a more granular,
information-centric perspective to guide the learning of retrievers.
Specifically, our method utilizes LLMs to construct easy-to-understand examples
from samples where the retriever performs poorly, focusing on three learning
objectives highly relevant to the RAG scenario: relevance, comprehensiveness,
and purity. These examples serve as scaffolding to ultimately align the
retriever with the LLM's preferences. Furthermore, we employ a dual curriculum
learning strategy and leverage the reciprocal feedback between LLM and
retriever to further enhance the performance of the RAG system. A series of
experiments demonstrate that our proposed framework enhances the performance of
RAG systems equipped with different retrievers and is applicable to various
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexicalization Is All You Need: Examining the Impact of Lexical
  Knowledge in a Compositional QALD System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Maria Schmidt, Mohammad Fazleh Elahi, Philipp Cimiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we examine the impact of lexicalization on Question Answering
over Linked Data (QALD). It is well known that one of the key challenges in
interpreting natural language questions with respect to SPARQL lies in bridging
the lexical gap, that is mapping the words in the query to the correct
vocabulary elements. We argue in this paper that lexicalization, that is
explicit knowledge about the potential interpretations of a word with respect
to the given vocabulary, significantly eases the task and increases the
performance of QA systems. Towards this goal, we present a compositional QA
system that can leverage explicit lexical knowledge in a compositional manner
to infer the meaning of a question in terms of a SPARQL query. We show that
such a system, given lexical knowledge, has a performance well beyond current
QA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ score
compared to the best QA system on QALD-9. This shows the importance and
potential of including explicit lexical knowledge. In contrast, we show that
LLMs have limited abilities to exploit lexical knowledge, with only marginal
improvements compared to a version without lexical knowledge. This shows that
LLMs have no ability to compositionally interpret a question on the basis of
the meaning of its parts, a key feature of compositional approaches. Taken
together, our work shows new avenues for QALD research, emphasizing the
importance of lexicalization and compositionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24th International Conference on Knowledge Engineering and Knowledge
  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Fusion of Synthetic Query Variants With Generative Large Language
  Models <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Breuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering query variance in information retrieval (IR) experiments is
beneficial for retrieval effectiveness. Especially ranking ensembles based on
different topically related queries retrieve better results than rankings based
on a single query alone. Recently, generative instruction-tuned Large Language
Models (LLMs) improved on a variety of different tasks in capturing human
language. To this end, this work explores the feasibility of using synthetic
query variants generated by instruction-tuned LLMs in data fusion experiments.
More specifically, we introduce a lightweight, unsupervised, and cost-efficient
approach that exploits principled prompting and data fusion techniques. In our
experiments, LLMs produce more effective queries when provided with additional
context information on the topic. Furthermore, our analysis based on four TREC
newswire benchmarks shows that data fusion based on synthetic query variants is
significantly better than baselines with single queries and also outperforms
pseudo-relevance feedback methods. We publicly share the code and query
datasets with the community as resources for follow-up studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The definitive version of record was published in SIGIR-AP '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Essence of the Essence from the Web:The Metasearch Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajender Nath, Satinder Bal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of information source on the web and in turn
continuing technological progress of searching the information by using tools
like Search Engines gives rise to many problems for the user to know which tool
is best for their query and which tool is not. At this time Metasearch Engine
comes into play by reducing the user burden by dispatching queries to multiple
search engines in parallel and refining the results of these search engines to
give the best out of best by doing superior job on their side. These engines do
not own a database of Web pages rather they send search terms to the databases
maintained by the search engine companies, get back results from all the search
engines queried and then compile the results to be presented to the user. In
this paper, we describe the working of a typical metasearch engine and then
present a comparative study of traditional search engines and metasearch
engines on the basis of different parameters and show how metasearch engines
are better than the other search engines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEGMN: A Structure-Enhanced Graph Matching Network for Graph Similarity
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Wang, Jiacheng Lu, Kejia Chen, Zheng Liu, Shilong Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph similarity computation (GSC) aims to quantify the similarity score
between two graphs. Although recent GSC methods based on graph neural networks
(GNNs) take advantage of intra-graph structures in message passing, few of them
fully utilize the structures presented by edges to boost the representation of
their connected nodes. Moreover, previous cross-graph node embedding matching
lacks the perception of the overall structure of the graph pair, due to the
fact that the node representations from GNNs are confined to the intra-graph
structure, causing the unreasonable similarity score. Intuitively, the
cross-graph structure represented in the assignment graph is helpful to rectify
the inappropriate matching. Therefore, we propose a structure-enhanced graph
matching network (SEGMN). Equipped with a dual embedding learning module and a
structure perception matching module, SEGMN achieves structure enhancement in
both embedding learning and cross-graph matching. The dual embedding learning
module incorporates adjacent edge representation into each node to achieve a
structure-enhanced representation. The structure perception matching module
achieves cross-graph structure enhancement through assignment graph
convolution. The similarity score of each cross-graph node pair can be
rectified by aggregating messages from structurally relevant node pairs.
Experimental results on benchmark datasets demonstrate that SEGMN outperforms
the state-of-the-art GSC methods in the GED regression task, and the structure
perception matching module is plug-and-play, which can further improve the
performance of the baselines by up to 25%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge
  Reasoning and Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, Chihang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to optimize the existing retrieval-augmented generation model
(RAG) by introducing a graph structure to improve the performance of the model
in dealing with complex knowledge reasoning tasks. The traditional RAG model
has the problem of insufficient processing efficiency when facing complex graph
structure information (such as knowledge graphs, hierarchical relationships,
etc.), which affects the quality and consistency of the generated results. This
study proposes a scheme to process graph structure data by combining graph
neural network (GNN), so that the model can capture the complex relationship
between entities, thereby improving the knowledge consistency and reasoning
ability of the generated text. The experiment used the Natural Questions (NQ)
dataset and compared it with multiple existing generation models. The results
show that the graph-based RAG model proposed in this paper is superior to the
traditional generation model in terms of quality, knowledge consistency, and
reasoning ability, especially when dealing with tasks that require
multi-dimensional reasoning. Through the combination of the enhancement of the
retrieval module and the graph neural network, the model in this study can
better handle complex knowledge background information and has broad potential
value in multiple practical application scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContextIQ: A Multimodal Expert-Based Video Retrieval System for
  Contextual Advertising <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Chaubey, Anoubhav Agarwaal, Sartaki Sinha Roy, Aayush Agrawal, Susmita Ghose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual advertising serves ads that are aligned to the content that the
user is viewing. The rapid growth of video content on social platforms and
streaming services, along with privacy concerns, has increased the need for
contextual advertising. Placing the right ad in the right context creates a
seamless and pleasant ad viewing experience, resulting in higher audience
engagement and, ultimately, better ad monetization. From a technology
standpoint, effective contextual advertising requires a video retrieval system
capable of understanding complex video content at a very granular level.
Current text-to-video retrieval models based on joint multimodal training
demand large datasets and computational resources, limiting their practicality
and lacking the key functionalities required for ad ecosystem integration. We
introduce ContextIQ, a multimodal expert-based video retrieval system designed
specifically for contextual advertising. ContextIQ utilizes modality-specific
experts-video, audio, transcript (captions), and metadata such as objects,
actions, emotion, etc.-to create semantically rich video representations. We
show that our system, without joint training, achieves better or comparable
results to state-of-the-art models and commercial solutions on multiple
text-to-video retrieval benchmarks. Our ablation studies highlight the benefits
of leveraging multiple modalities for enhanced video retrieval accuracy instead
of using a vision-language model alone. Furthermore, we show how video
retrieval systems such as ContextIQ can be used for contextual advertising in
an ad ecosystem while also addressing concerns related to brand safety and
filtering inappropriate content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersianRAG: A Retrieval-Augmented Generation System for Persian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Hosseini, Mohammad Sobhan Zare, Amir Hossein Mohammadi, Arefeh Kazemi, Zahra Zojaji, Mohammad Ali Nematbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Compositional Data Augmentation for Scientific Keyphrase Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mael Houbre, Florian Boudin, Beatrice Daille, Akiko Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models for keyphrase generation require large amounts of
training data to achieve good performance. However, obtaining keyphrase-labeled
documents can be challenging and costly. To address this issue, we present a
self-compositional data augmentation method. More specifically, we measure the
relatedness of training documents based on their shared keyphrases, and combine
similar documents to generate synthetic samples. The advantage of our method
lies in its ability to create additional training samples that keep domain
coherence, without relying on external data or resources. Our results on
multiple datasets spanning three different domains, demonstrate that our method
consistently improves keyphrase generation. A qualitative analysis of the
generated keyphrases for the Computer Science domain confirms this improvement
towards their representativity property.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to JCDL 2024. This is the author's version of the work. It
  is posted here for your personal use. Not for redistribution. The definitive
  version was published in the proceedings of the 2024 ACM/IEEE Joint
  Conference on Digital Libraries (JCDL 24)
  https://doi.org/10.1145/3677389.3702504</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CheX-<span class="highlight-title">GPT</span>: Harnessing Large Language Models for Enhanced Chest X-ray
  Report Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jawook Gu, Kihyun You, Han-Cheol Cho, Jiho Kim, Eun Kyoung Hong, Byungseok Roh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-text radiology reports present a rich data source for various medical
tasks, but effectively labeling these texts remains challenging. Traditional
rule-based labeling methods fall short of capturing the nuances of diverse
free-text patterns. Moreover, models using expert-annotated data are limited by
data scarcity and pre-defined classes, impacting their performance, flexibility
and scalability. To address these issues, our study offers three main
contributions: 1) We demonstrate the potential of GPT as an adept labeler using
carefully designed prompts. 2) Utilizing only the data labeled by GPT, we
trained a BERT-based labeler, CheX-GPT, which operates faster and more
efficiently than its GPT counterpart. 3) To benchmark labeler performance, we
introduced a publicly available expert-annotated test set, MIMIC-500,
comprising 500 cases from the MIMIC validation set. Our findings demonstrate
that CheX-GPT not only excels in labeling accuracy over existing models, but
also showcases superior efficiency, flexibility, and scalability, supported by
our introduction of the MIMIC-500 dataset for robust benchmarking. Code and
models are available at https://github.com/Soombit-ai/CheXGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Real-Time Adaptive Multi-Stream GPU System for Online Approximate
  Nearest Neighborhood Search <span class="chip">CIKM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Sun, Yang Shi, Jiaolong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Approximate Nearest Neighbor Search (ANNS) has played a
pivotal role in modern search and recommendation systems, especially in
emerging LLM applications like Retrieval-Augmented Generation. There is a
growing exploration into harnessing the parallel computing capabilities of GPUs
to meet the substantial demands of ANNS. However, existing systems primarily
focus on offline scenarios, overlooking the distinct requirements of online
applications that necessitate real-time insertion of new vectors. This
limitation renders such systems inefficient for real-world scenarios. Moreover,
previous architectures struggled to effectively support real-time insertion due
to their reliance on serial execution streams. In this paper, we introduce a
novel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our
architecture achieves its objectives through three key advancements: 1) We
initially examined the real-time insertion mechanisms in existing GPU ANNS
systems and discovered their reliance on repetitive copying and memory
allocation, which significantly hinders real-time effectiveness on GPUs. As a
solution, we introduce a dynamic vector insertion algorithm based on memory
blocks, which includes in-place rearrangement. 2) To enable real-time vector
insertion in parallel, we introduce a multi-stream parallel execution mode,
which differs from existing systems that operate serially within a single
stream. Our system utilizes a dynamic resource pool, allowing multiple streams
to execute concurrently without additional execution blocking. 3) Through
extensive experiments and comparisons, our approach effectively handles varying
QPS levels across different datasets, reducing latency by up to 40%-80%. The
proposed system has also been deployed in real-world industrial search and
recommendation systems, serving hundreds of millions of users daily, and has
achieved good results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CIKM'24, V2 fixes some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELASTIC: Efficient Linear Attention for Sequential Interest Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Deng, Shiyao Wang, Song Lu, Yinfeng Li, Xinchen Luo, Yuanjun Liu, Peixing Xu, Guorui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art sequential recommendation models heavily rely on
transformer's attention mechanism. However, the quadratic computational and
memory complexities of self attention have limited its scalability for modeling
users' long range behaviour sequences. To address this problem, we propose
ELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,
requiring only linear time complexity and decoupling model capacity from
computational cost. Specifically, ELASTIC introduces a fixed length interest
experts with linear dispatcher attention mechanism which compresses the
long-term behaviour sequences to a significantly more compact representation
which reduces up to 90% GPU memory usage with x2.7 inference speed up. The
proposed linear dispatcher attention mechanism significantly reduces the
quadratic complexity and makes the model feasible for adequately modeling
extremely long sequences. Moreover, in order to retain the capacity for
modeling various user interests, ELASTIC initializes a vast learnable interest
memory bank and sparsely retrieves compressed user's interests from the memory
with a negligible computational overhead. The proposed interest memory
retrieval technique significantly expands the cardinality of available interest
space while keeping the same computational cost, thereby striking a trade-off
between recommendation accuracy and efficiency. To validate the effectiveness
of our proposed ELASTIC, we conduct extensive experiments on various public
datasets and compare it with several strong sequential recommenders.
Experimental results demonstrate that ELASTIC consistently outperforms
baselines by a significant margin and also highlight the computational
efficiency of ELASTIC when modeling long sequences. We will make our
implementation code publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We hereby withdraw this paper from arXiv due to incomplete
  experiments. Upon further review, we have determined that additional
  experimental work is necessary to fully validate our findings and conclusions</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harmful YouTube Video Detection: A Taxonomy of Online Harm and MLLMs as
  Alternative Annotators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claire Wonjeong Jo, Miki Wesołowska, Magdalena Wojcieszak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short video platforms, such as YouTube, Instagram, or TikTok, are used by
billions of users globally. These platforms expose users to harmful content,
ranging from clickbait or physical harms to misinformation or online hate. Yet,
detecting harmful videos remains challenging due to an inconsistent
understanding of what constitutes harm and limited resources and mental tolls
involved in human annotation. As such, this study advances measures and methods
to detect harm in video content. First, we develop a comprehensive taxonomy for
online harm on video platforms, categorizing it into six categories:
Information, Hate and harassment, Addictive, Clickbait, Sexual, and Physical
harms. Next, we establish multimodal large language models as reliable
annotators of harmful videos. We analyze 19,422 YouTube videos using 14 image
frames, 1 thumbnail, and text metadata, comparing the accuracy of crowdworkers
(Mturk) and GPT-4-Turbo with domain expert annotations serving as the gold
standard. Our results demonstrate that GPT-4-Turbo outperforms crowdworkers in
both binary classification (harmful vs. harmless) and multi-label harm
categorization tasks. Methodologically, this study extends the application of
LLMs to multi-label and multi-modal contexts beyond text annotation and binary
classification. Practically, our study contributes to online harm mitigation by
guiding the definitions and identification of harmful content on video
platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Form Text-to-Music Generation with Adaptive <span class="highlight-title">Prompt</span>s: A Case of
  Study in Tabletop Role-Playing Games Soundtracks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Marra, Lucas N. Ferreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the capabilities of text-to-audio music generation
models in producing long-form music with prompts that change over time,
focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We
introduce Babel Bardo, a system that uses Large Language Models (LLMs) to
transform speech transcriptions into music descriptions for controlling a
text-to-music model. Four versions of Babel Bardo were compared in two TRPG
campaigns: a baseline using direct speech transcriptions, and three LLM-based
versions with varying approaches to music description generation. Evaluations
considered audio quality, story alignment, and transition smoothness. Results
indicate that detailed music descriptions improve audio quality while
maintaining consistency across consecutive descriptions enhances story
alignment and transition smoothness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at the LAMIR 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inter-Frame Coding for Dynamic Meshes via Coarse-to-Fine Anchor Mesh
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Huang, Lizhi Hou, Qi Yang, Yiling Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current Video-based Dynamic Mesh Coding (V-DMC) standard, inter-frame
coding is restricted to mesh frames with constant topology. Consequently,
temporal redundancy is not fully leveraged, resulting in suboptimal compression
efficacy. To address this limitation, this paper introduces a novel
coarse-to-fine scheme to generate anchor meshes for frames with time-varying
topology. Initially, we generate a coarse anchor mesh using an octree-based
nearest neighbor search. Motion estimation compensates for regions with
significant motion changes during this process. However, the quality of the
coarse mesh is low due to its suboptimal vertices. To enhance details, the fine
anchor mesh is further optimized using the Quadric Error Metrics (QEM)
algorithm to calculate more precise anchor points. The inter-frame anchor mesh
generated herein retains the connectivity of the reference base mesh, while
concurrently preserving superior quality. Experimental results show that our
method achieves 7.2% ~ 10.3% BD-rate gain compared to the existing V-DMC test
model version 7.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting dataset contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is sensitive to varying degrees of contamination and can
highlight significant performance improvements due to leakage of the training
set of multimodal benchmarks. Furthermore, We also explore the possibility of
contamination originating from the pre-training phase of LLMs used by MLLMs and
the fine-tuning phase of MLLMs, offering new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for
  Neural Image Codec <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Hyuk Kim, Seungeon Kim, Won-Hee Lee, Dokwan Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing a fast and effective entropy model is challenging but essential for
practical application of neural codecs. Beyond spatial autoregressive entropy
models, more efficient backward adaptation-based entropy models have been
recently developed. They not only reduce decoding time by using smaller number
of modeling steps but also maintain or even improve rate--distortion
performance by leveraging more diverse contexts for backward adaptation.
Despite their significant progress, we argue that their performance has been
limited by the simple adoption of the design convention for forward adaptation:
using only a single type of hyper latent representation, which does not provide
sufficient contextual information, especially in the first modeling step. In
this paper, we propose a simple yet effective entropy modeling framework that
leverages sufficient contexts for forward adaptation without compromising on
bit-rate. Specifically, we introduce a strategy of diversifying hyper latent
representations for forward adaptation, i.e., using two additional types of
contexts along with the existing single type of context. In addition, we
present a method to effectively use the diverse contexts for contextualizing
the current elements to be encoded/decoded. By addressing the limitation of the
previous approach, our proposed framework leads to significant performance
improvements. Experimental results on popular datasets show that our proposed
framework consistently improves rate--distortion performance across various
bit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline
on the Kodak dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Conceptual Blending of a Diffusion Model for Improving
  Nonword-to-Image Generation <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihaya Matsuhira, Marc A. Kastner, Takahiro Komamizu, Takatsugu Hirayama, Ichiro Ide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models sometimes depict blended concepts in the
generated images. One promising use case of this effect would be the
nonword-to-image generation task which attempts to generate images intuitively
imaginable from a non-existing word (nonword). To realize nonword-to-image
generation, an existing study focused on associating nonwords with
similar-sounding words. Since each nonword can have multiple similar-sounding
words, generating images containing their blended concepts would increase
intuitiveness, facilitating creative activities and promoting computational
psycholinguistics. Nevertheless, no existing study has quantitatively evaluated
this effect in either diffusion models or the nonword-to-image generation
paradigm. Therefore, this paper first analyzes the conceptual blending in a
pretrained diffusion model, Stable Diffusion. The analysis reveals that a high
percentage of generated images depict blended concepts when inputting an
embedding interpolating between the text embeddings of two text prompts
referring to different concepts. Next, this paper explores the best text
embedding space conversion method of an existing nonword-to-image generation
framework to ensure both the occurrence of conceptual blending and image
generation quality. We compare the conventional direct prediction approach with
the proposed method that combines $k$-nearest neighbor search and linear
regression. Evaluation reveals that the enhanced accuracy of the embedding
space conversion by the proposed method improves the image generation quality,
while the emergence of conceptual blending could be attributed mainly to the
specific dimensions of the high-dimensional text embedding space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at ACM MM 2024 (doi: 10.1145/3664647.3681202) with
  supplementary materials concatenated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Collecting Training <span class="highlight-title">Dataset</span> for 2D Object Detection by
  Online Visual Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Kiyokawa, Naoki Shirakura, Hiroki Katayama, Keita Tomochika, Jun Takamatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep-learning-based vision systems require the manual annotation of
a significant number of images. Such manual annotation is highly time-consuming
and labor-intensive. Although previous studies have attempted to eliminate the
effort required for annotation, the effort required for image collection was
retained. To address this, we propose a human-in-the-loop dataset collection
method that uses a web application. To counterbalance the workload and
performance by encouraging the collection of multi-view object image datasets
in an enjoyable manner, thereby amplifying motivation, we propose three types
of online visual feedback features to track the progress of the collection
status. Our experiments thoroughly investigated the impact of each feature on
collection performance and quality of operation. The results suggested the
feasibility of annotation and object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Learned Image Compression-Resistant Adversarial
  Perturbations <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks can readily disrupt the image classification system,
revealing the vulnerability of DNN-based recognition tasks. While existing
adversarial perturbations are primarily applied to uncompressed images or
compressed images by the traditional image compression method, i.e., JPEG,
limited studies have investigated the robustness of models for image
classification in the context of DNN-based image compression. With the rapid
evolution of advanced image compression, DNN-based learned image compression
has emerged as the promising approach for transmitting images in many
security-critical applications, such as cloud-based face recognition and
autonomous driving, due to its superior performance over traditional
compression. Therefore, there is a pressing need to fully investigate the
robustness of a classification system post-processed by learned image
compression. To bridge this research gap, we explore the adversarial attack on
a new pipeline that targets image classification models that utilize learned
image compressors as pre-processing modules. Furthermore, to enhance the
transferability of perturbations across various quality levels and
architectures of learned image compression models, we introduce a saliency
score-based sampling method to enable the fast generation of transferable
perturbation. Extensive experiments with popular attack methods demonstrate the
enhanced transferability of our proposed method when attacking images that have
been post-processed with different learned image compression models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Zhaoxi Ke, Yiyi Zhou, Gen Luo, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, mixture of experts (MoE) has become a popular paradigm for
achieving the trade-off between modal capacity and efficiency of multi-modal
large language models (MLLMs). Different from previous efforts, we are
dedicated to exploring the dynamic expert path in an already exist MLLM and
show that a standard MLLM can be also a mixture of experts. To approach this
target, we propose a novel dynamic expert scheme for MLLMs, termed Routing
Experts (RoE), which can achieve example-dependent optimal path routing without
obvious structure tweaks. Meanwhile, a new regularization of structure sparsity
is also introduced to enforce MLLMs to learn more short-cut inference, ensuring
the efficiency. In addition, we also realize the first attempt of aligning the
training and inference schemes of MLLMs in terms of network routing. To
validate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,
LLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL
benchmarks. The experiment results not only show the great advantages of our
RoE in improving MLLMs' efficiency, but also yield obvious advantages than
MoE-LLaVA in both performance and speed, e.g., an average performance gain of
3.3% on 5 benchmarks while being faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large
  Language Models <span class="chip">EMNLP24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Xianghu Yue, Xiaoxue Gao, Chen Zhang, Luis Fernando D'Haro, Robby T. Tan, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various audio-LLMs (ALLMs) have been explored recently for tackling different
audio tasks simultaneously using a single, unified model. While existing
evaluations of ALLMs primarily focus on single-audio tasks, real-world
applications often involve processing multiple audio streams simultaneously. To
bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark
that consists of 20 datasets from 11 multi-audio tasks encompassing both speech
and sound scenarios. Comprehensive experiments on MAE demonstrate that the
existing ALLMs, while being powerful in comprehending primary audio elements in
individual audio inputs, struggling to handle multi-audio scenarios. To this
end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among
multiple similar audios using discriminative learning on our proposed synthetic
data. The results demonstrate that the proposed MALLM outperforms all baselines
and achieves high data efficiency using synthetic data without requiring human
annotations. The proposed MALLM opens the door for ALLMs towards multi-audio
processing era and brings us closer to replicating human auditory capabilities
in machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP24 Findings. Data available at
  https://github.com/MatthewCYM/MALLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document Parsing Unveiled: Techniques, Challenges, and Prospects for
  Structured Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document parsing is essential for converting unstructured and semi-structured
documents-such as contracts, academic papers, and invoices-into structured,
machine-readable data. Document parsing extract reliable structured data from
unstructured inputs, providing huge convenience for numerous applications.
Especially with recent achievements in Large Language Models, document parsing
plays an indispensable role in both knowledge base construction and training
data generation. This survey presents a comprehensive review of the current
state of document parsing, covering key methodologies, from modular pipeline
systems to end-to-end models driven by large vision-language models. Core
components such as layout detection, content extraction (including text,
tables, and mathematical expressions), and multi-modal data integration are
examined in detail. Additionally, this paper discusses the challenges faced by
modular document parsing systems and vision-language models in handling complex
layouts, integrating multiple modules, and recognizing high-density text. It
emphasizes the importance of developing larger and more diverse datasets and
outlines future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-05T00:00:00Z">2024-11-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated, LLM enabled extraction of synthesis details for reticular
  materials from scientific literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viviane Torres da Silva, Alexandre Rademaker, Krystelle Lionti, Ronaldo Giro, Geisa Lima, Sandro Fiorini, Marcelo Archanjo, Breno W. Carvalho, Rodrigo Neumann, Anaximandro Souza, João Pedro Souza, Gabriela de Valnisio, Carmen Nilda Paz, Renato Cerqueira, Mathias Steiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated knowledge extraction from scientific literature can potentially
accelerate materials discovery. We have investigated an approach for extracting
synthesis protocols for reticular materials from scientific literature using
large language models (LLMs). To that end, we introduce a Knowledge Extraction
Pipeline (KEP) that automatizes LLM-assisted paragraph classification and
information extraction. By applying prompt engineering with in-context learning
(ICL) to a set of open-source LLMs, we demonstrate that LLMs can retrieve
chemical information from PDF documents, without the need for fine-tuning or
training and at a reduced risk of hallucination. By comparing the performance
of five open-source families of LLMs in both paragraph classification and
information extraction tasks, we observe excellent model performance even if
only few example paragraphs are included in the ICL prompts. The results show
the potential of the KEP approach for reducing human annotations and data
curation efforts in automated scientific knowledge extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Hierarchical Representation for Medication
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Liang, Yuting Liu, Yizhou Dang, Enneng Yang, Guibing Guo, Wei Cai, Jianzhe Zhao, Xingwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medication recommender is to suggest appropriate medication combinations
based on a patient's health history, e.g., diagnoses and procedures. Existing
works represent different diagnoses/procedures well separated by one-hot
encodings. However, they ignore the latent hierarchical structures of these
medical terms, undermining the generalization performance of the model. For
example, "Respiratory Diseases", "Chronic Respiratory Diseases" and "Chronic
Bronchiti" have a hierarchical relationship, progressing from general to
specific. To address this issue, we propose a novel hierarchical encoder named
HIER to hierarchically represent diagnoses and procedures, which is based on
standard medical codes and compatible with any existing methods. Specifically,
the proposed method learns relation embedding with a self-supervised objective
for incorporating the neighbor hierarchical structure. Additionally, we develop
the position encoding to explicitly introduce global hierarchical position.
Extensive experiments demonstrate significant and consistent improvements in
recommendation accuracy across four baselines and two real-world clinical
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Effective Adaptation of Multimodal Foundation Models in
  Sequential Recommendation <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchen Fu, Xuri Ge, Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Kaiwen Zheng, Yongxin Ni, Joemon M. Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models (MFMs) have revolutionized sequential
recommender systems through advanced representation learning. While
Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models,
studies often prioritize parameter efficiency, neglecting GPU memory and
training speed. To address this, we introduced the IISAN framework,
significantly enhancing efficiency. However, IISAN was limited to symmetrical
MFMs and identical text and image encoders, preventing the use of
state-of-the-art Large Language Models. To overcome this, we developed
IISAN-Versa, a versatile plug-and-play architecture compatible with both
symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT
structure and utilizes both intra- and inter-modal adaptation. It effectively
handles asymmetry through a simple yet effective combination of group
layer-dropping and dimension transformation alignment. Our research
demonstrates that IISAN-Versa effectively adapts large text encoders, and we
further identify a scaling effect where larger encoders generally perform
better. IISAN-Versa also demonstrates strong versatility in our defined
multimodal scenarios, which include raw titles and captions generated from
images and videos. Additionally, IISAN-Versa achieved state-of-the-art
performance on the Microlens public benchmark. We will release our code and
datasets to support future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The extension of IISAN in SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge
  in RAG Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has been shown to improve knowledge
capabilities and alleviate the hallucination problem of LLMs. The Web is a
major source of external knowledge used in RAG systems, and many commercial
systems such as ChatGPT and Perplexity have used Web search engines as their
major retrieval systems. Typically, such RAG systems retrieve search results,
download HTML sources of the results, and then extract plain texts from the
HTML sources. Plain text documents or chunks are fed into the LLMs to augment
the generation. However, much of the structural and semantic information
inherent in HTML, such as headings and table structures, is lost during this
plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,
which uses HTML instead of plain text as the format of retrieved knowledge in
RAG. We believe HTML is better than plain text in modeling knowledge in
external documents, and most LLMs possess robust capacities to understand HTML.
However, utilizing HTML presents new challenges. HTML contains additional
content such as tags, JavaScript, and CSS specifications, which bring extra
input tokens and noise to the RAG system. To address this issue, we propose
HTML cleaning, compression, and pruning strategies, to shorten the HTML while
minimizing the loss of information. Specifically, we design a two-step
block-tree-based pruning method that prunes useless HTML blocks and keeps only
the relevant part of the HTML. Experiments on six QA datasets confirm the
superiority of using HTML in RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document
  Relation Extraction with Graph-of-Thoughts Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhang, Ning Yan, Masood Mortazavi, Hoang H. Nguyen, Zhongfen Deng, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning capability on many NLP tasks. Recasting an NLP
task into a text-to-text generation task is a common practice so that
generative LLMs can be prompted to resolve it. However, performing
document-level relation extraction (DocRE) tasks with generative LLM models is
still challenging due to the structured output format of DocRE, which
complicates the conversion to plain text. Limited information available in
few-shot samples and prompt instructions induce further difficulties and
challenges in relation extraction for mentioned entities in a document. In this
paper, we represent the structured output as a graph-style triplet rather than
natural language expressions and leverage generative LLMs for the DocRE task.
Our approach, the Graph-DPEP framework is grounded in the reasoning behind
triplet explanation thoughts presented in natural language. In this framework,
we first introduce a ``decomposed-plug" method for performing the generation
from LLMs over prompts with type-space decomposition to alleviate the burden of
distinguishing all relation types. Second, we employ a verifier for calibrating
the generation and identifying overlooked query entity pairs. Third, we develop
"ensemble-play", reapplying generation on the entire type list by leveraging
the reasoning thoughts embedded in a sub-graph associated with the missing
query pair to address the missingness issue. Through extensive comparisons with
existing prompt techniques and alternative Language Models (LLMs), our
framework demonstrates superior performance on publicly available benchmarks in
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyin Chen, Haonan Ma, Haibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph has become increasingly integral to the advancement of recommendation
systems, particularly with the fast development of graph neural network(GNN).
By exploring the virtue of rich node features and link information, GNN is
designed to provide personalized and accurate suggestions. Meanwhile, the
privacy leakage of GNN in such contexts has also captured special attention.
Prior work has revealed that a malicious user can utilize auxiliary knowledge
to extract sensitive link data of the target graph, integral to recommendation
systems, via the decision made by the target GNN model. This poses a
significant risk to the integrity and confidentiality of data used in
recommendation system. Though important, previous works on GNN's privacy
leakage are still challenged in three aspects, i.e., limited stealing attack
scenarios, sub-optimal attack performance, and adaptation against defense. To
address these issues, we propose a diffusion model based link stealing attack,
named DM4Steal. It differs previous work from three critical aspects. (i)
Generality: aiming at six attack scenarios with limited auxiliary knowledge, we
propose a novel training strategy for diffusion models so that DM4Steal is
transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from
the retention of semantic structure in the diffusion model during the training
process, DM4Steal is capable to learn the precise topology of the target graph
through the GNN decision process. (iii) Adaptation: when GNN is defensive
(e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling
the score model multiple times to keep performance degradation to a minimum,
thus DM4Steal implements successful adaptive attack on defensive GNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual
  Visual Answer Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Wen, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Multilingual Visual Answer Localization (MVAL) is to locate a
video segment that answers a given multilingual question. Existing methods
either focus solely on visual modality or integrate visual and subtitle
modalities. However, these methods neglect the audio modality in videos,
consequently leading to incomplete input information and poor performance in
the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span
Localization (AVTSL) method that incorporates audio modality to augment both
visual and textual representations for the MVAL task. Specifically, we
integrate features from three modalities and develop three predictors, each
tailored to the unique contributions of the fused modalities: an audio-visual
predictor, a visual predictor, and a textual predictor. Each predictor
generates predictions based on its respective modality. To maintain consistency
across the predicted results, we introduce an Audio-Visual-Textual Consistency
module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing
each modality's predictor to dynamically learn from the others. This
collaborative learning ensures that the model generates consistent and
comprehensive answers. Extensive experiments show that our proposed method
outperforms several state-of-the-art (SOTA) methods, which demonstrates the
effectiveness of the audio modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African
  clean water access, sanitation and hygiene 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Kloker, Alex Cedric Luyima, Matthew Bazanya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate
rural African communities on clean water access, sanitation, and hygiene (WASH)
principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach
to address the limitations of previous approaches with limited reach or missing
contextualization. The paper details the development process, employing Design
Science Research Methodology. The evaluation consisted of two phases: content
validation by four WASH experts and community validation by potential users.
Content validation confirmed WASHtsApp's ability to provide accurate and
relevant WASH-related information. Community validation indicated high user
acceptance and perceived usefulness of the chatbot. The paper concludes by
discussing the potential for further development, including incorporating local
languages and user data analysis for targeted interventions. It also proposes
future research cycles focused on wider deployment and leveraging user data for
educational purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing EmoBot: An In-Depth Analysis of User Satisfaction and Faults
  in an Emotion-Aware Chatbot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taseen Mubassira, Mehedi Hasan, A. B. M. Alim Al Iislam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research community has traditionally shown a keen interest in emotion
modeling, with a notable emphasis on the detection aspect. In contrast, the
exploration of emotion generation has received less attention.This study delves
into an existing state-of-the-art emotional chatbot, EmoBot, designed for
generating emotions in general-purpose conversations. This research involves a
comprehensive examination, including a survey to evaluate EmoBot's proficiency
in key dimensions like usability, accuracy, and overall user satisfaction, with
a specific focus on fault tolerance. By closely examining the chatbot's
operations, we identified some noteworthy shortcomings in the existing model.
We propose some solutions designed to address and overcome the identified
issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, extended abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Vision-Language Models for Manufacturing Feature Recognition
  in CAD Designs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew Jin Tan, Seung Ki Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic feature recognition (AFR) is essential for transforming design
knowledge into actionable manufacturing information. Traditional AFR methods,
which rely on predefined geometric rules and large datasets, are often
time-consuming and lack generalizability across various manufacturing features.
To address these challenges, this study investigates vision-language models
(VLMs) for automating the recognition of a wide range of manufacturing features
in CAD designs without the need for extensive training datasets or predefined
rules. Instead, prompt engineering techniques, such as multi-view query images,
few-shot learning, sequential reasoning, and chain-of-thought, are applied to
enable recognition. The approach is evaluated on a newly developed CAD dataset
containing designs of varying complexity relevant to machining, additive
manufacturing, sheet metal forming, molding, and casting. Five VLMs, including
three closed-source models (GPT-4o, Claude-3.5-Sonnet, and Claude-3.0-Opus) and
two open-source models (LLava and MiniCPM), are evaluated on this dataset with
ground truth features labelled by experts. Key metrics include feature quantity
accuracy, feature name matching accuracy, hallucination rate, and mean absolute
error (MAE). Results show that Claude-3.5-Sonnet achieves the highest feature
quantity accuracy (74%) and name-matching accuracy (75%) with the lowest MAE
(3.2), while GPT-4o records the lowest hallucination rate (8%). In contrast,
open-source models have higher hallucination rates (>30%) and lower accuracies
(<40%). This study demonstrates the potential of VLMs to automate feature
recognition in CAD designs within diverse manufacturing scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper has been submitted to The ASME Journal of Computing and
  Information Science in Engineering (JCISE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models and Cycle Consistency for Self-Reflective Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiao Wangni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory Augmented Cross-encoders for Controllable Personalized Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheshera Mysore, Garima Dhanania, Kishor Patil, Surya Kallumadi, Andrew McCallum, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized search represents a problem where retrieval models condition on
historical user interaction data in order to improve retrieval results.
However, personalization is commonly perceived as opaque and not amenable to
control by users. Further, personalization necessarily limits the space of
items that users are exposed to. Therefore, prior work notes a tension between
personalization and users' ability for discovering novel items. While discovery
of novel items in personalization setups may be resolved through search result
diversification, these approaches do little to allow user control over
personalization. Therefore, in this paper, we introduce an approach for
controllable personalized search. Our model, CtrlCE presents a novel
cross-encoder model augmented with an editable memory constructed from users
historical items. Our proposed memory augmentation allows cross-encoder models
to condition on large amounts of historical user data and supports interaction
from users permitting control over personalization. Further, controllable
personalization for search must account for queries which don't require
personalization, and in turn user control. For this, we introduce a calibrated
mixing model which determines when personalization is necessary. This allows
system designers using CtrlCE to only obtain user input for control when
necessary. In multiple datasets of personalized search, we show CtrlCE to
result in effective personalization as well as fulfill various key goals for
controllable personalized search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanying Ding, Vinay K. Chaudhri, Naren Chittar, Krishna Konakanchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs have emerged as a compelling abstraction for capturing key
relationship among the entities of interest to enterprises and for integrating
data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by
leveraging knowledge graphs across the organization for multiple mission
critical applications such as risk assessment, fraud detection, investment
advice, etc. A core problem in leveraging a knowledge graph is to link mentions
(e.g., company names) that are encountered in textual sources to entities in
the knowledge graph. Although several techniques exist for entity linking, they
are tuned for entities that exist in Wikipedia, and fail to generalize for the
entities that are of interest to an enterprise. In this paper, we propose a
novel end-to-end neural entity linking model (JEL) that uses minimal context
information and a margin loss to generate entity embeddings, and a Wide & Deep
Learning model to match character and semantic information respectively. We
show that JEL achieves the state-of-the-art performance to link mentions of
company names in financial news with entities in our knowledge graph. We report
on our efforts to deploy this model in the company-wide system to generate
alerts in response to financial news. The methodology used for JEL is directly
applicable and usable by other enterprises who need entity linking solutions
for data that are unique to their respective situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, IAAI-21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial
  Knowledge Graphs <span class="chip">SIGIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanying Ding, Manoj Cherukumalli, Santosh Chikoti, Vinay K. Chaudhri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs have gained popularity for their ability to organize and
analyze complex data effectively. When combined with graph embedding
techniques, such as graph neural networks (GNNs), knowledge graphs become a
potent tool in providing valuable insights. This study explores the application
of graph embedding in identifying competitors from a financial knowledge graph.
Existing state-of-the-art(SOTA) models face challenges due to the unique
attributes of our knowledge graph, including directed and undirected
relationships, attributed nodes, and minimal annotated competitor connections.
To address these challenges, we propose a novel graph embedding model,
JPEC(JPMorgan Proximity Embedding for Competitor Detection), which utilizes
graph neural network to learn from both first-order and second-order node
proximity together with vital features for competitor retrieval. JPEC had
outperformed most existing models in extensive experiments, showcasing its
effectiveness in competitor retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, accepted by SIGIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaCE: Parsimonious Concept Engineering for Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being used for a wide variety of tasks.
While they are capable of generating human-like responses, they can also
produce undesirable output including potentially harmful information, racist or
sexist language, and hallucinations. Alignment methods are designed to reduce
such undesirable outputs via techniques such as fine-tuning, prompt
engineering, and representation engineering. However, existing methods face
several challenges: some require costly fine-tuning for every alignment task;
some do not adequately remove undesirable concepts, failing alignment; some
remove benign concepts, lowering the linguistic capabilities of LLMs. To
address these issues, we propose Parsimonious Concept Engineering (PaCE), a
novel activation engineering framework for alignment. First, to sufficiently
model the concepts, we construct a large-scale concept dictionary in the
activation space, in which each atom corresponds to a semantic concept. Given
any alignment task, we instruct a concept partitioner to efficiently annotate
the concepts as benign or undesirable. Then, at inference time, we decompose
the LLM activations along the concept dictionary via sparse coding, to
accurately represent the activations as linear combinations of benign and
undesirable components. By removing the latter ones from the activations, we
reorient the behavior of the LLM towards the alignment goal. We conduct
experiments on tasks such as response detoxification, faithfulness enhancement,
and sentiment revising, and show that PaCE achieves state-of-the-art alignment
performance while maintaining linguistic capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024. GitHub repository at
  https://github.com/peterljq/Parsimonious-Concept-Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R^3AG: First Workshop on Refined and Reliable Retrieval Augmented
  Generation <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Xuri Ge, Joemon M. Jose, Haitao Yu, Weizhi Ma, Zhaochun Ren, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has gained wide attention as the key
component to improve generative models with external knowledge augmentation
from information retrieval. It has shown great prominence in enhancing the
functionality and performance of large language model (LLM)-based applications.
However, with the comprehensive application of RAG, more and more problems and
limitations have been identified, thus urgently requiring further fundamental
exploration to improve current RAG frameworks. This workshop aims to explore in
depth how to conduct refined and reliable RAG for downstream AI tasks.
  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024
to call for participants to re-examine and formulate the basic principles and
practical implementation of refined and reliable RAG. The workshop serves as a
platform for both academia and industry researchers to conduct discussions,
share insights, and foster research to build the next generation of RAG
systems. Participants will engage in discussions and presentations focusing on
fundamental challenges, cutting-edge research, and potential pathways to
improve RAG. At the end of the workshop, we aim to have a clearer understanding
of how to improve the reliability and applicability of RAG with more robust
information retrieval and language generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>R^3AG workshop overview at SIGIR-AP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facilitating Interdisciplinary Knowledge Transfer with Research Paper
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eoghan Cunningham, Derek Greene, Barry Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the extensive recommender systems literature, novelty and diversity have
been identified as key properties of useful recommendations. However, these
properties have received limited attention in the specific sub-field of
research paper recommender systems. In this work, we argue for the importance
of offering novel and diverse research paper recommendations to scientists.
This approach aims to reduce siloed reading, break down filter bubbles, and
promote interdisciplinary research. We propose a novel framework for evaluating
the novelty and diversity of research paper recommendations that leverages
methods from network analysis and natural language processing. Using this
framework, we show that the choice of representational method within a larger
research paper recommendation system can have a measurable impact on the nature
of downstream recommendations, specifically on their novelty and diversity. We
highlight a novel paper embedding method, which we demonstrate offers more
innovative and diverse recommendations without sacrificing precision, compared
to other state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review at QSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario
  Route Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Yu, Yihai Duan, Longfei Xu, Chao Chen, Shuliang Liu, Kaikui Liu, Fan Yang, Xiangxiang Chu, Ning Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-scenario route ranking (MSRR) is crucial in many industrial mapping
systems. However, the industrial community mainly adopts interactive interfaces
to encourage users to select pre-defined scenarios, which may hinder the
downstream ranking performance. In addition, in the academic community, the
multi-scenario ranking works only come from other fields, and there are no
works specifically focusing on route data due to lacking a publicly available
MSRR dataset. Moreover, all the existing multi-scenario works still fail to
address the three specific challenges of MSRR simultaneously, i.e. explosion of
scenario number, high entanglement, and high-capacity demand. Different from
the prior, to address MSRR, our key idea is to factorize the complicated
scenario in route ranking into several disentangled factor scenario patterns.
Accordingly, we propose a novel method, Disentangled Scenario Factorization
Network (DSFNet), which flexibly composes scenario-dependent parameters based
on a high-capacity multi-factor-scenario-branch structure. Then, a novel
regularization is proposed to induce the disentanglement of factor scenarios.
Furthermore, two extra novel techniques, i.e. scenario-aware batch
normalization and scenario-aware feature filtering, are developed to improve
the network awareness of scenario representation. Additionally, to facilitate
MSRR research in the academic community, we propose MSDR, the first large-scale
publicly available annotated industrial Multi-Scenario Driving Route dataset.
Comprehensive experimental results demonstrate the superiority of our DSFNet,
which has been successfully deployed in AMap to serve the major online traffic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music Foundation Model as Generic Booster for Music Downstream Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        WeiHsiang Liao, Yuhta Takida, Yukara Ikemiya, Zhi Zhong, Chieh-Hsin Lai, Giorgio Fabbro, Kazuki Shimada, Keisuke Toyama, Kinwai Cheuk, Marco A. Martínez-Ramírez, Shusuke Takahashi, Stefan Uhlich, Taketo Akama, Woosung Choi, Yuichiro Koyama, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate the efficacy of using intermediate representations from a
single foundation model to enhance various music downstream tasks. We introduce
SoniDo, a music foundation model (MFM) designed to extract hierarchical
features from target music samples. By leveraging hierarchical intermediate
features, SoniDo constrains the information granularity, leading to improved
performance across various downstream tasks including both understanding and
generative tasks. We specifically evaluated this approach on representative
tasks such as music tagging, music transcription, music source separation, and
music mixing. Our results reveal that the features extracted from foundation
models provide valuable enhancements in training downstream task models. This
highlights the capability of using features extracted from music foundation
models as a booster for downstream tasks. Our approach not only benefits
existing task-specific models but also supports music downstream tasks
constrained by data scarcity. This paves the way for more effective and
accessible music processing solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages with 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Matters: Addressing Amplification Bias and Homogeneity Issue
  for LLM-based Recommendation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14900v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14900v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqin Bao, Jizhi Zhang, Yang Zhang, Xinyue Huo, Chong Chen, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting Large Language Models (LLMs) for recommendation requires careful
consideration of the decoding process, given the inherent differences between
generating items and natural language. Existing approaches often directly apply
LLMs' original decoding methods. However, we find these methods encounter
significant challenges: 1) amplification bias -- where standard length
normalization inflates scores for items containing tokens with generation
probabilities close to 1 (termed ghost tokens), and 2) homogeneity issue --
generating multiple similar or repetitive items for a user. To tackle these
challenges, we introduce a new decoding approach named Debiasing-Diversifying
Decoding (D3). D3 disables length normalization for ghost tokens to alleviate
amplification bias, and it incorporates a text-free assistant model to
encourage tokens less frequently generated by LLMs for counteracting
recommendation homogeneity. Extensive experiments on real-world datasets
demonstrate the method's effectiveness in enhancing accuracy and diversity. The
code is available at https://github.com/SAI990323/DecodingMatters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Green Recommender Systems: Optimizing <span class="highlight-title">Dataset</span> Size for Energy-Efficient
  Algorithm Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ardalan Arabzadeh, Tobias Vente, Joeran Beel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As recommender systems become increasingly prevalent, the environmental
impact and energy efficiency of training large-scale models have come under
scrutiny. This paper investigates the potential for energy-efficient algorithm
performance by optimizing dataset sizes through downsampling techniques in the
context of Green Recommender Systems. We conducted experiments on the MovieLens
100K, 1M, 10M, and Amazon Toys and Games datasets, analyzing the performance of
various recommender algorithms under different portions of dataset size. Our
results indicate that while more training data generally leads to higher
algorithm performance, certain algorithms, such as FunkSVD and BiasedMF,
particularly with unbalanced and sparse datasets like Amazon Toys and Games,
maintain high-quality recommendations with up to a 50% reduction in training
data, achieving nDCG@10 scores within approximately 13% of full dataset
performance. These findings suggest that strategic dataset reduction can
decrease computational and environmental costs without substantially
compromising recommendation quality. This study advances sustainable and green
recommender systems by providing insights for reducing energy consumption while
maintaining effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pearl: Personalizing Large Language Model Writing Assistants with
  Generation-Calibrated Retrievers <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Bahareh Sarrafzadeh, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful large language models have facilitated the development of writing
assistants that promise to significantly improve the quality and efficiency of
composition and communication. However, a barrier to effective assistance is
the lack of personalization in LLM outputs to the author's communication style,
specialized knowledge, and values. In this paper, we address this challenge by
proposing Pearl, a LLM writing assistant personalized with a retriever that is
trained to be generation-calibrated for personalization. Generation calibration
ensures that our retriever selects historic user authored documents to augment
an LLM prompt such that they are likely to help an LLM generation better adhere
to a users' preferences. We propose two key novelties for training such a
retriever: (1) A training data selection method that identifies user requests
likely to benefit from personalization and documents that provide that benefit;
and (2) A scale-calibrating KL-divergence objective that ensures that our
retriever scores remain proportional to the downstream generation quality from
using the document for personalized generation. In a series of holistic
evaluations, we demonstrate the effectiveness of Pearl in generating long-form
texts on multiple social media datasets. Finally, we demonstrate how a
generation-calibrated retriever can double as a performance predictor --
detecting low quality retrieval, and improving potentially under-performing
outputs via revision with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Workshop on Customizable NLP at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Multimodal Large Language Models for Multimodal Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09698v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09698v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Ye, Zhi Zheng, Yishan Shen, Tianshu Wang, Hengruo Zhang, Peijun Zhu, Runlong Yu, Kai Zhang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have demonstrated significant
potential in the field of Recommendation Systems (RSs). Most existing studies
have focused on converting user behavior logs into textual prompts and
leveraging techniques such as prompt tuning to enable LLMs for recommendation
tasks. Meanwhile, research interest has recently grown in multimodal
recommendation systems that integrate data from images, text, and other sources
using modality fusion techniques. This introduces new challenges to the
existing LLM-based recommendation paradigm which relies solely on text modality
information. Moreover, although Multimodal Large Language Models (MLLMs)
capable of processing multi-modal inputs have emerged, how to equip MLLMs with
multi-modal recommendation capabilities remains largely unexplored. To this
end, in this paper, we propose the Multimodal Large Language Model-enhanced
Multimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic
user preference, we design a two-stage user preference summarization method.
Specifically, we first utilize an MLLM-based item-summarizer to extract image
feature given an item and convert the image into text. Then, we employ a
recurrent user preference summarization generation paradigm to capture the
dynamic changes in user preferences based on an LLM-based user-summarizer.
Finally, to enable the MLLM for multi-modal recommendation task, we propose to
fine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)
techniques. Extensive evaluations across various datasets validate the
effectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt
to the evolving dynamics of user preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Graph Diffusion with Applications in Personalized
  PageRanks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00077v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00077v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhe Wei, Eli Chien, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph diffusion, which iteratively propagates real-valued substances among
the graph, is used in numerous graph/network-involved applications. However,
releasing diffusion vectors may reveal sensitive linking information in the
data such as transaction information in financial network data. However,
protecting the privacy of graph data is challenging due to its interconnected
nature. This work proposes a novel graph diffusion framework with edge-level
differential privacy guarantees by using noisy diffusion iterates. The
algorithm injects Laplace noise per diffusion iteration and adopts a
degree-based thresholding function to mitigate the high sensitivity induced by
low-degree nodes. Our privacy loss analysis is based on Privacy Amplification
by Iteration (PABI), which to our best knowledge, is the first effort that
analyzes PABI with Laplace noise and provides relevant applications. We also
introduce a novel Infinity-Wasserstein distance tracking method, which tightens
the analysis of privacy leakage and makes PABI more applicable in practice. We
evaluate this framework by applying it to Personalized Pagerank computation for
ranking tasks. Experiments on real-world network data demonstrate the
superiority of our method under stringent privacy conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appear in NeurIPS 2024. In this version, we provide a more rigorous
  analysis of graph distortion by establishing a tight bound, then update our
  corresponding experimental results, which are better than the previous
  version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech Separation with <span class="highlight-title">Pretrain</span>ed Frontend to Minimize Domain Mismatch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wupeng Wang, Zexu Pan, Xinke Li, Shuai Wang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech separation seeks to separate individual speech signals from a speech
mixture. Typically, most separation models are trained on synthetic data due to
the unavailability of target reference in real-world cocktail party scenarios.
As a result, there exists a domain gap between real and synthetic data when
deploying speech separation models in real-world applications. In this paper,
we propose a self-supervised domain-invariant pretrained (DIP) frontend that is
exposed to mixture data without the need for target reference speech. The DIP
frontend utilizes a Siamese network with two innovative pretext tasks, mixture
predictive coding (MPC) and mixture invariant coding (MIC), to capture shared
contextual cues between real and synthetic unlabeled mixtures. Subsequently, we
freeze the DIP frontend as a feature extractor when training the downstream
speech separation models on synthetic data. By pretraining the DIP frontend
with the contextual cues, we expect that the speech separation skills learned
from synthetic data can be effectively transferred to real data. To benefit
from the DIP frontend, we introduce a novel separation pipeline to align the
feature resolution of the separation models. We evaluate the speech separation
quality on standard benchmarks and real-world datasets. The results confirm the
superiority of our DIP frontend over existing speech separation models. This
study underscores the potential of large-scale pretraining to enhance the
quality and intelligibility of speech separation in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumanVLM: Foundation for Human-Scene Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Dai, Xu Long, Li Yutang, Zhang Yuanhui, Shuyin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-scene vision-language tasks are increasingly prevalent in diverse
social applications, yet recent advancements predominantly rely on models
specifically tailored to individual tasks. Emerging research indicates that
large vision-language models (VLMs) can enhance performance across various
downstream vision-language understanding tasks. However, general-domain models
often underperform in specialized fields. This study introduces a
domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model
(HumanVLM), designed to provide a foundation for human-scene Vision-Language
tasks. Specifically, (1) we create a large-scale human-scene multimodal
image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate
domain-specific alignment; (2) develop a captioning approach for human-centered
images, capturing human faces, bodies, and backgrounds, and construct a
high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs)
that contain as much detailed information as possible about human; (3) Using
HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments,
we then evaluate our HumanVLM across varous downstream tasks, where it
demonstrates superior overall performance among multimodal models of comparable
scale, particularly excelling in human-related tasks and significantly
outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM,
alongside the data introduced, will stimulate the research in human-around
fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages,11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-based Lossless Event Data Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmadreza Sezavar, Catarina Brites, Joao Ascenso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging event cameras acquire visual information by detecting time domain
brightness changes asynchronously at the pixel level and, unlike conventional
cameras, are able to provide high temporal resolution, very high dynamic range,
low latency, and low power consumption. Considering the huge amount of data
involved, efficient compression solutions are very much needed. In this
context, this paper presents a novel deep-learning-based lossless event data
compression scheme based on octree partitioning and a learned hyperprior model.
The proposed method arranges the event stream as a 3D volume and employs an
octree structure for adaptive partitioning. A deep neural network-based entropy
model, using a hyperprior, is then applied. Experimental results demonstrate
that the proposed method outperforms traditional lossless data compression
techniques in terms of compression ratio and bits per event.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Audio-Visual Sound Separation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, Yapeng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel continual audio-visual sound separation
task, aiming to continuously separate sound sources for new classes while
preserving performance on previously learned classes, with the aid of visual
guidance. This problem is crucial for practical visually guided auditory
perception as it can significantly enhance the adaptability and robustness of
audio-visual sound separation models, making them more applicable for
real-world scenarios where encountering new sound sources is commonplace. The
task is inherently challenging as our models must not only effectively utilize
information from both modalities in current tasks but also preserve their
cross-modal association in old tasks to mitigate catastrophic forgetting during
audio-visual continual learning. To address these challenges, we propose a
novel approach named ContAV-Sep (\textbf{Cont}inual
\textbf{A}udio-\textbf{V}isual Sound \textbf{Sep}aration). ContAV-Sep presents
a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the
cross-modal semantic similarity through incremental tasks and retain previously
acquired knowledge of semantic similarity in old models, mitigating the risk of
catastrophic forgetting. The CrossSDC can seamlessly integrate into the
training process of different audio-visual sound separation frameworks.
Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic
forgetting and achieve significantly better performance compared to other
continual learning baselines for audio-visual sound separation. Code is
available at: \url{https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual
  Visual Answer Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Wen, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Multilingual Visual Answer Localization (MVAL) is to locate a
video segment that answers a given multilingual question. Existing methods
either focus solely on visual modality or integrate visual and subtitle
modalities. However, these methods neglect the audio modality in videos,
consequently leading to incomplete input information and poor performance in
the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span
Localization (AVTSL) method that incorporates audio modality to augment both
visual and textual representations for the MVAL task. Specifically, we
integrate features from three modalities and develop three predictors, each
tailored to the unique contributions of the fused modalities: an audio-visual
predictor, a visual predictor, and a textual predictor. Each predictor
generates predictions based on its respective modality. To maintain consistency
across the predicted results, we introduce an Audio-Visual-Textual Consistency
module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing
each modality's predictor to dynamically learn from the others. This
collaborative learning ensures that the model generates consistent and
comprehensive answers. Extensive experiments show that our proposed method
outperforms several state-of-the-art (SOTA) methods, which demonstrates the
effectiveness of the audio modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alloy Das, Sanket Biswas, Prasun Roy, Subhankar Ghosh, Umapada Pal, Michael Blumenstein, Josep Lladós, Saumik Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Text Editing (STE) is a challenging research problem, that primarily
aims towards modifying existing texts in an image while preserving the
background and the font style of the original text. Despite its utility in
numerous real-world applications, existing style-transfer-based approaches have
shown sub-par editing performance due to (1) complex image backgrounds, (2)
diverse font attributes, and (3) varying word lengths within the text. To
address such limitations, in this paper, we propose a novel font-agnostic scene
text editing and rendering framework, named FASTER, for simultaneously
generating text in arbitrary styles and locations while preserving a natural
and realistic appearance and structure. A combined fusion of target mask
generation and style transfer units, with a cascaded self-attention mechanism
has been proposed to focus on multi-level text region edits to handle varying
word lengths. Extensive evaluation on a real-world database with further
subjective human evaluation study indicates the superiority of FASTER in both
scene text editing and rendering tasks, in terms of model performance and
efficiency. Our code will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POINTS: Improving Your Vision-language Model with Affordable Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04828v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04828v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, vision-language models have made significant strides,
excelling in tasks like optical character recognition and geometric
problem-solving. However, several critical issues remain: 1) Proprietary models
often lack transparency about their architectures, while open-source models
need more detailed ablations of their training strategies. 2) Pre-training data
in open-source works is under-explored, with datasets added empirically, making
the process cumbersome. 3) Fine-tuning often focuses on adding datasets,
leading to diminishing returns. To address these issues, we propose the
following contributions: 1) We trained a robust baseline model using the latest
advancements in vision-language models, introducing effective improvements and
conducting comprehensive ablation and validation for each technique. 2)
Inspired by recent work on large language models, we filtered pre-training data
using perplexity, selecting the lowest perplexity data for training. This
approach allowed us to train on a curated 1M dataset, achieving competitive
performance. 3) During visual instruction tuning, we used model soup on
different datasets when adding more datasets yielded marginal improvements.
These innovations resulted in a 9B parameter model that performs competitively
with state-of-the-art models. Our strategies are efficient and lightweight,
making them easily adoptable by the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-11-13T05:27:12.247832982Z">
            2024-11-13 05:27:12 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
